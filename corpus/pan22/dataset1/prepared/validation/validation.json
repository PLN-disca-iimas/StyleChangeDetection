[
    [
        {
            "id": "1-1-2",
            "pair": [
                "If you can handle a slight delay in the data you are reporting on then a easy solution would be to restore the database less frequently.\n",
                "You can still backup and copy every 5 minutes but just change the frequency of the restore job to once an hour or whatever is appropriate. \n"
            ]
        },
        {
            "id": "1-2-3",
            "pair": [
                "You can still backup and copy every 5 minutes but just change the frequency of the restore job to once an hour or whatever is appropriate. \n",
                "Users may still get disconnected but the frequency will be lower.\n"
            ]
        },
        {
            "id": "1-3-4",
            "pair": [
                "Users may still get disconnected but the frequency will be lower.\n",
                "If you need to keep online access to the data, you've got a couple of options (assuming you're wanting to use just SQL native functionality for the solution, if you are open to 3rd party software and/or hardware, you've got quite a few other options):\n"
            ]
        },
        {
            "id": "1-4-5",
            "pair": [
                "If you need to keep online access to the data, you've got a couple of options (assuming you're wanting to use just SQL native functionality for the solution, if you are open to 3rd party software and/or hardware, you've got quite a few other options):\n",
                "1) Replication - most likely transactional replication and a single read-only subscriber (msdn is a good start for an overview, I'd post a link but I can only use 1 at the moment, just google \"sql server replication msdn\" and it will be at the top)\n"
            ]
        },
        {
            "id": "1-5-6",
            "pair": [
                "1) Replication - most likely transactional replication and a single read-only subscriber (msdn is a good start for an overview, I'd post a link but I can only use 1 at the moment, just google \"sql server replication msdn\" and it will be at the top)\n",
                "2) Keep your log shipping configuration to get data to the secondary server and leverage database snapshots combined with a common database and rotating synonyms (see here for details on this type of architecture). This will only work if you are using the enterprise edition on the secondary server (only edition that supports snapshots)."
            ]
        }
    ],
    [
        {
            "id": "10-1-2",
            "pair": [
                "Use redraw-current-line function of bind builtin. First check if it's already bound maybe:\n",
                "I've never seen it bound by default, so you will probably need to bind it. Pick a key combination, let's say Ctrl+Y. Check if it's already taken:\n"
            ]
        },
        {
            "id": "10-2-3",
            "pair": [
                "I've never seen it bound by default, so you will probably need to bind it. Pick a key combination, let's say Ctrl+Y. Check if it's already taken:\n",
                "Empty output means the combination is unused. If so, let's bind redraw-current-line to it:\n"
            ]
        },
        {
            "id": "10-3-4",
            "pair": [
                "Empty output means the combination is unused. If so, let's bind redraw-current-line to it:\n",
                "Now, whenever a background process messes with your command line, hit Ctrl+Y. Then your prompt will be redrawn along with whatever command you have just partially typed (if any), so you can continue as if nothing happened.\n"
            ]
        },
        {
            "id": "10-4-5",
            "pair": [
                "Now, whenever a background process messes with your command line, hit Ctrl+Y. Then your prompt will be redrawn along with whatever command you have just partially typed (if any), so you can continue as if nothing happened.\n",
                "To make the binding permanent you could add the above command to your ~/.bashrc, but don't. The right approach is to modify ~/.inputrc (for user) or /etc/inputrc (system-wide). This way any program that uses readline(3) library will obey. The line to add to either file looks like this:\n"
            ]
        },
        {
            "id": "10-5-6",
            "pair": [
                "To make the binding permanent you could add the above command to your ~/.bashrc, but don't. The right approach is to modify ~/.inputrc (for user) or /etc/inputrc (system-wide). This way any program that uses readline(3) library will obey. The line to add to either file looks like this:\n",
                "But if you create ~/.inputrc anew, make sure its first line says $include /etc/inputrc. This is because up to this point readline has used /etc/inputrc and maybe your workflow relies on what's in this file. From now on, the library will use your ~/.inputrc instead; the line $include /etc/inputrc makes it parse the system-wide file as well.\n"
            ]
        },
        {
            "id": "10-6-7",
            "pair": [
                "But if you create ~/.inputrc anew, make sure its first line says $include /etc/inputrc. This is because up to this point readline has used /etc/inputrc and maybe your workflow relies on what's in this file. From now on, the library will use your ~/.inputrc instead; the line $include /etc/inputrc makes it parse the system-wide file as well.\n",
                "If you press Ctrl+L, it will partially do what you want. It will redraw the current line including everything you typed up to that point including the cursor position, but it will clear the screen, so your previous output is lost (or in case of a terminal window in the scrollback buffer). On the other hand, you were willing to try clear, so maybe that is not a problem."
            ]
        }
    ],
    [
        {
            "id": "100-1-2",
            "pair": [
                "A SQL statement always runs in a transaction.  If you don't start one explicitly, every SQL statement will run in a transaction of itself.\n",
                "The only choice is whether you bundle multiple statements in one transaction. Transactions that span multiple statements leave locks that hurt concurrency.  So \"always\" creating a transactions is not a good idea.  You should balance the cost against the benefit.\n"
            ]
        },
        {
            "id": "100-2-3",
            "pair": [
                "The only choice is whether you bundle multiple statements in one transaction. Transactions that span multiple statements leave locks that hurt concurrency.  So \"always\" creating a transactions is not a good idea.  You should balance the cost against the benefit.\n",
                "The issue is whether a group of operations must be treated as a single action.  In other words all of the operations must be completed and committed successfully or none of the operations can be committed.\n"
            ]
        },
        {
            "id": "100-3-4",
            "pair": [
                "The issue is whether a group of operations must be treated as a single action.  In other words all of the operations must be completed and committed successfully or none of the operations can be committed.\n",
                "If you have a scenario that requires you to read preliminary data and then perform updates based on that data then the initial read should probably be part of the transaction.\n"
            ]
        },
        {
            "id": "100-4-5",
            "pair": [
                "If you have a scenario that requires you to read preliminary data and then perform updates based on that data then the initial read should probably be part of the transaction.\n",
                "Note: I am avoiding Select/Insert/Update on purpose.  The transaction scope may actually be at the application level and involve multiple database(s) operations.\n"
            ]
        },
        {
            "id": "100-5-6",
            "pair": [
                "Note: I am avoiding Select/Insert/Update on purpose.  The transaction scope may actually be at the application level and involve multiple database(s) operations.\n",
                "Think of classics patterns such as Airplane Seat Reservation or Bank Balance Query/Withdrawal.\n"
            ]
        },
        {
            "id": "100-6-7",
            "pair": [
                "Think of classics patterns such as Airplane Seat Reservation or Bank Balance Query/Withdrawal.\n",
                "One must take a wider view of the problem to ensure the whole application yields reliable, consistent data."
            ]
        }
    ],
    [
        {
            "id": "101-1-2",
            "pair": [
                "This code is, depending on your point of view, either over-engineered or just wrong.\n",
                "The whole point of separating the interface from the implementation would be to make the interface independent of the implementation.  Yet, your interface mentions EngineerImpl.View, which defeats the whole purpose of that separation.\n"
            ]
        },
        {
            "id": "101-2-3",
            "pair": [
                "The whole point of separating the interface from the implementation would be to make the interface independent of the implementation.  Yet, your interface mentions EngineerImpl.View, which defeats the whole purpose of that separation.\n",
                "The fix, if you consider the code to be over-engineered, is to forget about splitting the interface and implementation, and just make an Engineer class.  If you don't intend to write any other implementations of the interface, that would be the way to go.  Alternatively, you could fix it by moving EngineerImpl.View to Engineer.View.\n"
            ]
        },
        {
            "id": "101-3-4",
            "pair": [
                "The fix, if you consider the code to be over-engineered, is to forget about splitting the interface and implementation, and just make an Engineer class.  If you don't intend to write any other implementations of the interface, that would be the way to go.  Alternatively, you could fix it by moving EngineerImpl.View to Engineer.View.\n",
                "In addition, the EngineersFactory is in a weird place.  A factory could be useful if you want the caller not to instantiate objects directly \u2014 for example, if the caller doesn't know what class to instantiate.  Putting the factory as an inner class of EngineerImpl seems to suggest that the factory is meant to only produce EngineerImpl objects, though.\n"
            ]
        },
        {
            "id": "101-4-5",
            "pair": [
                "In addition, the EngineersFactory is in a weird place.  A factory could be useful if you want the caller not to instantiate objects directly \u2014 for example, if the caller doesn't know what class to instantiate.  Putting the factory as an inner class of EngineerImpl seems to suggest that the factory is meant to only produce EngineerImpl objects, though.\n",
                "The naming of the methods in the factory is also off.  \"Get\" implies fetching an existing instance (like a singleton), not creating a new one.  For a factory, names like createEngineer(), makeEngineer(), newEngineer(), or just engineer(\u2026) would be more appropriate.\n"
            ]
        },
        {
            "id": "101-5-6",
            "pair": [
                "The naming of the methods in the factory is also off.  \"Get\" implies fetching an existing instance (like a singleton), not creating a new one.  For a factory, names like createEngineer(), makeEngineer(), newEngineer(), or just engineer(\u2026) would be more appropriate.\n",
                "Again, I'm not sure what value the factory adds.  The EngineerImpl(name, type) constructor is already public, as are the string constants for the types.  If you abandon the interface-implementation separation, then you would be better off eliminating the factory as well.  Alternatively, keep the factory, but make the EngineerImpl constructor private so that there is one right way to instantiate objects.\n"
            ]
        },
        {
            "id": "101-6-7",
            "pair": [
                "Again, I'm not sure what value the factory adds.  The EngineerImpl(name, type) constructor is already public, as are the string constants for the types.  If you abandon the interface-implementation separation, then you would be better off eliminating the factory as well.  Alternatively, keep the factory, but make the EngineerImpl constructor private so that there is one right way to instantiate objects.\n",
                "The makeOffer() method could be cleaned up. Since the shouldIAcceptNewOffer() and acceptOffer(), both are only used once for this single method it would make more sense to handle all the logic in one method.\n"
            ]
        },
        {
            "id": "101-7-8",
            "pair": [
                "The makeOffer() method could be cleaned up. Since the shouldIAcceptNewOffer() and acceptOffer(), both are only used once for this single method it would make more sense to handle all the logic in one method.\n",
                "This method saves passing through the parameters salary and employer again unnecessarily and is a more neater/conventional approach. "
            ]
        }
    ],
    [
        {
            "id": "102-1-2",
            "pair": [
                "Actually, I would say the answer is yes... There is an ever so slightly higher risk than an internal network card. You are probably more likely to be bitten by a shark while being stuck by lightning for this to occur (an exaggeration, but used to get the point across). \n",
                "But to put that into perspective, it would be almost impossible to exploit that without you knowing it in any attended location because the hacker attempting to compromise your network at this point would have to physically be in the connection between your PC and network adapter. The likelihood of that occurring without being discovered is extremely remote. You are probably more likely to have a hacker to get into a random room and physically plug into a network jack, which would be a multitude of times more dangerous to your network in most cases than someone compromising your adapter. \n"
            ]
        },
        {
            "id": "102-2-3",
            "pair": [
                "But to put that into perspective, it would be almost impossible to exploit that without you knowing it in any attended location because the hacker attempting to compromise your network at this point would have to physically be in the connection between your PC and network adapter. The likelihood of that occurring without being discovered is extremely remote. You are probably more likely to have a hacker to get into a random room and physically plug into a network jack, which would be a multitude of times more dangerous to your network in most cases than someone compromising your adapter. \n",
                "I want to get an external network card for WiFi and Bluetooth for my PC. A friend of mine advised me to buy one from Asia, since those are significantly cheaper. The difference in actual performance aside, do these cards pose security threads? Is it possible to monitor my network traffic with such a card, and can I detect this?"
            ]
        }
    ],
    [
        {
            "id": "103-1-2",
            "pair": [
                "Looks like you're selecting the Unit of Measure (UoM) id column instead of the value column.\n",
                "You'll need to make sure you replace the [fimensions unit of measure id] column with [dimensions unit of measure VALUE] column\n"
            ]
        },
        {
            "id": "103-2-3",
            "pair": [
                "You'll need to make sure you replace the [fimensions unit of measure id] column with [dimensions unit of measure VALUE] column\n",
                "NOTE: I'll assume your table are named product and unit; please replace those with the actual names if you try to use any of this.\n"
            ]
        },
        {
            "id": "103-3-4",
            "pair": [
                "NOTE: I'll assume your table are named product and unit; please replace those with the actual names if you try to use any of this.\n",
                "If every row in product has a valid  weight or dimension unit of measure ID- that is to say, if none of the IDs is NULL, and all ID values actually exist in unit - then you can do this with an INNER JOIN.\n"
            ]
        },
        {
            "id": "103-4-5",
            "pair": [
                "If every row in product has a valid  weight or dimension unit of measure ID- that is to say, if none of the IDs is NULL, and all ID values actually exist in unit - then you can do this with an INNER JOIN.\n",
                "If some rows in product have a NULL value in that field, or have a value that doesn't match the actual ids in unit, and you'd still want those product rows to show up in your results, then you need to use an OUTER JOIN.\n"
            ]
        },
        {
            "id": "103-5-6",
            "pair": [
                "If some rows in product have a NULL value in that field, or have a value that doesn't match the actual ids in unit, and you'd still want those product rows to show up in your results, then you need to use an OUTER JOIN.\n",
                "I'll construct a demonstration query using a LEFT OUTER JOIN for both IDs. IF you have a row where the actual unit row can't be found, then that unit name will come out as NULL:\n"
            ]
        },
        {
            "id": "103-6-7",
            "pair": [
                "I'll construct a demonstration query using a LEFT OUTER JOIN for both IDs. IF you have a row where the actual unit row can't be found, then that unit name will come out as NULL:\n",
                "When joining to the same table twice, you must use a table alias so you can tell which joined table you're referring to."
            ]
        }
    ],
    [
        {
            "id": "104-1-2",
            "pair": [
                "It depends on what the rate-limiting factor is. Most of the time, the end-user's Internet bandwidth is the limiting factor, and then it doesn't matter. But if, for example, the limiting factor is competing with other traffic, then more streams will tend to get a larger share of that traffic. If the limiting factor is packet loss, then more streams may be a bit more resistant (as a drop on one stream won't affect traffic on the others).\n",
                "I have Firefox browser with Down Them All extension, running in Ubuntu.  Every additional file download increases the average speed and decreases the total time to completion.  Six simultaneous downloads of roughly the same size can complete in 1/6 the elapsed time of a single download, if they all complete within a few seconds of each other.  If 5 of them finish at 3 minutes, and 1 is not yet finished, the time to completion of that 1 will increase rapidly to as much as 20 minutes.  I have found that repeatedly running an internet speed test at the same time as a single download will greatly decrease the time it takes to complete that download."
            ]
        }
    ],
    [
        {
            "id": "105-1-2",
            "pair": [
                "I've used RapidSSL for years for my clients and have no problem with them at all with browser support.\n",
                "Super-fast verification: some of them require you to fax in company letterhead, and/or their \"verification team\" takes 3-4 business days to activate.  RapidSSL uses an automated system whereas you're presented with a verification code on the screen that must be keyed into the phone when they call.\n"
            ]
        },
        {
            "id": "105-2-3",
            "pair": [
                "Super-fast verification: some of them require you to fax in company letterhead, and/or their \"verification team\" takes 3-4 business days to activate.  RapidSSL uses an automated system whereas you're presented with a verification code on the screen that must be keyed into the phone when they call.\n",
                "Phone support is a bit spotty, although I've only had to call them once when I didn't have a direct line to use... which brings me to the second minor con: you need a direct line, no extensions, etc. and it can't be a cell phone for the verification to work.  This usually isn't an issue though as you can give the receptionist a heads-up that you're expecting a call in x amount of seconds (you can specify how long to wait to dial).\n"
            ]
        },
        {
            "id": "105-3-4",
            "pair": [
                "Phone support is a bit spotty, although I've only had to call them once when I didn't have a direct line to use... which brings me to the second minor con: you need a direct line, no extensions, etc. and it can't be a cell phone for the verification to work.  This usually isn't an issue though as you can give the receptionist a heads-up that you're expecting a call in x amount of seconds (you can specify how long to wait to dial).\n",
                "Again, these are all really minor when you factor in the low cost and the near-instant access to your certificates.\n"
            ]
        },
        {
            "id": "105-4-5",
            "pair": [
                "Again, these are all really minor when you factor in the low cost and the near-instant access to your certificates.\n",
                "You could use this website to get an opinion: http://www.sslshopper.com/ssl-certificate-wizard.html"
            ]
        }
    ],
    [
        {
            "id": "106-1-2",
            "pair": [
                "Looking at dump with Windbg shows this string \\??\\ACPI#INT3400#2&daba3ff&3#{ee27098e-1b22-472a-89d8-5ccce16b1356}\n",
                "Which seams to be an \"Intel DPTF Manager\" or related driver. So look inf this driver was updated and rollback to older version.\n"
            ]
        },
        {
            "id": "106-2-3",
            "pair": [
                "Which seams to be an \"Intel DPTF Manager\" or related driver. So look inf this driver was updated and rollback to older version.\n",
                "Really hoping someone can help me. I'm at work and trying to fix one of our laptops. It's Dell Latitude E5450 running Windows 10.\n"
            ]
        },
        {
            "id": "106-3-4",
            "pair": [
                "Really hoping someone can help me. I'm at work and trying to fix one of our laptops. It's Dell Latitude E5450 running Windows 10.\n",
                "A couple of hours ago (before I got in, unfortunately) it displayed a popup with an update from Dell. My coworker opted to install it and get it out of the way.\n"
            ]
        },
        {
            "id": "106-4-5",
            "pair": [
                "A couple of hours ago (before I got in, unfortunately) it displayed a popup with an update from Dell. My coworker opted to install it and get it out of the way.\n",
                "It rebooted as part of the update process, and began bluescreening with the error code: PNP_DETECTED_FATAL_ERROR.\n"
            ]
        },
        {
            "id": "106-5-6",
            "pair": [
                "It rebooted as part of the update process, and began bluescreening with the error code: PNP_DETECTED_FATAL_ERROR.\n",
                "It does this for a few times until it goes to the recovery menu. I can get into safe mode.\n"
            ]
        },
        {
            "id": "106-6-7",
            "pair": [
                "It does this for a few times until it goes to the recovery menu. I can get into safe mode.\n",
                "Here are the five latest Minidump files: minidumps\n"
            ]
        },
        {
            "id": "106-7-8",
            "pair": [
                "Here are the five latest Minidump files: minidumps\n",
                "Nothing looks off in the device manager. There's three disabled devices, but all of them because of safe mode.\n"
            ]
        },
        {
            "id": "106-8-9",
            "pair": [
                "Nothing looks off in the device manager. There's three disabled devices, but all of them because of safe mode.\n",
                "Dell support was.. Well, well-meaning and as helpful as they could be. Unfortunately, it seems like I know more than they did and their only solution as \"reinstall\".\n"
            ]
        },
        {
            "id": "106-9-10",
            "pair": [
                "Dell support was.. Well, well-meaning and as helpful as they could be. Unfortunately, it seems like I know more than they did and their only solution as \"reinstall\".\n",
                "I've tried similiar things with another latop a few months back. System restore worked then, but doesn't now.\n"
            ]
        },
        {
            "id": "106-10-11",
            "pair": [
                "I've tried similiar things with another latop a few months back. System restore worked then, but doesn't now.\n",
                "I'm really hoping to save reinstalling as an absolutely last resort.\n"
            ]
        },
        {
            "id": "106-11-12",
            "pair": [
                "I'm really hoping to save reinstalling as an absolutely last resort.\n",
                "Anybody have any ideas on what could be wrong and what I can try to fix it?"
            ]
        }
    ],
    [
        {
            "id": "107-1-2",
            "pair": [
                "Power down the PC, start the PC, before you get to the login screen when Windows is booting press and hold the Power Button until the PC shuts off.\n",
                "Do this three times or until you get the Automatic Repair screen, let repair load and run its Diagnosis, when that is done click the Advanced Options button when it appears, then Troubleshoot button, then Advanced Options, then System Restore button.\n"
            ]
        },
        {
            "id": "107-2-3",
            "pair": [
                "Do this three times or until you get the Automatic Repair screen, let repair load and run its Diagnosis, when that is done click the Advanced Options button when it appears, then Troubleshoot button, then Advanced Options, then System Restore button.\n",
                "When restore loads hit Next button, now you will see your restore points, highlight either the first one or second it the date is not over 10 days old (make note of the name and date of the restore point), once selected hit next. This process takes a long while depending on the speed of your hard drive, be patient and Do Not interrupt the process.\n"
            ]
        },
        {
            "id": "107-3-4",
            "pair": [
                "When restore loads hit Next button, now you will see your restore points, highlight either the first one or second it the date is not over 10 days old (make note of the name and date of the restore point), once selected hit next. This process takes a long while depending on the speed of your hard drive, be patient and Do Not interrupt the process.\n",
                "If it does not solve it start over and pick an older restore point that is next in line.\n"
            ]
        },
        {
            "id": "107-4-5",
            "pair": [
                "If it does not solve it start over and pick an older restore point that is next in line.\n",
                "My Windows 10 desktop has begun displaying a completely gray screen immediately after login. I recently installed new graphics drivers for Ubuntu and I suspect that my graphics drivers for Windows 10 were deleted during this process. Is there any way for me to reinstall graphics drivers when I cannot see the screen, or possibly install into the Windows partition through Ubuntu? Graphics card is an RTX 2080. I have tried booting into Safe Mode, but the result is the same, with the addition of a non-functional flashing taskbar."
            ]
        }
    ],
    [
        {
            "id": "108-1-2",
            "pair": [
                "The OOM killer only wreaks havoc if you have overloaded your system.  Give it enough swap, and don't run applications that suddenly decide to eat massive amounts of RAM, and you won't have a problem.\n",
                "Basically, my experience is that turning off overcommit is a nice experiment that rarely works as well in practice as it sounds in theory.  This nicely corresponds with my experiences with other tunables in the kernel -- the Linux kernel developers are almost always smarter than you, and the defaults work the best for the vast, vast majority of cases.  Leave them alone, and instead go find what process has the leak and fix it.\n"
            ]
        },
        {
            "id": "108-2-3",
            "pair": [
                "Basically, my experience is that turning off overcommit is a nice experiment that rarely works as well in practice as it sounds in theory.  This nicely corresponds with my experiences with other tunables in the kernel -- the Linux kernel developers are almost always smarter than you, and the defaults work the best for the vast, vast majority of cases.  Leave them alone, and instead go find what process has the leak and fix it.\n",
                "The OOM killer on Linux wreaks havoc with various applications every so often, and it appears that not much is really done on the kernel development side to improve this.  Would it not be better, as a best practice when setting up a new server, to reverse the default on the memory overcommitting, that is, turn it off (vm.overcommit_memory=2) unless you know you want it on for your particular use?  And what would those use cases be where you know you want the overcommitting on?\n"
            ]
        },
        {
            "id": "108-3-4",
            "pair": [
                "The OOM killer on Linux wreaks havoc with various applications every so often, and it appears that not much is really done on the kernel development side to improve this.  Would it not be better, as a best practice when setting up a new server, to reverse the default on the memory overcommitting, that is, turn it off (vm.overcommit_memory=2) unless you know you want it on for your particular use?  And what would those use cases be where you know you want the overcommitting on?\n",
                "As a bonus, since the behavior in case of vm.overcommit_memory=2 depends on vm.overcommit_ratio and swap space, what would be a good rule of thumb for sizing the latter two so that this whole setup keeps working reasonably?"
            ]
        }
    ],
    [
        {
            "id": "109-1-2",
            "pair": [
                "The problem using the /etc/fstab file is that the mountpoint is mounted always and if not reacheable, it can hang your machine because the kernel will continuously try to mount the point.\n",
                "To avoid this, you could use autofs (/etc/auto.master), which is a kernel-based automounter. In that case, the mountpoint will only be mounted if you (or some process) access the configured directory, and if you don't, or the mountpoint is idle for some time, it will automatically be unmounted and this way you can avoid the the hanging behavior.\n"
            ]
        },
        {
            "id": "109-2-3",
            "pair": [
                "To avoid this, you could use autofs (/etc/auto.master), which is a kernel-based automounter. In that case, the mountpoint will only be mounted if you (or some process) access the configured directory, and if you don't, or the mountpoint is idle for some time, it will automatically be unmounted and this way you can avoid the the hanging behavior.\n",
                "I believe not all operating systems have the autofs package installed by default. You can check if it's installed and if not, install the autofs package. Once installed, simply run man autofs to get some help.\n"
            ]
        },
        {
            "id": "109-3-4",
            "pair": [
                "I believe not all operating systems have the autofs package installed by default. You can check if it's installed and if not, install the autofs package. Once installed, simply run man autofs to get some help.\n",
                "I have that entry in my fstab which works pretty well when I'm at home and 192.168.1.195 is in my LAN\n"
            ]
        },
        {
            "id": "109-4-5",
            "pair": [
                "I have that entry in my fstab which works pretty well when I'm at home and 192.168.1.195 is in my LAN\n",
                "Now when I'm not connected at home, on another network, I face that issue launching deluge-gtk  where it takes ages to launch and outputs that error message\n"
            ]
        },
        {
            "id": "109-5-6",
            "pair": [
                "Now when I'm not connected at home, on another network, I face that issue launching deluge-gtk  where it takes ages to launch and outputs that error message\n",
                "If I comment the fstab entry, then deluge-gtk launches absolutely fine and  don't get that message.\n"
            ]
        },
        {
            "id": "109-6-7",
            "pair": [
                "If I comment the fstab entry, then deluge-gtk launches absolutely fine and  don't get that message.\n",
                "What can I do in order to solve that issue, is there a param in /etc/fstab i could add to detect I'm not at home ?"
            ]
        }
    ],
    [
        {
            "id": "11-1-2",
            "pair": [
                "Try setting networkwait=1 in your ntpconfig, if your time synch at boot is not completing correctly. it may also be a good idea to add iburst to the end of your server declaration in your clients ntp.conf.\n",
                "https://access.redhat.com/site/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Deployment_Guide/sect-Date_and_Time_Configuration-Command_Line_Configuration-Network_Time_Protocol.html\n"
            ]
        },
        {
            "id": "11-2-3",
            "pair": [
                "https://access.redhat.com/site/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Deployment_Guide/sect-Date_and_Time_Configuration-Command_Line_Configuration-Network_Time_Protocol.html\n",
                "since you are on ubuntu, which ships clients with ntpupdate by default, and which does attempt a synch on boot, it is likely that your clients network is still down when the request occurs. ubuntu also updates the time daily via chron job. \n"
            ]
        },
        {
            "id": "11-3-4",
            "pair": [
                "since you are on ubuntu, which ships clients with ntpupdate by default, and which does attempt a synch on boot, it is likely that your clients network is still down when the request occurs. ubuntu also updates the time daily via chron job. \n",
                "https://help.ubuntu.com/10.04/serverguide/NTP.html\n"
            ]
        },
        {
            "id": "11-4-5",
            "pair": [
                "https://help.ubuntu.com/10.04/serverguide/NTP.html\n",
                "Minpoll and maxpoll seem to have a lot to do with the internal workings of NTP, but they don't seem like things you want to change without a deep understanding of the implications. I would restore them to default for now, and try the advice above once before manipulating them further.\n"
            ]
        },
        {
            "id": "11-5-6",
            "pair": [
                "Minpoll and maxpoll seem to have a lot to do with the internal workings of NTP, but they don't seem like things you want to change without a deep understanding of the implications. I would restore them to default for now, and try the advice above once before manipulating them further.\n",
                "I simply try to set-up NTP to my system. Both server and clients will run on my computer which are linked together with local link. One of them will have the reference clock. \n"
            ]
        },
        {
            "id": "11-6-7",
            "pair": [
                "I simply try to set-up NTP to my system. Both server and clients will run on my computer which are linked together with local link. One of them will have the reference clock. \n",
                "Both the server and Client are linux Ubuntu. I install  ntp daemon to both sides. In clients, I enter the ip address of the server to /etc/ntp.conf. Everything works fine.\n"
            ]
        },
        {
            "id": "11-7-8",
            "pair": [
                "Both the server and Client are linux Ubuntu. I install  ntp daemon to both sides. In clients, I enter the ip address of the server to /etc/ntp.conf. Everything works fine.\n",
                "However, the setting of the time to correct time in client side takes too long time (around 17 minutes). Is it possible to gather correct time just at startup. I write some code that regularly calls \"ntpdate \" by system call and the problem is solved but there has to be something that allows me to decrease the poll time of the client to 1-2 minutes. There are some settings as maxpoll - minpoll in ntp.conf, but I couldn't manage to understand their function, because with the best configuration (minpoll 4? 16 seconds?) I also cannot see that client side corrects its time before 10 minutes.\n"
            ]
        },
        {
            "id": "11-8-9",
            "pair": [
                "However, the setting of the time to correct time in client side takes too long time (around 17 minutes). Is it possible to gather correct time just at startup. I write some code that regularly calls \"ntpdate \" by system call and the problem is solved but there has to be something that allows me to decrease the poll time of the client to 1-2 minutes. There are some settings as maxpoll - minpoll in ntp.conf, but I couldn't manage to understand their function, because with the best configuration (minpoll 4? 16 seconds?) I also cannot see that client side corrects its time before 10 minutes.\n",
                "In addition, in some cases my client is an embedded system (ARM - IGEP board) and it always opens with an irrelevant date (2-3 years ago). So the time that takes to correct the time should not depend on the time difference also. "
            ]
        }
    ],
    [
        {
            "id": "110-1-2",
            "pair": [
                "I've setup similar VirtualDocumentRoot-based for wild-card sub-domains before. It's just so useful to be able to create a directory inside someone's home-directory - usually sharing their directory over Samba so that they can be used from the developer's desktop - and you have create a new website - at least for development & testing.\n",
                "As for your problem, I just put them onto different IP addresses, without a default vhost.\n"
            ]
        },
        {
            "id": "110-2-3",
            "pair": [
                "As for your problem, I just put them onto different IP addresses, without a default vhost.\n",
                "I have a development server for PHP programmers set up and runnig for some time now.\n"
            ]
        },
        {
            "id": "110-3-4",
            "pair": [
                "I have a development server for PHP programmers set up and runnig for some time now.\n",
                "I have set up about 30 name based VirtualHosts, one for each project and for each developer. \n"
            ]
        },
        {
            "id": "110-4-5",
            "pair": [
                "I have set up about 30 name based VirtualHosts, one for each project and for each developer. \n",
                "I do the configuration with scripts, but now I have to add about 15 new project, all of them with standard directory structure, so I thought, I will use mod_vhost_alias\n"
            ]
        },
        {
            "id": "110-5-6",
            "pair": [
                "I do the configuration with scripts, but now I have to add about 15 new project, all of them with standard directory structure, so I thought, I will use mod_vhost_alias\n",
                "The result is, when I connect to http://my-page.com.web.dev.example.com I get the default vhost from /var/www/404 directory. \n"
            ]
        },
        {
            "id": "110-6-7",
            "pair": [
                "The result is, when I connect to http://my-page.com.web.dev.example.com I get the default vhost from /var/www/404 directory. \n",
                "The directory /var/www/web/my-page.com/ exists, and contains copy of my-page.com real webpage. \n"
            ]
        },
        {
            "id": "110-7-8",
            "pair": [
                "The directory /var/www/web/my-page.com/ exists, and contains copy of my-page.com real webpage. \n",
                "What am I doing wrong? Do I need separate ip addresses for \"standard\" NameVirtualHost and for vhost_alias?"
            ]
        }
    ],
    [
        {
            "id": "111-1-2",
            "pair": [
                "I asked on the btrfs mailing list, and got the answer I needed.\n",
                "To summarize, the btrfs raid 1 disk access algorithms work by reading from one disk for even-numbered PIDs, and the other for odd-numbered PIDs.  It's on the roadmap to upgrade this to a more advanced algorithm, but obviously that sort of lottery probably won't work out very well for one fast disk and one slow disk.  And on top of that, it's possible that if the slower disk falls too far behind the faster disk while writing, it may cause errors.\n"
            ]
        },
        {
            "id": "111-2-3",
            "pair": [
                "To summarize, the btrfs raid 1 disk access algorithms work by reading from one disk for even-numbered PIDs, and the other for odd-numbered PIDs.  It's on the roadmap to upgrade this to a more advanced algorithm, but obviously that sort of lottery probably won't work out very well for one fast disk and one slow disk.  And on top of that, it's possible that if the slower disk falls too far behind the faster disk while writing, it may cause errors.\n",
                "I've opted to simply make backups to the slower disk and restore from those in the event of an error.  That way I can still enjoy the bitrot protection of btrfs' checksumming while avoiding possible issues with different disk speeds.\n"
            ]
        },
        {
            "id": "111-3-4",
            "pair": [
                "I've opted to simply make backups to the slower disk and restore from those in the event of an error.  That way I can still enjoy the bitrot protection of btrfs' checksumming while avoiding possible issues with different disk speeds.\n",
                "My laptop has two disks, a SSD and a traditional magnetic disk. I plan to make a partition on the mag disk equal in size the SSD and set up BTRFS RAID1. This I know how to do. \n"
            ]
        },
        {
            "id": "111-4-5",
            "pair": [
                "My laptop has two disks, a SSD and a traditional magnetic disk. I plan to make a partition on the mag disk equal in size the SSD and set up BTRFS RAID1. This I know how to do. \n",
                "Although I am not routinely familiar with BTRFS, I do know today's file systems repair themselves, and when a mirror RAID1 used, the file system can operate only from one mirror if necessary. And with the right file system fsck.btrfs .. the repairs and read are performed from the first disk to access the information and then passed to the process which needs it. \n"
            ]
        },
        {
            "id": "111-5-6",
            "pair": [
                "Although I am not routinely familiar with BTRFS, I do know today's file systems repair themselves, and when a mirror RAID1 used, the file system can operate only from one mirror if necessary. And with the right file system fsck.btrfs .. the repairs and read are performed from the first disk to access the information and then passed to the process which needs it. \n",
                "The only reason I'm doing the RAID1 is for the self-healing. I realize writing large amounts of data will be slower that the SSD alone, but is it possible to set it up to only read from the magnetic drive if there's an error reading from the SSD?\n"
            ]
        },
        {
            "id": "111-6-7",
            "pair": [
                "The only reason I'm doing the RAID1 is for the self-healing. I realize writing large amounts of data will be slower that the SSD alone, but is it possible to set it up to only read from the magnetic drive if there's an error reading from the SSD?\n",
                "In summary, to set it up to only read from the magnetic drive if there's an error will be done summarily as an auto process because of the RAID1 setup. "
            ]
        }
    ],
    [
        {
            "id": "112-1-2",
            "pair": [
                "The complement problems Minimum (Edge) Bipartization and MaxCut are both in FPT, so the conjecture is false.\n",
                "By \"complement problems\", I mean the two problems' objective functions are complement. For example, the vertex cover and its complement independent set in this sense. For a graph $G(V,E)$, their answers add up to $|V|$. Another example is a problem I am working on. The dominating set problem and its complement \"packing as many edges as possible so that there is no path or circle whose length is more than 2\". Dominating set is $W[2]$-complete in the sense, and the edge packing problem is in $W[1]$-complete( whether it is complete in $W[1]$, I am still looking). \n"
            ]
        },
        {
            "id": "112-2-3",
            "pair": [
                "By \"complement problems\", I mean the two problems' objective functions are complement. For example, the vertex cover and its complement independent set in this sense. For a graph $G(V,E)$, their answers add up to $|V|$. Another example is a problem I am working on. The dominating set problem and its complement \"packing as many edges as possible so that there is no path or circle whose length is more than 2\". Dominating set is $W[2]$-complete in the sense, and the edge packing problem is in $W[1]$-complete( whether it is complete in $W[1]$, I am still looking). \n",
                "If problem A and problem B are complement, then A and B are not in the same class in the parameterized complexity structure, i.e. $FPT \\subseteq W[1] \\subseteq W[2] \\subseteq \\cdots W[SAT] $. Of course, there is another condition : A and B are not $P$ in the classical complexity hierarchy.\n"
            ]
        },
        {
            "id": "112-3-4",
            "pair": [
                "If problem A and problem B are complement, then A and B are not in the same class in the parameterized complexity structure, i.e. $FPT \\subseteq W[1] \\subseteq W[2] \\subseteq \\cdots W[SAT] $. Of course, there is another condition : A and B are not $P$ in the classical complexity hierarchy.\n",
                "I am not very familiar with parameterized complexity theory. So, any advice is welcome. Thank you very much."
            ]
        }
    ],
    [
        {
            "id": "113-1-2",
            "pair": [
                "The goal of persistance is to allow you to keep the content of one folder (for instance /home) across reboots. You cannot have persistance on the whole system because it is stored in an ISO file that gets loaded when you boot. \n",
                "What you are looking for is to do a full install of Kali on USB stick. In that way, you'll be able to access the whole file system, like if it was installed on your computer. To do that, follow this tutorial to install kali : http://docs.kali.org/installation/kali-linux-hard-disk-install and when choosing for a disk at step 10, select your USB drive. Make sure not to select your hard drive cause it would wipe it out. For more security, consider unplugging your hard drive before doing that. \n"
            ]
        },
        {
            "id": "113-2-3",
            "pair": [
                "What you are looking for is to do a full install of Kali on USB stick. In that way, you'll be able to access the whole file system, like if it was installed on your computer. To do that, follow this tutorial to install kali : http://docs.kali.org/installation/kali-linux-hard-disk-install and when choosing for a disk at step 10, select your USB drive. Make sure not to select your hard drive cause it would wipe it out. For more security, consider unplugging your hard drive before doing that. \n",
                "Actually I've noticed two folders appear on the persistent partition, 'rw' and 'work'. It seems that a properly placed persistence.conf file (with the contents '/ union' causes this. The commands you mentioned do this from the command line, but it doesn't matter how you get it there. All the updates and downloads you add and perform after this, while booted into live USB w/ persistence from grub, will appear in the rw folder. I suspect the work folder to be a place holder for an additional encrypted partition, but have found precious little documentation on setting this up and creating the proper entry in grub to boot into it, or how it is supposed to 'work'... I don't usually require that level of security. For basic USB persistence while live booting, the key is placing that text file in the root of the 3rd partition. The rest should be automatically handled by the os"
            ]
        }
    ],
    [
        {
            "id": "114-1-2",
            "pair": [
                "From what I can tell SQL 2014 Business Intelligence Edition should be somewhat cheaper than Enterprise Edition.  We currently run all our SQL servers on Azure and rent our licences monthly.  I am trying to find an Azure image that contains the BI edition, but have been unsuccessful.  \n",
                "Searching \"DataWarehousing\" in the Azure Portal gives only enterprise edition options. \n"
            ]
        },
        {
            "id": "114-2-3",
            "pair": [
                "Searching \"DataWarehousing\" in the Azure Portal gives only enterprise edition options. \n",
                "Searching \"Business Intelligence\" in the Azure Portal also gives only enterprise edition options. \n"
            ]
        },
        {
            "id": "114-3-4",
            "pair": [
                "Searching \"Business Intelligence\" in the Azure Portal also gives only enterprise edition options. \n",
                "The following link for SQL Pricing shows only Web, Standard, and Enterprise Editions. \n"
            ]
        },
        {
            "id": "114-4-5",
            "pair": [
                "The following link for SQL Pricing shows only Web, Standard, and Enterprise Editions. \n",
                "http://azure.microsoft.com/en-us/pricing/details/virtual-machines/#Sql\n"
            ]
        },
        {
            "id": "114-5-6",
            "pair": [
                "http://azure.microsoft.com/en-us/pricing/details/virtual-machines/#Sql\n",
                "Is Business Intelligence not a supported option in Azure Images?\n"
            ]
        },
        {
            "id": "114-6-7",
            "pair": [
                "Is Business Intelligence not a supported option in Azure Images?\n",
                "Unfortunately there's no BI licensing, but you have some of the BI features in Standard images.\n"
            ]
        },
        {
            "id": "114-7-8",
            "pair": [
                "Unfortunately there's no BI licensing, but you have some of the BI features in Standard images.\n",
                "SQL Server Business Intelligence in Azure Virtual Machines\n"
            ]
        },
        {
            "id": "114-8-9",
            "pair": [
                "SQL Server Business Intelligence in Azure Virtual Machines\n",
                "https://msdn.microsoft.com/library/azure/jj992719.aspx/#bkmk_supported_features\n"
            ]
        },
        {
            "id": "114-9-10",
            "pair": [
                "https://msdn.microsoft.com/library/azure/jj992719.aspx/#bkmk_supported_features\n",
                "There's also a few enterprise images optimized for DataWarehouse and Transactional Workloads."
            ]
        }
    ],
    [
        {
            "id": "115-1-2",
            "pair": [
                "Most receivers require a microcontroller on the receiving end. The XBee already has a micro on board, so you can use it to directly turn on/off a pin on the xbee. \n",
                "If you don't want to have to program some micro-controllers (e.g. arduino's), you best bet is to get some more remote-plugs. \n"
            ]
        },
        {
            "id": "115-2-3",
            "pair": [
                "If you don't want to have to program some micro-controllers (e.g. arduino's), you best bet is to get some more remote-plugs. \n",
                "If you don't want to use 220V, just crack them open, and reuse the board inside. You'll have to provide it with low dc-power yourself.\n"
            ]
        },
        {
            "id": "115-3-4",
            "pair": [
                "If you don't want to use 220V, just crack them open, and reuse the board inside. You'll have to provide it with low dc-power yourself.\n",
                "Instead of reusing the remote, you can use one of these modules to send the commands.  That way you are not limited to only 3/4 devices (depending on the remote you're using).\n"
            ]
        },
        {
            "id": "115-4-5",
            "pair": [
                "Instead of reusing the remote, you can use one of these modules to send the commands.  That way you are not limited to only 3/4 devices (depending on the remote you're using).\n",
                "Recently I got cheap RF remote controlled plug points. I hacked the remote to control buttons using raspberry pi and it works great. My question is, is there any generic RF transmitter and receiver module? Receiving side, i want few GPIO pins so that we can control multiple motors or relays. I saw xbee modules, but they are very costly. Is there any such cheap RF transmitter receiver? Please help me how to go about it."
            ]
        }
    ],
    [
        {
            "id": "116-1-2",
            "pair": [
                "I found an implementation in Clojure that's in terms of fairly elementary functions and macros (rewriters) that I can translate into other languages like Mathematica or C++ or even python with macropy.  I think this will be easier for me to deal with than the implementations in Haskell (with its deep monad libraries, which I don't want to translate into other languages [too much work]) or Scheme (with its built-in call/cc, which I can't translate into other languages [without a lot of compiler hacking or similar trouble]).\n",
                "I think you might be misreading those rewrite rules.\n"
            ]
        },
        {
            "id": "116-2-3",
            "pair": [
                "I think you might be misreading those rewrite rules.\n",
                "A call to shift will find to the nearest enclosing call to reset, regardless of how deeply nested it is. Other examples in the tutorial you linked to should confirm that.\n"
            ]
        },
        {
            "id": "116-3-4",
            "pair": [
                "A call to shift will find to the nearest enclosing call to reset, regardless of how deeply nested it is. Other examples in the tutorial you linked to should confirm that.\n",
                "The Racket Reference has a slightly different set of rewrite rules for reset/shift, that might make things a little clearer.\n"
            ]
        },
        {
            "id": "116-4-5",
            "pair": [
                "The Racket Reference has a slightly different set of rewrite rules for reset/shift, that might make things a little clearer.\n",
                "I don't speak Mathematica, but it might be possible to transcribe the rewrite rules from the Racket Reference to get what you want."
            ]
        }
    ],
    [
        {
            "id": "117-1-2",
            "pair": [
                "Yes, or at least mostly \"yes\". You still have to distinguish whether a method or property should be private by design, or if you are just not using it publicly yet. In the latter case you may ignore the warning, but in the first case, IntelliJ is most likely right.\n",
                "Simple rule: High inner, low outer connectivity. Your classes and packages should only expose the bare minimum to the outside, and keep the implementation specific details hidden.\n"
            ]
        },
        {
            "id": "117-2-3",
            "pair": [
                "Simple rule: High inner, low outer connectivity. Your classes and packages should only expose the bare minimum to the outside, and keep the implementation specific details hidden.\n",
                "When it's a magic number, ignore that hint. The IDE will warn because it's only used once, but being verbose doesn't hurt this time.\n"
            ]
        },
        {
            "id": "117-3-4",
            "pair": [
                "When it's a magic number, ignore that hint. The IDE will warn because it's only used once, but being verbose doesn't hurt this time.\n",
                "While these two are obviously magic numbers, you are being very inconsequent by deriving a lot of other magic numbers in your codebase (essentially all screen space coordinates!) based on the value of these two.\n"
            ]
        },
        {
            "id": "117-4-5",
            "pair": [
                "While these two are obviously magic numbers, you are being very inconsequent by deriving a lot of other magic numbers in your codebase (essentially all screen space coordinates!) based on the value of these two.\n",
                "Take e.g. the bounding boxes in MouseInput, every single occasion where you are drawing an UI element at a fixed location, or even the ingame object placement.\n"
            ]
        },
        {
            "id": "117-5-6",
            "pair": [
                "Take e.g. the bounding boxes in MouseInput, every single occasion where you are drawing an UI element at a fixed location, or even the ingame object placement.\n",
                "All of these should have been derived from these two constants!\n"
            ]
        },
        {
            "id": "117-6-7",
            "pair": [
                "All of these should have been derived from these two constants!\n",
                "You absolutely love writing duplicate code, don't you?\n"
            ]
        },
        {
            "id": "117-7-8",
            "pair": [
                "You absolutely love writing duplicate code, don't you?\n",
                "In every single location in your codebase where you need to handle either 2D coordinates, or 2D bounding boxes, you always chose to store each single component in an individual variable.\n"
            ]
        },
        {
            "id": "117-8-9",
            "pair": [
                "In every single location in your codebase where you need to handle either 2D coordinates, or 2D bounding boxes, you always chose to store each single component in an individual variable.\n",
                "But you didn't just duplicate the pattern of storing the components individually, you also typed the bounding box tests over an over again manually.\n"
            ]
        },
        {
            "id": "117-9-10",
            "pair": [
                "But you didn't just duplicate the pattern of storing the components individually, you also typed the bounding box tests over an over again manually.\n",
                "It's so simple to solve that, just group 2D coordinates into a Point object, and bounding boxes into Box object. The Box class should also contain the commonly used methods for test for Box with Box collisions, and Point in Box inclusions.\n"
            ]
        },
        {
            "id": "117-10-11",
            "pair": [
                "It's so simple to solve that, just group 2D coordinates into a Point object, and bounding boxes into Box object. The Box class should also contain the commonly used methods for test for Box with Box collisions, and Point in Box inclusions.\n",
                "So if the player was already dead, he actually dies only on the next collision. But until you let him play, even if the health bar goes already into the negative.\n"
            ]
        },
        {
            "id": "117-11-12",
            "pair": [
                "So if the player was already dead, he actually dies only on the next collision. But until you let him play, even if the health bar goes already into the negative.\n",
                "Well, it doesn't, since your implementation of the health bar actually hit's exactly 0. But this starts to bug out the second you allow custom damage values for the health bar.\n"
            ]
        },
        {
            "id": "117-12-13",
            "pair": [
                "Well, it doesn't, since your implementation of the health bar actually hit's exactly 0. But this starts to bug out the second you allow custom damage values for the health bar.\n",
                "You just spun off 3 threads. The one handling the input is acceptable, you don't want that one to be blocked by anything else.\n"
            ]
        },
        {
            "id": "117-13-14",
            "pair": [
                "You just spun off 3 threads. The one handling the input is acceptable, you don't want that one to be blocked by anything else.\n",
                "That stuff belongs into the actual game loop. Which you don't have in your design. It would look pretty much like this, if you had one:\n"
            ]
        },
        {
            "id": "117-14-15",
            "pair": [
                "That stuff belongs into the actual game loop. Which you don't have in your design. It would look pretty much like this, if you had one:\n",
                "Have you even counted how often you tried to repaint the panel? If you type the same command over and over again, all over the place, something smells really fishy.\n"
            ]
        },
        {
            "id": "117-15-16",
            "pair": [
                "Have you even counted how often you tried to repaint the panel? If you type the same command over and over again, all over the place, something smells really fishy.\n",
                "In this case it was the indicator, that several components which should have been managed by the GamePanel (well, actually not even that, but by the game loop!) instead took control over it.\n"
            ]
        },
        {
            "id": "117-16-17",
            "pair": [
                "In this case it was the indicator, that several components which should have been managed by the GamePanel (well, actually not even that, but by the game loop!) instead took control over it.\n",
                "If you take a look at the proposed game loop - you only call that repaint method once after all components have been updated.\n"
            ]
        },
        {
            "id": "117-17-18",
            "pair": [
                "If you take a look at the proposed game loop - you only call that repaint method once after all components have been updated.\n",
                "Eh, nope. Looks like you actually managed to bake the game state and logic into the class which was only supposed to be responsible for presentation.\n"
            ]
        },
        {
            "id": "117-18-19",
            "pair": [
                "Eh, nope. Looks like you actually managed to bake the game state and logic into the class which was only supposed to be responsible for presentation.\n",
                "Well, at least that means the setup of the game state is still in a single location?\n"
            ]
        },
        {
            "id": "117-19-20",
            "pair": [
                "Well, at least that means the setup of the game state is still in a single location?\n",
                "WTF? Why is that part of the setup suddenly part of CollisionDetector?\n"
            ]
        },
        {
            "id": "117-20-21",
            "pair": [
                "WTF? Why is that part of the setup suddenly part of CollisionDetector?\n",
                "This class is apparently strongly coupled with GamePanel. Once again an indicator that the guideline about low outer connectivity is violated.\n"
            ]
        },
        {
            "id": "117-21-22",
            "pair": [
                "This class is apparently strongly coupled with GamePanel. Once again an indicator that the guideline about low outer connectivity is violated.\n",
                "Let's take a look at \"CollisionDetector\" then. Actually, that class would make a fine container for the game state, if it wasn't for misleading naming.\n"
            ]
        },
        {
            "id": "117-22-23",
            "pair": [
                "Let's take a look at \"CollisionDetector\" then. Actually, that class would make a fine container for the game state, if it wasn't for misleading naming.\n",
                "What that class actually does, is that it handles most of the actual game logic. Do yourself a favor, and strip the stuff which doesn't belong to GamePanel from that class and move if to CollisionDetector where it belongs. Oh, and give that class a proper name which actually describes it's function.\n"
            ]
        },
        {
            "id": "117-23-24",
            "pair": [
                "What that class actually does, is that it handles most of the actual game logic. Do yourself a favor, and strip the stuff which doesn't belong to GamePanel from that class and move if to CollisionDetector where it belongs. Oh, and give that class a proper name which actually describes it's function.\n",
                "Problem with that? You managed to do this for every single instance of Bullet. Some problem with Obstacle and Spaceship.\n"
            ]
        },
        {
            "id": "117-24-25",
            "pair": [
                "Problem with that? You managed to do this for every single instance of Bullet. Some problem with Obstacle and Spaceship.\n",
                "In your simple application, the effect may be minor. You only have 10 bullets, 6 obstacles and 1 ship in your game.\n"
            ]
        },
        {
            "id": "117-25-26",
            "pair": [
                "In your simple application, the effect may be minor. You only have 10 bullets, 6 obstacles and 1 ship in your game.\n",
                "But that doesn't change the fact that you managed to keep 10 identical copies of the very same image in memory.\n"
            ]
        },
        {
            "id": "117-26-27",
            "pair": [
                "But that doesn't change the fact that you managed to keep 10 identical copies of the very same image in memory.\n",
                "This is actually not so simple to solve. The problem is, that you encapsulated resource management entirely within each instance.\n"
            ]
        },
        {
            "id": "117-27-28",
            "pair": [
                "This is actually not so simple to solve. The problem is, that you encapsulated resource management entirely within each instance.\n",
                "Solving this requires a different pattern. Treat Bullet, Obstacle and Spaceship solely as data containers, holding only the attributes unique to each instance. These containers don't know anything about being displayed.\n"
            ]
        },
        {
            "id": "117-28-29",
            "pair": [
                "Solving this requires a different pattern. Treat Bullet, Obstacle and Spaceship solely as data containers, holding only the attributes unique to each instance. These containers don't know anything about being displayed.\n",
                "Manage the resources, and also the painting, in a central location. This actually fit's very well into the GamePanel class. That class only needs to hold one copy of each texture, and the simply iterates over the list of positions at which the instances should be drawn.\n"
            ]
        },
        {
            "id": "117-29-30",
            "pair": [
                "Manage the resources, and also the painting, in a central location. This actually fit's very well into the GamePanel class. That class only needs to hold one copy of each texture, and the simply iterates over the list of positions at which the instances should be drawn.\n",
                "Alternatively, at least load the resources only once per class, and not per instance.\n"
            ]
        },
        {
            "id": "117-30-31",
            "pair": [
                "Alternatively, at least load the resources only once per class, and not per instance.\n",
                "Just to avoid confusion, the former paragraph isn't applicable for AllTimeLeaders, Menu and alike.\n"
            ]
        },
        {
            "id": "117-31-32",
            "pair": [
                "Just to avoid confusion, the former paragraph isn't applicable for AllTimeLeaders, Menu and alike.\n",
                "Abusing a public void paint(Graphics g) to pass in the render surface of the parent JPanel is a horrible hack. Especially when you then continue to render to absolute coordinates in the parents screenspace.\n"
            ]
        },
        {
            "id": "117-32-33",
            "pair": [
                "Abusing a public void paint(Graphics g) to pass in the render surface of the parent JPanel is a horrible hack. Especially when you then continue to render to absolute coordinates in the parents screenspace.\n",
                "At least have the decency to leave the layout to GamePanel. That is simply reached by additionally passing x and y offsets to paint, so that these components (at least internally) don't need to know the layout.\n"
            ]
        },
        {
            "id": "117-33-34",
            "pair": [
                "At least have the decency to leave the layout to GamePanel. That is simply reached by additionally passing x and y offsets to paint, so that these components (at least internally) don't need to know the layout.\n",
                "The clean solution would be not to let these components draw directly to the render surface, but to use separate panes instead, completely separating them from the parent component.\n"
            ]
        },
        {
            "id": "117-34-35",
            "pair": [
                "The clean solution would be not to let these components draw directly to the render surface, but to use separate panes instead, completely separating them from the parent component.\n",
                "Bonus points for wanting to support *nix systems. All gone for totally messing it up.\n"
            ]
        },
        {
            "id": "117-35-36",
            "pair": [
                "Bonus points for wanting to support *nix systems. All gone for totally messing it up.\n",
                "~/, if at the start of a path, is already the home directory. System.getProperty(\"user.home\") + \"~/.\" is not a valid path.\n"
            ]
        },
        {
            "id": "117-36-37",
            "pair": [
                "~/, if at the start of a path, is already the home directory. System.getProperty(\"user.home\") + \"~/.\" is not a valid path.\n",
                "Argh. You had it right further down in the same file. The correct syntax is File(createDataFolder(), fileName).\n"
            ]
        },
        {
            "id": "117-37-38",
            "pair": [
                "Argh. You had it right further down in the same file. The correct syntax is File(createDataFolder(), fileName).\n",
                "But not to worry too much about that single line. The whole class FileReaderAndWriter smells. For some strange reason, you were obsessed with passing around directory and file names as String, when the proper datatype would have been File.\n"
            ]
        },
        {
            "id": "117-38-39",
            "pair": [
                "But not to worry too much about that single line. The whole class FileReaderAndWriter smells. For some strange reason, you were obsessed with passing around directory and file names as String, when the proper datatype would have been File.\n",
                "And please don't use a trailing / unless you know exactly what you are doing. You don't need it when using the File class to build paths. Respectively, it doesn't even do what you think it does.\n"
            ]
        },
        {
            "id": "117-39-40",
            "pair": [
                "And please don't use a trailing / unless you know exactly what you are doing. You don't need it when using the File class to build paths. Respectively, it doesn't even do what you think it does.\n",
                "You know there are other implementations of the List interface than just LinkedList, right? You are not even using any implementation specific behavior, so that declaration is overly specific.\n"
            ]
        },
        {
            "id": "117-40-41",
            "pair": [
                "You know there are other implementations of the List interface than just LinkedList, right? You are not even using any implementation specific behavior, so that declaration is overly specific.\n",
                "When declaring the variable, only declare it to be the interface you are going to use. The specific implementation only needs to be specified when initializing it. Or not at all, when using e.g. a factory.\n"
            ]
        },
        {
            "id": "117-41-42",
            "pair": [
                "When declaring the variable, only declare it to be the interface you are going to use. The specific implementation only needs to be specified when initializing it. Or not at all, when using e.g. a factory.\n",
                "In this case, the only interface you were interested in is the List one:\n"
            ]
        },
        {
            "id": "117-42-43",
            "pair": [
                "In this case, the only interface you were interested in is the List one:\n",
                "This also goes for return types and method parameters. You should never require a specific type, if other implementations of the used interface work perfectly well either.\n"
            ]
        },
        {
            "id": "117-43-44",
            "pair": [
                "This also goes for return types and method parameters. You should never require a specific type, if other implementations of the used interface work perfectly well either.\n",
                "I'm going to stop here for now. There are still many undiscussed problems with that code base, but after resolving the already mentioned ones, the code base will already look entirely different.\n"
            ]
        },
        {
            "id": "117-44-45",
            "pair": [
                "I'm going to stop here for now. There are still many undiscussed problems with that code base, but after resolving the already mentioned ones, the code base will already look entirely different.\n",
                "I made this game which should imitate a spaceship and obstacles (planets). Move spaceship and try to hit everything.\n"
            ]
        },
        {
            "id": "117-45-46",
            "pair": [
                "I made this game which should imitate a spaceship and obstacles (planets). Move spaceship and try to hit everything.\n",
                "I will add some others features and functionality (better images or whatever).\n"
            ]
        },
        {
            "id": "117-46-47",
            "pair": [
                "I will add some others features and functionality (better images or whatever).\n",
                "My IDE (IntelliJ) gives me some warning and I try to remove them. One of them is \"access can be package-private\". I know what it means, it tries to \"narrow down\"  the access. So, according to this I should have written for example\n"
            ]
        },
        {
            "id": "117-47-48",
            "pair": [
                "My IDE (IntelliJ) gives me some warning and I try to remove them. One of them is \"access can be package-private\". I know what it means, it tries to \"narrow down\"  the access. So, according to this I should have written for example\n",
                "void myMethod() instead of public void myMethod().\n"
            ]
        },
        {
            "id": "117-48-49",
            "pair": [
                "void myMethod() instead of public void myMethod().\n",
                "The question : should I be obedient and change everything?\n"
            ]
        },
        {
            "id": "117-49-50",
            "pair": [
                "The question : should I be obedient and change everything?\n",
                "I am confused because a lot of code is written in \"bad\" way.\n"
            ]
        },
        {
            "id": "117-50-51",
            "pair": [
                "I am confused because a lot of code is written in \"bad\" way.\n",
                "Other warning is \"field can be converted to a local variable\". \n"
            ]
        },
        {
            "id": "117-51-52",
            "pair": [
                "Other warning is \"field can be converted to a local variable\". \n",
                "Don't make (private final int SOMETHING = 1231) and make this in appropiate method, right? I want to ask because some variables have to be in the first lines of class (must be accessed generally by whole class) and some in one method. For me it's cleaner when I have all variables like this in one place, but compiler tells to write some here and some in methods.\n"
            ]
        },
        {
            "id": "117-52-53",
            "pair": [
                "Don't make (private final int SOMETHING = 1231) and make this in appropiate method, right? I want to ask because some variables have to be in the first lines of class (must be accessed generally by whole class) and some in one method. For me it's cleaner when I have all variables like this in one place, but compiler tells to write some here and some in methods.\n",
                "I would like to ask if my serialization is in proper way, and reading and writing to files.\n"
            ]
        },
        {
            "id": "117-53-54",
            "pair": [
                "I would like to ask if my serialization is in proper way, and reading and writing to files.\n",
                "What are the best classes which can deal with simple wav or mp3 file? I want to add some music to that.\n"
            ]
        },
        {
            "id": "117-54-55",
            "pair": [
                "What are the best classes which can deal with simple wav or mp3 file? I want to add some music to that.\n",
                "Exceptions - I know I have to catch exceptions or add throws but what should I do with that exception at the end? Make some message like JOptionPaneor what?"
            ]
        }
    ],
    [
        {
            "id": "118-1-2",
            "pair": [
                "I've been in charge of hiring for game programmers, been an interviewer, etc. and can say that degree really doesn't matter. I've worked with electrical engineers, people without any degree, and plenty of computer science guys.\n",
                "That being said, you're still going to have to be spectacular. Personally, when I interview, I dig into algorithmic complexity, optimization, and continually probe until I find a topic the candidate doesn't know and, at that point, have them figure it out. So, you may have some catching up to do.\n"
            ]
        },
        {
            "id": "118-2-3",
            "pair": [
                "That being said, you're still going to have to be spectacular. Personally, when I interview, I dig into algorithmic complexity, optimization, and continually probe until I find a topic the candidate doesn't know and, at that point, have them figure it out. So, you may have some catching up to do.\n",
                "Remember, it's not just about knowledge. It's about thought process, as well. You need to be able to solve problems, even when they are completely out of your realm of comfort.\n"
            ]
        },
        {
            "id": "118-3-4",
            "pair": [
                "Remember, it's not just about knowledge. It's about thought process, as well. You need to be able to solve problems, even when they are completely out of your realm of comfort.\n",
                "Read this blog post. If you're afraid of what it's saying, challenge that fear with knowledge. You're going to need to know everything to get a solid job. You'll need to know more than this, as I'll expect you to know everything here AND as much related to games.\n"
            ]
        },
        {
            "id": "118-4-5",
            "pair": [
                "Read this blog post. If you're afraid of what it's saying, challenge that fear with knowledge. You're going to need to know everything to get a solid job. You'll need to know more than this, as I'll expect you to know everything here AND as much related to games.\n",
                "Write games. When I get a resume from someone who hasn't even built a side project game, I typically chuck it. I know that sounds harsh; but, I need someone who'll understand what I'm talking about when/if things are on fire. In games, there's very little time for hand holding.\n"
            ]
        },
        {
            "id": "118-5-6",
            "pair": [
                "Write games. When I get a resume from someone who hasn't even built a side project game, I typically chuck it. I know that sounds harsh; but, I need someone who'll understand what I'm talking about when/if things are on fire. In games, there's very little time for hand holding.\n",
                "Write a lot of games. You're going to need to learn about collision, scene graphs, space partitioning, networking, etc. Build games (start small) that let you understand these.\n"
            ]
        },
        {
            "id": "118-6-7",
            "pair": [
                "Write a lot of games. You're going to need to learn about collision, scene graphs, space partitioning, networking, etc. Build games (start small) that let you understand these.\n",
                "Write games with other people. You're going to need to fail, and hard. The fastest way to fail is to add more people to your team. If you can overcome these challenges, you'll learn a lot. I need people who can work with artists, designers, other engineers, and executives.\n"
            ]
        },
        {
            "id": "118-7-8",
            "pair": [
                "Write games with other people. You're going to need to fail, and hard. The fastest way to fail is to add more people to your team. If you can overcome these challenges, you'll learn a lot. I need people who can work with artists, designers, other engineers, and executives.\n",
                "I am a 19 year old who has always been interested in video & computer games. I developed the interested for game programming about three months ago and started researching on the profession. The only degrees always suggested on the internet and in books are those of computer science, physics, mathematics, & game development. BSc Information Technology has been my major for the past two years; and even though my university teaches we the I.T. students computer programming (in c++, c#, java) and offers us the opportunity to undertake some computer science courses of our choice in addition to the regular I.T. courses, I am feeling insecure about my prospects in getting into the profession. My question is: Will a game development company hire me if I exhibit good math, physics and game programming skills with an I.T. degree? If NO, will I have to obtain an MSc in a much more related course."
            ]
        }
    ],
    [
        {
            "id": "119-1-2",
            "pair": [
                "To track folder modifications you can turn on auditing on the problem locations to log the activity. \n",
                "That should help you track down the issue, the logs will be recorded in the windows security log.\n"
            ]
        },
        {
            "id": "119-2-3",
            "pair": [
                "That should help you track down the issue, the logs will be recorded in the windows security log.\n",
                "You might want to do more virus scanning etc in safe mode using msconfig and disabling all nonmicrosoft services. Then try and run the microsoft Malicious Software Removal Tool after a full virus scan, and might want to enable the windows firewall as part of your protection with fighting this issue.\n"
            ]
        },
        {
            "id": "119-3-4",
            "pair": [
                "You might want to do more virus scanning etc in safe mode using msconfig and disabling all nonmicrosoft services. Then try and run the microsoft Malicious Software Removal Tool after a full virus scan, and might want to enable the windows firewall as part of your protection with fighting this issue.\n",
                "So I have a clients server that is infected with malware. Basically any index.php file on the server was infected as well as header.php, function.php. The infection seems to be primarily wordpress sites although other types of sites reside on the account as well.\n"
            ]
        },
        {
            "id": "119-4-5",
            "pair": [
                "So I have a clients server that is infected with malware. Basically any index.php file on the server was infected as well as header.php, function.php. The infection seems to be primarily wordpress sites although other types of sites reside on the account as well.\n",
                "The hack seems to generate some type of a \"key\" in a file called \"..\" That file, even though it gets deleted, returns within in a few minutes. (probably 10 or 15 minutes). I've gone ahead and cleaned all the infected files I've found with some scripting.\n"
            ]
        },
        {
            "id": "119-5-6",
            "pair": [
                "The hack seems to generate some type of a \"key\" in a file called \"..\" That file, even though it gets deleted, returns within in a few minutes. (probably 10 or 15 minutes). I've gone ahead and cleaned all the infected files I've found with some scripting.\n",
                "Here's my question, I am presuming that if I can find the file(s) that are respawning the \"..\" file(s) back on the server, I am thinking it will lead me to the source of the infection as well. But what I'd like to know is, how can I actually track that down?\n"
            ]
        },
        {
            "id": "119-6-7",
            "pair": [
                "Here's my question, I am presuming that if I can find the file(s) that are respawning the \"..\" file(s) back on the server, I am thinking it will lead me to the source of the infection as well. But what I'd like to know is, how can I actually track that down?\n",
                "I was thinking maybe tailing the logs in SSH or something to see if that would show me something, but I am not quite sure. I know I'd like to find the infection point and get it removed before I have to do a complete reclean of the server. Right now - I'm just baby sitting respawned \"..\" files."
            ]
        }
    ],
    [
        {
            "id": "12-1-2",
            "pair": [
                "This is a confusing situation, as you have shown by including the USB pinout in your question.\n",
                "Compounding the confusion is the use of the same colours that USB uses, but for completely different functions.\n"
            ]
        },
        {
            "id": "12-2-3",
            "pair": [
                "Compounding the confusion is the use of the same colours that USB uses, but for completely different functions.\n",
                "There is a chip buried inside the USB plug that is doing the USB-UART conversion.\n"
            ]
        },
        {
            "id": "12-3-4",
            "pair": [
                "There is a chip buried inside the USB plug that is doing the USB-UART conversion.\n",
                "There is no standard for what white/green means in this situation. There is also no standard that they even use white/green.\n"
            ]
        },
        {
            "id": "12-4-5",
            "pair": [
                "There is no standard for what white/green means in this situation. There is also no standard that they even use white/green.\n",
                "You may improve your situation by making sure you buy the same part number from the same manufacturer every time.\n"
            ]
        },
        {
            "id": "12-5-6",
            "pair": [
                "You may improve your situation by making sure you buy the same part number from the same manufacturer every time.\n",
                "If you need to find out which is which, you can try connecting each wire to an LED first. Transmitting at 9600 baud will cause the LED on the TXD line to flash.\n"
            ]
        },
        {
            "id": "12-6-7",
            "pair": [
                "If you need to find out which is which, you can try connecting each wire to an LED first. Transmitting at 9600 baud will cause the LED on the TXD line to flash.\n",
                "if you want to be able to find where is +5V, GND, TX and RX pins, you need a multimeter (a voltmeter basically), 1kOhm resistor and a simple LED.\n"
            ]
        },
        {
            "id": "12-7-8",
            "pair": [
                "if you want to be able to find where is +5V, GND, TX and RX pins, you need a multimeter (a voltmeter basically), 1kOhm resistor and a simple LED.\n",
                "first you need to find the GND and +5V, most probably these are BLACK and RED, however might be any colour. connect multimeter to the GND and touch other pins starting from RED, once you see 5V on your multimeter, you've found the power pins. if you don't see 5V on the multimeter, choose another pin as GND, and measure voltage between the selected pin and other pins. repeat as necessary.\n"
            ]
        },
        {
            "id": "12-8-9",
            "pair": [
                "first you need to find the GND and +5V, most probably these are BLACK and RED, however might be any colour. connect multimeter to the GND and touch other pins starting from RED, once you see 5V on your multimeter, you've found the power pins. if you don't see 5V on the multimeter, choose another pin as GND, and measure voltage between the selected pin and other pins. repeat as necessary.\n",
                "then you may try to find the TX pin. make a probe, according to the picture:\n"
            ]
        },
        {
            "id": "12-9-10",
            "pair": [
                "then you may try to find the TX pin. make a probe, according to the picture:\n",
                "touch +5V with the probe end, the LED should become ON, otherwise you either missed the GND and +5V or the LED polarity (the longer lead should connect to the probe).\n"
            ]
        },
        {
            "id": "12-10-11",
            "pair": [
                "touch +5V with the probe end, the LED should become ON, otherwise you either missed the GND and +5V or the LED polarity (the longer lead should connect to the probe).\n",
                "from now on, you may safely ignore +5V pin, because it's not required for a proper operation of the serial port, unless you're going to power RasPi from USB port, that's generally not recommended and will eventually fail.\n"
            ]
        },
        {
            "id": "12-11-12",
            "pair": [
                "from now on, you may safely ignore +5V pin, because it's not required for a proper operation of the serial port, unless you're going to power RasPi from USB port, that's generally not recommended and will eventually fail.\n",
                "if everything works so far, connect the probe to the probable candidate to TX pin and type something into the terminal window (choose lower baud rate in the terminal settings, something like 2400, 4800 or 9600), connected to USB tty port on your PC. see if the LED will flash when you type. if nothing happens, try another possible TX candidate pin. at least one of them will make the LED flash, and the other will be RX pin.\n"
            ]
        },
        {
            "id": "12-12-13",
            "pair": [
                "if everything works so far, connect the probe to the probable candidate to TX pin and type something into the terminal window (choose lower baud rate in the terminal settings, something like 2400, 4800 or 9600), connected to USB tty port on your PC. see if the LED will flash when you type. if nothing happens, try another possible TX candidate pin. at least one of them will make the LED flash, and the other will be RX pin.\n",
                "ps. I have mistaken TX and RX pins many times in the past, with TTL serial levels it's quite difficult to do any damage, you should not worry much about that."
            ]
        }
    ],
    [
        {
            "id": "120-1-2",
            "pair": [
                "Typically what I do is I have the sound manager prevent the same sound from being played over top of each other within a small period of time.\n",
                "So let's say you have 5 towers that all would start firing at the same time (for the sake of argument, let's say the same frame).  They would all tell the sound manager to play a \"tower type X fire\" sound.  The sound manager would see this, and only play a single instance of this sound.  That sound type won't be allowed to play again until some short period of time later (a couple of frames at least, maybe close to something like a quarter of a second or so).  That'll at least prevent the sound from being really loud.\n"
            ]
        },
        {
            "id": "120-2-3",
            "pair": [
                "So let's say you have 5 towers that all would start firing at the same time (for the sake of argument, let's say the same frame).  They would all tell the sound manager to play a \"tower type X fire\" sound.  The sound manager would see this, and only play a single instance of this sound.  That sound type won't be allowed to play again until some short period of time later (a couple of frames at least, maybe close to something like a quarter of a second or so).  That'll at least prevent the sound from being really loud.\n",
                "There's some game/audio design that can go into that as well.  Keep the pace of the game slow enough and your sounds short enough that you don't have so many bullets firing on screen at a given point in time.\n"
            ]
        },
        {
            "id": "120-3-4",
            "pair": [
                "There's some game/audio design that can go into that as well.  Keep the pace of the game slow enough and your sounds short enough that you don't have so many bullets firing on screen at a given point in time.\n",
                "When you ask to play a sound, it helps to play a 'logical' sound rather than directly triggering a physical wave. Instead, leave the logical sound object to handle that part, and it can do something like playing 1 of several different versions of that sound in sequence (aka 'round-robin' samples), or picking one at random, or pitch-shifting and changing the volume on them slightly to give the impression of them sounding unique, etc.\n"
            ]
        },
        {
            "id": "120-4-5",
            "pair": [
                "When you ask to play a sound, it helps to play a 'logical' sound rather than directly triggering a physical wave. Instead, leave the logical sound object to handle that part, and it can do something like playing 1 of several different versions of that sound in sequence (aka 'round-robin' samples), or picking one at random, or pitch-shifting and changing the volume on them slightly to give the impression of them sounding unique, etc.\n",
                "Most AAA games have dedicated editors so that the audio guy can specify the different samples that go into each logical sound without needing to do any programming. The game then just loads in the sound bank that the tool generates."
            ]
        }
    ],
    [
        {
            "id": "121-1-2",
            "pair": [
                "I have already dual boot on different HDD now I want install triple boot . On both HDD has Windows 7 I want install Ubuntu 14.04 LTS if I choose Install alongside Windows 7 then other OS Windows 7 will unused?\n",
                "Triple booting may be possible. But you want to plan this. First, backup your system. Because I have done double and triple boots and there is a LOT that can go wrong. So you should do a bare metal backup of the whole partition onto an external hard disk or the cloud or whatever you like. You must, must back up. \n"
            ]
        },
        {
            "id": "121-2-3",
            "pair": [
                "Triple booting may be possible. But you want to plan this. First, backup your system. Because I have done double and triple boots and there is a LOT that can go wrong. So you should do a bare metal backup of the whole partition onto an external hard disk or the cloud or whatever you like. You must, must back up. \n",
                "But I am not sure from the question exactly what you are trying to triple boot. A good site for discussing triple boots is:\n"
            ]
        },
        {
            "id": "121-3-4",
            "pair": [
                "But I am not sure from the question exactly what you are trying to triple boot. A good site for discussing triple boots is:\n",
                "though an archive article might have to be found in the site. \n"
            ]
        },
        {
            "id": "121-4-5",
            "pair": [
                "though an archive article might have to be found in the site. \n",
                "But I'd say that rather than triple booting have you considered using Virtualbox instead? You could run an OS in a  virtual way? That would be simpler and not risk your data and bootloader and so on. Again, the dedoimedo site has articles on virtualisation. \n"
            ]
        },
        {
            "id": "121-5-6",
            "pair": [
                "But I'd say that rather than triple booting have you considered using Virtualbox instead? You could run an OS in a  virtual way? That would be simpler and not risk your data and bootloader and so on. Again, the dedoimedo site has articles on virtualisation. \n",
                "Sorry but I can't write a lot more. Triple booting 2 Linux partitions and 1 windows? Or 2 windows and 1 Linux? They would involve different considerations. "
            ]
        }
    ],
    [
        {
            "id": "122-1-2",
            "pair": [
                "If you have the enterprise version of the software (easily determined by the presence of an \"admin\" folder in the root directory structure), you can run setup /admin and create an msp file. Put the msp file in the updates directory and run setup (from a logon script or similar); the setup program will now follow whatever settings you put in the msp file.\n",
                "If you don't have an enterprise version (you get what you pay for); you have to install it manually on every computer (or do something different like AppV).\n"
            ]
        },
        {
            "id": "122-2-3",
            "pair": [
                "If you don't have an enterprise version (you get what you pay for); you have to install it manually on every computer (or do something different like AppV).\n",
                "Sorry, you are \"done\" - MS does not support MSI installation but pushes people to online script, as you found out. An \"EASY\" way does not exist - you can take WIX, write your own cusom action and push out the exchange install in it, but this is not an easy way (especially the possible rollback scenario).\n"
            ]
        },
        {
            "id": "122-3-4",
            "pair": [
                "Sorry, you are \"done\" - MS does not support MSI installation but pushes people to online script, as you found out. An \"EASY\" way does not exist - you can take WIX, write your own cusom action and push out the exchange install in it, but this is not an easy way (especially the possible rollback scenario).\n",
                "I sugest geting the (official) logon script working. The script at http://technet.microsoft.com/en-us/library/ff602181.aspx works fine for me - note that \"double click\" is not the same as starting via GPO (where it runs with elevated priviledges)."
            ]
        }
    ],
    [
        {
            "id": "123-1-2",
            "pair": [
                "Some \"monitors\" that really are TVs have braindead logic of always applying analog TV era -overscanning to digital inputs, too. Such displays can never ever display sharp image from any source. (The whole analog overscanning was an artefact of less-than-perfect tube drivers, and later lazy TV stations not bothering to render the image correctly to the edge because \"all viewers are overscanning the image\".)\n",
                "Try to look at user manual of the TV or search internet forums for tricks to get TV to accept native resolution. Technically the only correct solution is to set both graphics output of your computer and the TV to \"overscan: 0%\" setting. This may be tricky for some TVs; for example, one Samsung TV model required image source to be connected to HDMI2 connector (the TV had 4 HDMI inputs) and the connector had to be labelled \"PC\" (instead of, say \"PS3\") for the magical overscanning and image processing circuit to switch off.\n"
            ]
        },
        {
            "id": "123-2-3",
            "pair": [
                "Try to look at user manual of the TV or search internet forums for tricks to get TV to accept native resolution. Technically the only correct solution is to set both graphics output of your computer and the TV to \"overscan: 0%\" setting. This may be tricky for some TVs; for example, one Samsung TV model required image source to be connected to HDMI2 connector (the TV had 4 HDMI inputs) and the connector had to be labelled \"PC\" (instead of, say \"PS3\") for the magical overscanning and image processing circuit to switch off.\n",
                "Try going through monitor test pages at http://www.lagom.nl/lcd-test/ and see if you can achieve 1:1 native pixel mapping from your display adapter to monitor display.\n"
            ]
        },
        {
            "id": "123-3-4",
            "pair": [
                "Try going through monitor test pages at http://www.lagom.nl/lcd-test/ and see if you can achieve 1:1 native pixel mapping from your display adapter to monitor display.\n",
                "I have a monitor (with TV tuner - Philips 221T1SB), native resolution of 1920x1080, but there is an Overscan in Catalyst. It says from left to right '10%' and '0%', if i set it (all the way to the left) on 10%, i get screen with black borders (you know what i mean), but if i set it to 0%, i get a filled monitor. It does make sense, but why the hell is there such an option?\n"
            ]
        },
        {
            "id": "123-4-5",
            "pair": [
                "I have a monitor (with TV tuner - Philips 221T1SB), native resolution of 1920x1080, but there is an Overscan in Catalyst. It says from left to right '10%' and '0%', if i set it (all the way to the left) on 10%, i get screen with black borders (you know what i mean), but if i set it to 0%, i get a filled monitor. It does make sense, but why the hell is there such an option?\n",
                "I never had any AA issues before, but now, with this monitor, here they go. I can't find any solution, i have only one lead = overscan, or pretty much anything with the CCC settings.\n"
            ]
        },
        {
            "id": "123-5-6",
            "pair": [
                "I never had any AA issues before, but now, with this monitor, here they go. I can't find any solution, i have only one lead = overscan, or pretty much anything with the CCC settings.\n",
                "Yes, i almost forgot. After a bit of trying to configure the monitor with remote, i found there is a PC channel option, few other channels, and then a HDMI option. I have monitor through HDMI, but why i get black screen when i select the PC channel?\n"
            ]
        },
        {
            "id": "123-6-7",
            "pair": [
                "Yes, i almost forgot. After a bit of trying to configure the monitor with remote, i found there is a PC channel option, few other channels, and then a HDMI option. I have monitor through HDMI, but why i get black screen when i select the PC channel?\n",
                "I tried almost everything. Please guys, give me a hand. I don't want jaggies!"
            ]
        }
    ],
    [
        {
            "id": "124-1-2",
            "pair": [
                "Try running a tool like wc to see how much data is in the file.  You may be getting a sparse file with the next record at the file position just after the position where the file was when it truncated and nothing before that. Some services keep their file position and write at that position.  This does not work well when using copytruncate to rotate the log file. \n",
                "Some applications reopen their files when sent a signal, and it is common to allow logrotate to move and recreate the file as log data will continue to be written to the log file until it is reopened.   Use delaycompress when using this method.  The logrotation specification for the syslog daemon is a good example of this.  \n"
            ]
        },
        {
            "id": "124-2-3",
            "pair": [
                "Some applications reopen their files when sent a signal, and it is common to allow logrotate to move and recreate the file as log data will continue to be written to the log file until it is reopened.   Use delaycompress when using this method.  The logrotation specification for the syslog daemon is a good example of this.  \n",
                "See the man page for logrotate for a discussion on rotating logs. \n"
            ]
        },
        {
            "id": "124-3-4",
            "pair": [
                "See the man page for logrotate for a discussion on rotating logs. \n",
                "Found the issue... So I had setup my system according to this link: https://github.com/dcos/dcos-docs/blob/master/1.9/monitoring/logging/aggregating/elk.md\n"
            ]
        },
        {
            "id": "124-4-5",
            "pair": [
                "Found the issue... So I had setup my system according to this link: https://github.com/dcos/dcos-docs/blob/master/1.9/monitoring/logging/aggregating/elk.md\n",
                "There's a > missing. Their docs have since been corrected. But since I did this step once when I first installed the issue was hidden until we noticed the logs where growing and not truncating..."
            ]
        }
    ],
    [
        {
            "id": "125-1-2",
            "pair": [
                "This is indeed triggered by Parallels' \"Access Windows folders to Mac\" but not systematically. The most likely way to trigger it seems to be login into the Windows VM while that sharing is enabled. Some \"Advanced Settings\" in that sharing configuration may also be needed to reproduce.\n",
                "Once mDNSResponder has gone wild, nothing in Parallels can restore its sanity: the problem can only be solved on the Mac. Good news: there's no need to completely reboot the mac; restarting only the corresponding daemon is enough:\n"
            ]
        },
        {
            "id": "125-2-3",
            "pair": [
                "Once mDNSResponder has gone wild, nothing in Parallels can restore its sanity: the problem can only be solved on the Mac. Good news: there's no need to completely reboot the mac; restarting only the corresponding daemon is enough:\n",
                "Make sure the exit status is '0' for success using launchctl list | grep -i dns.\n"
            ]
        },
        {
            "id": "125-3-4",
            "pair": [
                "Make sure the exit status is '0' for success using launchctl list | grep -i dns.\n",
                "See https://launchd.info and man launchctl for more details.\n"
            ]
        },
        {
            "id": "125-4-5",
            "pair": [
                "See https://launchd.info and man launchctl for more details.\n",
                "We are experiencing occasional performance problems in our WIFI. With Wireshark I see a high number (currently 50/s) of MDNS responses for localhost_prl:127.0.0.1.\n"
            ]
        },
        {
            "id": "125-5-6",
            "pair": [
                "We are experiencing occasional performance problems in our WIFI. With Wireshark I see a high number (currently 50/s) of MDNS responses for localhost_prl:127.0.0.1.\n",
                "I don't think that this is normal. A MDNS response with 127.0.0.1 seem pointless to me. Additionally, the responses sometimes include the same answer multiple times (see screenshot).\n"
            ]
        },
        {
            "id": "125-6-7",
            "pair": [
                "I don't think that this is normal. A MDNS response with 127.0.0.1 seem pointless to me. Additionally, the responses sometimes include the same answer multiple times (see screenshot).\n",
                "I'm not aware of any loops in our network. I couldn't find the reason for those packages too. I only see the responses, but no requests for localhost_prl.\n"
            ]
        },
        {
            "id": "125-7-8",
            "pair": [
                "I'm not aware of any loops in our network. I couldn't find the reason for those packages too. I only see the responses, but no requests for localhost_prl.\n",
                "I appreciate any input. The traffic is constantly high volume.\n"
            ]
        },
        {
            "id": "125-8-9",
            "pair": [
                "I appreciate any input. The traffic is constantly high volume.\n",
                "P.S.: I originally posted this problem on networkengineering, but was asked to move it here instead.\n"
            ]
        },
        {
            "id": "125-9-10",
            "pair": [
                "P.S.: I originally posted this problem on networkengineering, but was asked to move it here instead.\n",
                "This seems to be a bug in Parallels Desktop 14. Disabling \"Sharing - Share Windows - Access Windows Folders from Mac\" (not sure about the exact option name, I use it in another language) and rebooting the Mac afterwards seems to solve this problem."
            ]
        }
    ],
    [
        {
            "id": "126-1-2",
            "pair": [
                "First of all you need to make your Domino accept emails for this domain. By default a Domino server only accepts mails for external domains that are listed in its Global Domain Document(s).\n",
                "You can edit several fields in the configuration document of your server. If there is only a small number of hosts (e.g. Firewall, mailsweeper, etc), that send mails to your domino for this domain, you could Add that hosts to the Allow messages only from the following Internet hosts to be sent to external Internet domains field.\n"
            ]
        },
        {
            "id": "126-2-3",
            "pair": [
                "You can edit several fields in the configuration document of your server. If there is only a small number of hosts (e.g. Firewall, mailsweeper, etc), that send mails to your domino for this domain, you could Add that hosts to the Allow messages only from the following Internet hosts to be sent to external Internet domains field.\n",
                "If you want to generally allow this domain you could edit Allow messages to be sent only to the following external Internet domains field. Please check the domino administration help for description of the fields. You find them here: Configuration Settings document - Router/SMTP - Restrictions and Controls - SMTP Inbound Controls tab\n"
            ]
        },
        {
            "id": "126-3-4",
            "pair": [
                "If you want to generally allow this domain you could edit Allow messages to be sent only to the following external Internet domains field. Please check the domino administration help for description of the fields. You find them here: Configuration Settings document - Router/SMTP - Restrictions and Controls - SMTP Inbound Controls tab\n",
                "I have a Lotus server 6.5 that has a domain d1.com that recevies mail in the lotus directory I am trying to add a second domain d2.com to lotus and setup lotus in a way that mail receved for this domain is routed to a local zimbra mail server.\n"
            ]
        },
        {
            "id": "126-4-5",
            "pair": [
                "I have a Lotus server 6.5 that has a domain d1.com that recevies mail in the lotus directory I am trying to add a second domain d2.com to lotus and setup lotus in a way that mail receved for this domain is routed to a local zimbra mail server.\n",
                "I have added a foreign SMTP domain on the Lotus server to point to the local zimbra server but the Lotus server isn't routing the incoming email \n"
            ]
        },
        {
            "id": "126-5-6",
            "pair": [
                "I have added a foreign SMTP domain on the Lotus server to point to the local zimbra server but the Lotus server isn't routing the incoming email \n",
                "I receve this error whan trying to send to the zimbra email from gmail: "
            ]
        }
    ],
    [
        {
            "id": "127-1-2",
            "pair": [
                "Your programs are always running in virtual memory all the time. (Windows' terminology in that dialog where you set the pagefile size is grossly misleading.) \n",
                "What you are asking for is the ability to force a program to be paged out. There is no way to do that directly. \n"
            ]
        },
        {
            "id": "127-2-3",
            "pair": [
                "What you are asking for is the ability to force a program to be paged out. There is no way to do that directly. \n",
                "That doesn't happen. Memory pages that are not being accessed are always available to be released for other uses (and written to disk as necessary); the OS doesn't \"try to keep pieces of every program\" in memory regardless of whether they're being accessed. If they're not being accessed, which is normally the case for private pages of an idle program, they will only stay in memory until there's pressure caused by other programs' needs. (Until then there's no point in paging them out, right?) \n"
            ]
        },
        {
            "id": "127-3-4",
            "pair": [
                "That doesn't happen. Memory pages that are not being accessed are always available to be released for other uses (and written to disk as necessary); the OS doesn't \"try to keep pieces of every program\" in memory regardless of whether they're being accessed. If they're not being accessed, which is normally the case for private pages of an idle program, they will only stay in memory until there's pressure caused by other programs' needs. (Until then there's no point in paging them out, right?) \n",
                "Through Windows XP, minimizing an app's Windows would force a working set purge, but I think XP was the last where this was true. \n"
            ]
        },
        {
            "id": "127-4-5",
            "pair": [
                "Through Windows XP, minimizing an app's Windows would force a working set purge, but I think XP was the last where this was true. \n",
                "If you really want to do this, though, you can use the VMmap tool from SysInternals. When it starts up it will ask you to select a process. Do that, then select View | Empty Working Set. \n"
            ]
        },
        {
            "id": "127-5-6",
            "pair": [
                "If you really want to do this, though, you can use the VMmap tool from SysInternals. When it starts up it will ask you to select a process. Do that, then select View | Empty Working Set. \n",
                "Note however that this only releases pages to the modified or standby page list. (And that only for pages that aren't in the working sets of any other processes.) Pages dropped to the modified list will be written to the pagefile and then moved to the standby list. Pages on the standby list are considered \"available\" but until they are repurposed for some other use they will still contain the contents from the original process. \n"
            ]
        },
        {
            "id": "127-6-7",
            "pair": [
                "Note however that this only releases pages to the modified or standby page list. (And that only for pages that aren't in the working sets of any other processes.) Pages dropped to the modified list will be written to the pagefile and then moved to the standby list. Pages on the standby list are considered \"available\" but until they are repurposed for some other use they will still contain the contents from the original process. \n",
                "The net result is exactly the same as what Windows will do if there is pressure for available memory - you're just doing it sooner, before such demands actually exist. \n"
            ]
        },
        {
            "id": "127-7-8",
            "pair": [
                "The net result is exactly the same as what Windows will do if there is pressure for available memory - you're just doing it sooner, before such demands actually exist. \n",
                "Full details are of course in the Memory Management chapter of Windows Internals by Solomon, Russinovich, and Ionescu. \n"
            ]
        },
        {
            "id": "127-8-9",
            "pair": [
                "Full details are of course in the Memory Management chapter of Windows Internals by Solomon, Russinovich, and Ionescu. \n",
                "If you have many open programs and your machine is going too high on memory/cpu utilization you might want to close a program without loosing its state (sending it to virtual memory) and then resume the program later. This is automatically done by the OS but the OS does not know how much time you want to pause your program so it eventually becomes really slow by trying to keep pieces of every program on memory and swapping other pieces to virtual memory. Manually sending a whole program to virtual memory and then retrieving it in the same state when desired would be a nice feature, is there any tool to do this? or can it be written?"
            ]
        }
    ],
    [
        {
            "id": "128-1-2",
            "pair": [
                "I cannot say for sure why RMAN behaves in this way, but as a workaround you can edit the resulting script within the same workflow in EM before the job is created:\n",
                " ALLOCATE CHANNEL disk1 DEVICE TYPE DISK FORMAT '/disk1/%d_backups/%U';\n"
            ]
        },
        {
            "id": "128-2-3",
            "pair": [
                " ALLOCATE CHANNEL disk1 DEVICE TYPE DISK FORMAT '/disk1/%d_backups/%U';\n",
                "This will ensure the backups are placed in the destination you specified regardless of RMAN settings in control file saved with CONFIGURE command.\n"
            ]
        },
        {
            "id": "128-3-4",
            "pair": [
                "This will ensure the backups are placed in the destination you specified regardless of RMAN settings in control file saved with CONFIGURE command.\n",
                "On one database, the Oracle-Suggested Backup scheduled from Enterprise Manager always ends up in the recovery area, despite RMAN configuration showing that device type disk format points elsewhere.\n"
            ]
        },
        {
            "id": "128-4-5",
            "pair": [
                "On one database, the Oracle-Suggested Backup scheduled from Enterprise Manager always ends up in the recovery area, despite RMAN configuration showing that device type disk format points elsewhere.\n",
                "As far as I can see, the scheduled backup job is simply:\n"
            ]
        },
        {
            "id": "128-5-6",
            "pair": [
                "As far as I can see, the scheduled backup job is simply:\n",
                "Asking RMAN to show all reveals that device type disk is indeed configured to store elsewhere:\n"
            ]
        },
        {
            "id": "128-6-7",
            "pair": [
                "Asking RMAN to show all reveals that device type disk is indeed configured to store elsewhere:\n",
                "If I run the script manually,  the backupset is placed at the above location, when the script is run from the job scheduler the backupset goes to the RECO group on ASM,\n"
            ]
        },
        {
            "id": "128-7-8",
            "pair": [
                "If I run the script manually,  the backupset is placed at the above location, when the script is run from the job scheduler the backupset goes to the RECO group on ASM,\n",
                "Why might Oracle still choose to dump the backupset to the db_recovery_file_dest? \n"
            ]
        },
        {
            "id": "128-8-9",
            "pair": [
                "Why might Oracle still choose to dump the backupset to the db_recovery_file_dest? \n",
                "Ultimately, how can I change the backup destination?"
            ]
        }
    ],
    [
        {
            "id": "129-1-2",
            "pair": [
                "I've been using SecondCopy (7.0.0.146) on XP for a few years now to copy important files from one hard-disk to another.\n",
                "One thing that bothers me is that it is unable to copy some files that are open. I assume Windows provides an API that allows an application to put an exclusive lock on an open file and backup utilities like SecondCopy just can't access them until they are closed.\n"
            ]
        },
        {
            "id": "129-2-3",
            "pair": [
                "One thing that bothers me is that it is unable to copy some files that are open. I assume Windows provides an API that allows an application to put an exclusive lock on an open file and backup utilities like SecondCopy just can't access them until they are closed.\n",
                "As a result, since I have to close a bunch of files/applications for SecondCopy to complete successfully, I typically don't run SecondCopy regularly like I should... which pretty much beats the whole purpose of backing up data :-/\n"
            ]
        },
        {
            "id": "129-3-4",
            "pair": [
                "As a result, since I have to close a bunch of files/applications for SecondCopy to complete successfully, I typically don't run SecondCopy regularly like I should... which pretty much beats the whole purpose of backing up data :-/\n",
                "For those of you using a similar solution to back up your important file onto a second mass storage solution...\n"
            ]
        },
        {
            "id": "129-4-5",
            "pair": [
                "For those of you using a similar solution to back up your important file onto a second mass storage solution...\n",
                "I use SyncToy from Microsoft. I am unsure about how it behaves with locked files as I have it configured just to copy my data files and I run it at night.\n"
            ]
        },
        {
            "id": "129-5-6",
            "pair": [
                "I use SyncToy from Microsoft. I am unsure about how it behaves with locked files as I have it configured just to copy my data files and I run it at night.\n",
                "You can configure multiple pairs of folders so, if you can separate between files you are still working on and those that are done or not in use at the moment (eg, Pictures, Music, ...) you can get at least those copied over and once you can close the other ones, you can copy those then.\n"
            ]
        },
        {
            "id": "129-6-7",
            "pair": [
                "You can configure multiple pairs of folders so, if you can separate between files you are still working on and those that are done or not in use at the moment (eg, Pictures, Music, ...) you can get at least those copied over and once you can close the other ones, you can copy those then.\n",
                "But try running it through open files and see how it behaves."
            ]
        }
    ],
    [
        {
            "id": "13-1-2",
            "pair": [
                "if you want to exclude the DNS problem, try acessing a webpage by IP address.\n",
                "Also, you can check that with ping command; ping hostname and then IP address of a known site. If IP responds and hostname does not, it's clearly a DNS problem. If the IP does not respond or you cannot access the webpage via IP, then you have a http/https blocking problem.\n"
            ]
        },
        {
            "id": "13-2-3",
            "pair": [
                "Also, you can check that with ping command; ping hostname and then IP address of a known site. If IP responds and hostname does not, it's clearly a DNS problem. If the IP does not respond or you cannot access the webpage via IP, then you have a http/https blocking problem.\n",
                "We have an SBS 2003 Server that has developed what I believe is a DNS fault. It is a domain server used for DNS, DHCP, file server and exchange duties. About 24hrs ago it became apparent that any internal clients, also external clients connecting via VPN were not able to display web pages. It was also noticed that the server itself was unable to display web pages. The unusual thing is that a number of services are still working such as Skype, Exchange is still sending/receiving emails for both internal and external clients, RDP and file access. So the only thing that doesn't seem to be working is displaying web pages.\n"
            ]
        },
        {
            "id": "13-3-4",
            "pair": [
                "We have an SBS 2003 Server that has developed what I believe is a DNS fault. It is a domain server used for DNS, DHCP, file server and exchange duties. About 24hrs ago it became apparent that any internal clients, also external clients connecting via VPN were not able to display web pages. It was also noticed that the server itself was unable to display web pages. The unusual thing is that a number of services are still working such as Skype, Exchange is still sending/receiving emails for both internal and external clients, RDP and file access. So the only thing that doesn't seem to be working is displaying web pages.\n",
                "The steps already taken in an attempt to resolve this are Winsock repair, checked Forward lookup, replaced router. None with success. "
            ]
        }
    ],
    [
        {
            "id": "130-1-2",
            "pair": [
                "Since the scope of the question was to perform an \"initial\" hystorical migration of mailbox, here are different brute force approaches: move emails from old IMAP to new inbox. From then on, fetchmail daemon will work fine with new mail\n",
                "Synchronize both mailboxes, select all (or groups of) emails and drag&drop or cut&paste them into the new Inbox. This will take some time and manual work, but can do none or little damage, especially if you move emails in groups\n"
            ]
        },
        {
            "id": "130-2-3",
            "pair": [
                "Synchronize both mailboxes, select all (or groups of) emails and drag&drop or cut&paste them into the new Inbox. This will take some time and manual work, but can do none or little damage, especially if you move emails in groups\n",
                "Since I have access to the server, a tool such as imap2maildir can dump an IMAP box into a Maildir compatible box. Merging the original Maildir with the new one initializes message collection correctly\n"
            ]
        },
        {
            "id": "130-3-4",
            "pair": [
                "Since I have access to the server, a tool such as imap2maildir can dump an IMAP box into a Maildir compatible box. Merging the original Maildir with the new one initializes message collection correctly\n",
                "There are plenties of tools made for IMAP migration/synchronization. There are even online tools, to which you must provide your box password (for your security you may change the password to a temporary one during migration), and you must trust the service not to keep your personal data.\n"
            ]
        },
        {
            "id": "130-4-5",
            "pair": [
                "There are plenties of tools made for IMAP migration/synchronization. There are even online tools, to which you must provide your box password (for your security you may change the password to a temporary one during migration), and you must trust the service not to keep your personal data.\n",
                "It appears that the date exposed to the user by Windows Mail does not come from the Date header field, but rather from the latest Received field (Received fields are primarily meant to trace a message through delivery gateways, as mentioned in RFC5321 and RFC5322).\n"
            ]
        },
        {
            "id": "130-5-6",
            "pair": [
                "It appears that the date exposed to the user by Windows Mail does not come from the Date header field, but rather from the latest Received field (Received fields are primarily meant to trace a message through delivery gateways, as mentioned in RFC5321 and RFC5322).\n",
                "When fetchmail delivers a message, even with the mda option (not using SMTP), by default it adds its own Received field with the current date and time.\n"
            ]
        },
        {
            "id": "130-6-7",
            "pair": [
                "When fetchmail delivers a message, even with the mda option (not using SMTP), by default it adds its own Received field with the current date and time.\n",
                "Fortunately it provides an option to disable this:\n"
            ]
        },
        {
            "id": "130-7-8",
            "pair": [
                "Fortunately it provides an option to disable this:\n",
                "I believe you want to combine this option with a non-SMTP delivery, for instance with --mda /usr/lib/dovecot/deliver. Otherwise the SMTP server would probably add its own Received field after fetchmail."
            ]
        }
    ],
    [
        {
            "id": "131-1-2",
            "pair": [
                "I tried adding additional info to the answer by sphilp but that edit got rejected because some people seem to think it's incorrect. I can assure you: it is not.\n",
                "The Registered Servers functionality in SSMS can be used to connect to several servers in just a couple of clicks.  If you group the servers together by creating a New Server Group under the Local Server Groups node, you'll be able to right-click that group and select Object Explorer. This will open all servers in ... Object Explorer!\n"
            ]
        },
        {
            "id": "131-2-3",
            "pair": [
                "The Registered Servers functionality in SSMS can be used to connect to several servers in just a couple of clicks.  If you group the servers together by creating a New Server Group under the Local Server Groups node, you'll be able to right-click that group and select Object Explorer. This will open all servers in ... Object Explorer!\n",
                "More details: SSMS: Connect To Several Servers In One Click (Okay, Two)\n"
            ]
        },
        {
            "id": "131-3-4",
            "pair": [
                "More details: SSMS: Connect To Several Servers In One Click (Okay, Two)\n",
                "You can even influence the order in which the connections are opened (alphabetically).\n"
            ]
        },
        {
            "id": "131-4-5",
            "pair": [
                "You can even influence the order in which the connections are opened (alphabetically).\n",
                "If you use the SSMSBoost plugin, then you can set any of your preferred connections to \"Connect object explorer at startup\".\n"
            ]
        },
        {
            "id": "131-5-6",
            "pair": [
                "If you use the SSMSBoost plugin, then you can set any of your preferred connections to \"Connect object explorer at startup\".\n",
                "This option can be found on the SSMSBoost->Settings->Preferred Connections->List page of the connection.\n"
            ]
        },
        {
            "id": "131-6-7",
            "pair": [
                "This option can be found on the SSMSBoost->Settings->Preferred Connections->List page of the connection.\n",
                "Since I like having the master databases of all the instances I routinely work with at the top of my preferred list, I just enable this option for the ones I want to always have available in the Object Explorer."
            ]
        }
    ],
    [
        {
            "id": "132-1-2",
            "pair": [
                "This is what I would start looking into at this point.\n",
                "If you have another instance of SQL Server running on your machine on the same port.  In my experience; having run SQL Server and then have installed Visual Studios where an instance of SQL is installed , and it become a race condition on who owns the port.\n"
            ]
        },
        {
            "id": "132-2-3",
            "pair": [
                "If you have another instance of SQL Server running on your machine on the same port.  In my experience; having run SQL Server and then have installed Visual Studios where an instance of SQL is installed , and it become a race condition on who owns the port.\n",
                "Enter your services (Crtl R)  for the run box type services.msc to load the services manager.\n"
            ]
        },
        {
            "id": "132-3-4",
            "pair": [
                "Enter your services (Crtl R)  for the run box type services.msc to load the services manager.\n",
                "From there locate SQL Server and disable the service you are not using and restart the desired SQL Server and then try to connect.\n"
            ]
        },
        {
            "id": "132-4-5",
            "pair": [
                "From there locate SQL Server and disable the service you are not using and restart the desired SQL Server and then try to connect.\n",
                "I just installed MS SQL Server 2014 a couple days ago and it was working fine until today. I attempted to connect using the Object Viewer and I got the error message \n"
            ]
        },
        {
            "id": "132-5-6",
            "pair": [
                "I just installed MS SQL Server 2014 a couple days ago and it was working fine until today. I attempted to connect using the Object Viewer and I got the error message \n",
                " A connection was successfully established with the server, but then an error occurred during the pre-login handshake.(provider: Shared Memory Provider, error: 0 - Mo process is on the other end of the pipe.) (Microsoft SQL Server, Error: 233)\n"
            ]
        },
        {
            "id": "132-6-7",
            "pair": [
                " A connection was successfully established with the server, but then an error occurred during the pre-login handshake.(provider: Shared Memory Provider, error: 0 - Mo process is on the other end of the pipe.) (Microsoft SQL Server, Error: 233)\n",
                "The consensus seems to be that the issue is caused either by Named Pipes being disabled in my SQL Server Configuration Manager or that Named Pipes come before TCP/IP in the protocol order. I have made sure that my protocol order is appropriate. I have it set to Shared Memory first, TCP/IP Second and lastly Named Pipes. Another source said that my port might be blocked but I already have a working rule on my firewall to allow access to the default port which is 1433 in my case. \n"
            ]
        },
        {
            "id": "132-7-8",
            "pair": [
                "The consensus seems to be that the issue is caused either by Named Pipes being disabled in my SQL Server Configuration Manager or that Named Pipes come before TCP/IP in the protocol order. I have made sure that my protocol order is appropriate. I have it set to Shared Memory first, TCP/IP Second and lastly Named Pipes. Another source said that my port might be blocked but I already have a working rule on my firewall to allow access to the default port which is 1433 in my case. \n",
                "Another source said that I need to click the Allow Remote Connections under server properties in SQL Serer Server Manager but every time I right-click and click properties the program crashes... "
            ]
        }
    ],
    [
        {
            "id": "133-1-2",
            "pair": [
                "I'm studying /usr/lib/php/sessionclean at the moment. There is one thing I don't fully understand. What is the purpose of touching all opened session files before cleaning up? I understand that this prevents the deletion of expired sessions that are currently open but have not yet been written to. But isn't this still a race condition? What if a PHP process opens an expired session file between the touch and the delete commands?\n",
                "Session files that are older than the maximum lifetime, at the time the find command evaluates them,  will be deleted.\n"
            ]
        },
        {
            "id": "133-2-3",
            "pair": [
                "Session files that are older than the maximum lifetime, at the time the find command evaluates them,  will be deleted.\n",
                "Files are updated at the latest when the PHP script finishes execution. However, perhaps it is not really fair to kill a session for being idle while it is executing.  Thus, the touch updates the modify time on session files which have a php process currently executing. \n"
            ]
        },
        {
            "id": "133-3-4",
            "pair": [
                "Files are updated at the latest when the PHP script finishes execution. However, perhaps it is not really fair to kill a session for being idle while it is executing.  Thus, the touch updates the modify time on session files which have a php process currently executing. \n",
                "Yes, there is a race condition of a php process starting after the touch, resuming a too old session, and getting deleted because it did not finish before the find did garbage collection. The session already existed for its full lifetime, probably several minutes. Missing an extension in a fraction of a second window isn't a big deal.\n"
            ]
        },
        {
            "id": "133-4-5",
            "pair": [
                "Yes, there is a race condition of a php process starting after the touch, resuming a too old session, and getting deleted because it did not finish before the find did garbage collection. The session already existed for its full lifetime, probably several minutes. Missing an extension in a fraction of a second window isn't a big deal.\n",
                "The alternative, PHP's built in implementation, has a 1% or so chance of running garbage collection on execution. For low volume sites, it might not trigger reliably. \n"
            ]
        },
        {
            "id": "133-5-6",
            "pair": [
                "The alternative, PHP's built in implementation, has a 1% or so chance of running garbage collection on execution. For low volume sites, it might not trigger reliably. \n",
                "Plus, the external script allows locking down the security on the session directory, which is why Debian maintainers did it that way."
            ]
        }
    ],
    [
        {
            "id": "134-1-2",
            "pair": [
                "It's solid. I've been running Windows XP/7 on a mac mini for years. If you have enough hardware resources (CPU, Disk, RAM) it's really nothing to worry about.\n",
                "Macs are more expensive in terms of price the quality is well worth it. Macs are better (quality) than OEM value-added desktop manufacturers (Dell, HP, etc.). I tend to think of the mac as a better alternative to a PC as macs can have a foothold in two worlds (windows and osx) where as windows pcs cannot (excluding hackintoshes of course).\n"
            ]
        },
        {
            "id": "134-2-3",
            "pair": [
                "Macs are more expensive in terms of price the quality is well worth it. Macs are better (quality) than OEM value-added desktop manufacturers (Dell, HP, etc.). I tend to think of the mac as a better alternative to a PC as macs can have a foothold in two worlds (windows and osx) where as windows pcs cannot (excluding hackintoshes of course).\n",
                "If you're debating on whether it's a good purchase or not, you should approach it more functionally and see if the programs you need will run with windows via bootcamp OR vmware/parallels. \n"
            ]
        },
        {
            "id": "134-3-4",
            "pair": [
                "If you're debating on whether it's a good purchase or not, you should approach it more functionally and see if the programs you need will run with windows via bootcamp OR vmware/parallels. \n",
                "From my personal experience, using Windows on Mac are just fine both in Bootcamp and in VMWare.\n"
            ]
        },
        {
            "id": "134-4-5",
            "pair": [
                "From my personal experience, using Windows on Mac are just fine both in Bootcamp and in VMWare.\n",
                "You might want to take a look here, where apple lists the requirements for running windows through bootcamp.\n"
            ]
        },
        {
            "id": "134-5-6",
            "pair": [
                "You might want to take a look here, where apple lists the requirements for running windows through bootcamp.\n",
                "Basically, if your OS can run Lion 10.7, and you have 16+GB of hard drive space (this is the absolute minimum, but I would recommend more since you want to install applications on the windows partition)."
            ]
        }
    ],
    [
        {
            "id": "135-1-2",
            "pair": [
                "I would recommend - to any still facing this problem - to backup the Easy Settings version and find the proper installer before.\n",
                "it works adapting it to W7 compatibility once reinstalled after the needed OS upgrades.\n"
            ]
        },
        {
            "id": "135-2-3",
            "pair": [
                "it works adapting it to W7 compatibility once reinstalled after the needed OS upgrades.\n",
                "The new Samsung Settings do not work properly for every Fn key.\n"
            ]
        },
        {
            "id": "135-3-4",
            "pair": [
                "The new Samsung Settings do not work properly for every Fn key.\n",
                "If one of you is still fighting to get Easy Settings to work on Windows 10, here is what I did to finally get backlit keyboard back on a Samsung Series 9 NP900X3C-A03FR after installing Windows 10 version 1709 (Fall Creator Update) :\n"
            ]
        },
        {
            "id": "135-4-5",
            "pair": [
                "If one of you is still fighting to get Easy Settings to work on Windows 10, here is what I did to finally get backlit keyboard back on a Samsung Series 9 NP900X3C-A03FR after installing Windows 10 version 1709 (Fall Creator Update) :\n",
                "- You will want to get version 2.1 of Samsung Settings (new name of this utility).\n"
            ]
        },
        {
            "id": "135-5-6",
            "pair": [
                "- You will want to get version 2.1 of Samsung Settings (new name of this utility).\n",
                "- First install the latest version of Samsung Software Update :\n"
            ]
        },
        {
            "id": "135-6-7",
            "pair": [
                "- First install the latest version of Samsung Software Update :\n",
                "http://downloadcenter.samsung.com/content/SW/201305/20130521090434453/SWUpdate_2.1.15.1.ZIP\n"
            ]
        },
        {
            "id": "135-7-8",
            "pair": [
                "http://downloadcenter.samsung.com/content/SW/201305/20130521090434453/SWUpdate_2.1.15.1.ZIP\n",
                "- Launch Software Update and type in your Samsung model number in the upper right corner.\n"
            ]
        },
        {
            "id": "135-8-9",
            "pair": [
                "- Launch Software Update and type in your Samsung model number in the upper right corner.\n",
                "- Select Windows 10: you will get a list of software and utilities to download.\n"
            ]
        },
        {
            "id": "135-9-10",
            "pair": [
                "- Select Windows 10: you will get a list of software and utilities to download.\n",
                "- You will need Samsung Settings for the Fn keys for backlit keyboard and Display Manager for the screen backlight control.\n"
            ]
        },
        {
            "id": "135-10-11",
            "pair": [
                "- You will need Samsung Settings for the Fn keys for backlit keyboard and Display Manager for the screen backlight control.\n",
                "- The downloaded software goes into a folder named SystemSoftware.\n"
            ]
        },
        {
            "id": "135-11-12",
            "pair": [
                "- The downloaded software goes into a folder named SystemSoftware.\n",
                "- Install the software in Compatibility Mode and restart computer. Works!"
            ]
        }
    ],
    [
        {
            "id": "136-1-2",
            "pair": [
                "I would go with Fiber, there is the possibility that in the future we might hit a dead end in how much speed can be had from copper. I mean twisted pair is as old as the telephone, now sure that means that it has had plenty of time to mature, but there exists the possibility of multiple colors running on the same fiber.\n",
                "Most of the key issues have been mentioned - Fiber is easier to handle, weighs less, makes for a much neater environment and has better range (generally). As LapTop006 mentioned one consequence of this is that the port density on switches is generally higher for fiber, you'll find that many modular switches support a higher SFP+ port count than either CX4 (Twin-AX) or 10GBase-T (STP).\n"
            ]
        },
        {
            "id": "136-2-3",
            "pair": [
                "Most of the key issues have been mentioned - Fiber is easier to handle, weighs less, makes for a much neater environment and has better range (generally). As LapTop006 mentioned one consequence of this is that the port density on switches is generally higher for fiber, you'll find that many modular switches support a higher SFP+ port count than either CX4 (Twin-AX) or 10GBase-T (STP).\n",
                "One other key difference is that at the moment 10GBase-T consumes more power per port than CX4 and in turn CX4 consumes more power than SFP+. That may not be of significance if you are only connecting up a few links but if you are building out a significant infrastructure then this may be something you need to pay attention to."
            ]
        }
    ],
    [
        {
            "id": "137-1-2",
            "pair": [
                "(I'm assuming for the purposes of this question you are using Kali Linux for security related functions, and you are not just trying to prevent casual access - as Kali Linux is, AFAIK a pen testing platform)\n",
                "If you want to hide them from root so its not possible for root to read them, you will need to encrypt the Windows data - if the disk is in the machine its possible for root to read it - even if it means reading the raw partition data.  Thus the only way to protect it would be to remove the disk, or scramble the bytes as a decent encryption system would do.\n"
            ]
        },
        {
            "id": "137-2-3",
            "pair": [
                "If you want to hide them from root so its not possible for root to read them, you will need to encrypt the Windows data - if the disk is in the machine its possible for root to read it - even if it means reading the raw partition data.  Thus the only way to protect it would be to remove the disk, or scramble the bytes as a decent encryption system would do.\n",
                "If you have specific documents you need to hide, but hide them in such a way that it can't be proven you are hiding them, look at Veracrypt and \"Plausable Deniability/hidden volumes\".  [ I have not used this program, I did use its predecessor truecrypt though ]\n"
            ]
        },
        {
            "id": "137-3-4",
            "pair": [
                "If you have specific documents you need to hide, but hide them in such a way that it can't be proven you are hiding them, look at Veracrypt and \"Plausable Deniability/hidden volumes\".  [ I have not used this program, I did use its predecessor truecrypt though ]\n",
                "I've currently got Kali-Linux installed and running on a USB drive. I recently realised that I can access all the files from the hard drive on the Windows and OSX partitions from file browser. \n"
            ]
        },
        {
            "id": "137-4-5",
            "pair": [
                "I've currently got Kali-Linux installed and running on a USB drive. I recently realised that I can access all the files from the hard drive on the Windows and OSX partitions from file browser. \n",
                "I would like to know if there is a way to lock these away so root on Kali cannot access them. "
            ]
        }
    ],
    [
        {
            "id": "138-1-2",
            "pair": [
                "Microsoft IT: A Case Study on \u201cHekaton\u201d against RPM \u2013 SQL Server 2014 CTP1 seems to demonstrate the syntax,\n",
                "Hekaton was the internal Microsoft project that references the In-Memory OLTP feature, starting with SQL 2014. Some people still use the word Hekaton generically, to describe the In-Memory feature in any version of SQL server (2014, 2016, 2017). \n"
            ]
        },
        {
            "id": "138-2-3",
            "pair": [
                "Hekaton was the internal Microsoft project that references the In-Memory OLTP feature, starting with SQL 2014. Some people still use the word Hekaton generically, to describe the In-Memory feature in any version of SQL server (2014, 2016, 2017). \n",
                "In-Memory OLTP is a feature that stands alone - it is not part of any other feature, i.e. Analysis Services. \n"
            ]
        },
        {
            "id": "138-3-4",
            "pair": [
                "In-Memory OLTP is a feature that stands alone - it is not part of any other feature, i.e. Analysis Services. \n",
                "There is tons of info out there on the web about In-Memory OLTP, but I would caution you to avoid anything that references SQL 2014 (as you have above), because that was essentially v1.0, and much has changed since then. \n"
            ]
        },
        {
            "id": "138-4-5",
            "pair": [
                "There is tons of info out there on the web about In-Memory OLTP, but I would caution you to avoid anything that references SQL 2014 (as you have above), because that was essentially v1.0, and much has changed since then. \n",
                "First - have you proven through a valid POC that your workload is likely to benefit from using In-Memory OLTP? \n"
            ]
        },
        {
            "id": "138-5-6",
            "pair": [
                "First - have you proven through a valid POC that your workload is likely to benefit from using In-Memory OLTP? \n",
                "I have blogged extensively about In-Memory OLTP, but you might be better off starting with the documentation, here:\n"
            ]
        },
        {
            "id": "138-6-7",
            "pair": [
                "I have blogged extensively about In-Memory OLTP, but you might be better off starting with the documentation, here:\n",
                "https://docs.microsoft.com/en-us/sql/relational-databases/in-memory-oltp/in-memory-oltp-in-memory-optimization"
            ]
        }
    ],
    [
        {
            "id": "139-1-2",
            "pair": [
                "Izzy's answer is fine if you don't care that the Administrators group will effectively be locked out of future changes from the local machine.  This will also wipe out any groups that were already members of the Administrators group before the policy setting was applied.\n",
                "However, you can use the same policy setting in a slightly different way to bypass those annoyances (assuming you even consider them annoyances).\n"
            ]
        },
        {
            "id": "139-2-3",
            "pair": [
                "However, you can use the same policy setting in a slightly different way to bypass those annoyances (assuming you even consider them annoyances).\n",
                "It's a subtle but important difference in the way the two sections work.  Members of this group effectively works out to be \"Group A will only ever contain Groups X, Y, and Z\". This group is a member of effectively works out to be \"Make sure Group A is a member of Groups X, Y, and Z\".\n"
            ]
        },
        {
            "id": "139-3-4",
            "pair": [
                "It's a subtle but important difference in the way the two sections work.  Members of this group effectively works out to be \"Group A will only ever contain Groups X, Y, and Z\". This group is a member of effectively works out to be \"Make sure Group A is a member of Groups X, Y, and Z\".\n",
                "Once you've set policy with Members of this group, the only thing that can modify the group's membership is an overriding policy object that also uses Members of this group or any other policy using This group is a member of.\n"
            ]
        },
        {
            "id": "139-4-5",
            "pair": [
                "Once you've set policy with Members of this group, the only thing that can modify the group's membership is an overriding policy object that also uses Members of this group or any other policy using This group is a member of.\n",
                "You say adding new hires is what's a hassle, but shouldn't it be adding new tablets that would be a hassle?\n"
            ]
        },
        {
            "id": "139-5-6",
            "pair": [
                "You say adding new hires is what's a hassle, but shouldn't it be adding new tablets that would be a hassle?\n",
                "Have a domain security group that contains all the users that should be administrators on the tablet PCs (i.e. TabletAdministrators).\n"
            ]
        },
        {
            "id": "139-6-7",
            "pair": [
                "Have a domain security group that contains all the users that should be administrators on the tablet PCs (i.e. TabletAdministrators).\n",
                "On each tablet, add that group to the Administrators group.\n"
            ]
        },
        {
            "id": "139-7-8",
            "pair": [
                "On each tablet, add that group to the Administrators group.\n",
                "Whether this is the proper technique or not, I don't know.  It's just the first idea that comes to me on how to implement."
            ]
        }
    ],
    [
        {
            "id": "14-1-2",
            "pair": [
                "According to one user here: http://neosmart.net/blog/2008/download-windows-vista-x64-recovery-disc/\n",
                "A vista recovery disk can properly repair bootloader on 2008.  The repair disk is available for download on the linked to page.  \n"
            ]
        },
        {
            "id": "14-2-3",
            "pair": [
                "A vista recovery disk can properly repair bootloader on 2008.  The repair disk is available for download on the linked to page.  \n",
                "As far as getting XP to install, I would hide the 2008 partition with whatever disk mgmt utility and just install.  Then put the necessary lines in the boot.ini file on the 2k8 server\n"
            ]
        },
        {
            "id": "14-3-4",
            "pair": [
                "As far as getting XP to install, I would hide the 2008 partition with whatever disk mgmt utility and just install.  Then put the necessary lines in the boot.ini file on the 2k8 server\n",
                "Ok I think I think I've much work to do, I want to dual boot windows server and xp, but I've linux and grub installed. But before I begin I need to ask some questions.\n"
            ]
        },
        {
            "id": "14-4-5",
            "pair": [
                "Ok I think I think I've much work to do, I want to dual boot windows server and xp, but I've linux and grub installed. But before I begin I need to ask some questions.\n",
                "My linux distro is on an external hard drive, and windows is on my main hard drive.\n"
            ]
        },
        {
            "id": "14-5-6",
            "pair": [
                "My linux distro is on an external hard drive, and windows is on my main hard drive.\n",
                "I ultimately want to remove grub and linux from my system and replace linux with windows xp.\n"
            ]
        },
        {
            "id": "14-6-7",
            "pair": [
                "I ultimately want to remove grub and linux from my system and replace linux with windows xp.\n",
                "The problem is I don't have the repair disk for windows server, and thus I cannot repair the bootloader. I cannot replace linux with xp before grub has been removed because if I do grub will complain about not being able to find the operating systems, and will make me unable to boot to windows. \n"
            ]
        },
        {
            "id": "14-7-8",
            "pair": [
                "The problem is I don't have the repair disk for windows server, and thus I cannot repair the bootloader. I cannot replace linux with xp before grub has been removed because if I do grub will complain about not being able to find the operating systems, and will make me unable to boot to windows. \n",
                "So question 1: How to repair my bootloader so I can dual boot windows server and windows xp? or how to replace linux with windows xp in my grub menu.\n"
            ]
        },
        {
            "id": "14-8-9",
            "pair": [
                "So question 1: How to repair my bootloader so I can dual boot windows server and windows xp? or how to replace linux with windows xp in my grub menu.\n",
                "Like I said I've windows server 2008 on my main hard drive. I'm not sure but I thought that windows xp didn't feature partition selection? I thought win xp was a blind missile that just installed itself wherever it could. Am I right?"
            ]
        }
    ],
    [
        {
            "id": "140-1-2",
            "pair": [
                "I'd like to design a settings class thread save. The settings have 2 attributes: String x and int y and should provide listener functionality to notify listener about changes. The problem is, how to make it thread safe.\n",
                "x and y are acess controlled, becouse getX, getY uses the read lock and setX, setY uses the write lock.\n"
            ]
        },
        {
            "id": "140-2-3",
            "pair": [
                "x and y are acess controlled, becouse getX, getY uses the read lock and setX, setY uses the write lock.\n",
                "The interesting part is the copyFrom method. It must be atomic, i. e. x and y must set atomically. To get it atomically, I've wrapped it in a lock-unlock call:\n"
            ]
        },
        {
            "id": "140-3-4",
            "pair": [
                "The interesting part is the copyFrom method. It must be atomic, i. e. x and y must set atomically. To get it atomically, I've wrapped it in a lock-unlock call:\n",
                "The problem is, that the listeners methods xChanged, yChanged are called so external code is executed. What about the listener call getY in xChanged?:\n"
            ]
        },
        {
            "id": "140-4-5",
            "pair": [
                "The problem is, that the listeners methods xChanged, yChanged are called so external code is executed. What about the listener call getY in xChanged?:\n",
                "The write lock holds (from copyFrom), so the listener thread is blocked to aqquire the read lock (from getX). The result is a dead lock I think.\n"
            ]
        },
        {
            "id": "140-5-6",
            "pair": [
                "The write lock holds (from copyFrom), so the listener thread is blocked to aqquire the read lock (from getX). The result is a dead lock I think.\n",
                "Another possible failure is, that the listener can throw an exception. When an exception is not handled, copyFrom returns abrupt without releasing the write lock. If is it handled (via try-catch), what should I do with this exception?\n"
            ]
        },
        {
            "id": "140-6-7",
            "pair": [
                "Another possible failure is, that the listener can throw an exception. When an exception is not handled, copyFrom returns abrupt without releasing the write lock. If is it handled (via try-catch), what should I do with this exception?\n",
                "Are there best practices to handle such stateful classes with listener functionality (I have the same problem for a finite state machine etc.)?\n"
            ]
        },
        {
            "id": "140-7-8",
            "pair": [
                "Are there best practices to handle such stateful classes with listener functionality (I have the same problem for a finite state machine etc.)?\n",
                "Java isn't my first language, so bear with me if I confuse the syntax a bit. I agree that your copyFrom is not atomic in that the changes are externally visible before the copy is complete. I would also say that it isn't atomic because the origin is not locked and so could change while the copy occurs. Yes, I believe this is a possible point of deadlock - in general I believe it's best to always avoid firing events under a lock for this very reason. \n"
            ]
        },
        {
            "id": "140-8-9",
            "pair": [
                "Java isn't my first language, so bear with me if I confuse the syntax a bit. I agree that your copyFrom is not atomic in that the changes are externally visible before the copy is complete. I would also say that it isn't atomic because the origin is not locked and so could change while the copy occurs. Yes, I believe this is a possible point of deadlock - in general I believe it's best to always avoid firing events under a lock for this very reason. \n",
                "If you just want to ensure that copyFrom is atomic in that sense that no intermediate state of the copy is visible externally, then you could do like this:\n"
            ]
        },
        {
            "id": "140-9-10",
            "pair": [
                "If you just want to ensure that copyFrom is atomic in that sense that no intermediate state of the copy is visible externally, then you could do like this:\n",
                "In terms of exception safety, notice that I follow every lock operation with a try..finally and a single unlock in the finally. This makes the code clear, and exception safe with regards to unlocking. In terms of exception safety regarding the internal consistency of the combined state of X and Y (for example if Y is always meant to be the string representation of X or something), you would perform consistency checks before any mutations but after entering the lock.\n"
            ]
        },
        {
            "id": "140-10-11",
            "pair": [
                "In terms of exception safety, notice that I follow every lock operation with a try..finally and a single unlock in the finally. This makes the code clear, and exception safe with regards to unlocking. In terms of exception safety regarding the internal consistency of the combined state of X and Y (for example if Y is always meant to be the string representation of X or something), you would perform consistency checks before any mutations but after entering the lock.\n",
                "A general tip from my experience is to keep locks as short and simple as possible, not too many calls and calculations, and very simple flow control. You want to get the information you need to perform the full mutation, then lock, perform the mutation leaving the object in a perfectly consistent state at the end, and unlock asap. You can follow the same pattern for pretty much anything.\n"
            ]
        },
        {
            "id": "140-11-12",
            "pair": [
                "A general tip from my experience is to keep locks as short and simple as possible, not too many calls and calculations, and very simple flow control. You want to get the information you need to perform the full mutation, then lock, perform the mutation leaving the object in a perfectly consistent state at the end, and unlock asap. You can follow the same pattern for pretty much anything.\n",
                "The part where I said there could be \"consistency checking here\" would be important if the combination of input and previous state does not make a valid mutation. For example in the case of a state machine you may want to verify that the state transition is valid and, if it isn't, throw an exception or simply return false or something (which are both safe at that point in the code). You can't do that before the lock because your checks may be working with data that changes under your feet, and you can't do it after changing part of the combined state because then aborting the mutation part way will leave the object in an inconsistent state.\n"
            ]
        },
        {
            "id": "140-12-13",
            "pair": [
                "The part where I said there could be \"consistency checking here\" would be important if the combination of input and previous state does not make a valid mutation. For example in the case of a state machine you may want to verify that the state transition is valid and, if it isn't, throw an exception or simply return false or something (which are both safe at that point in the code). You can't do that before the lock because your checks may be working with data that changes under your feet, and you can't do it after changing part of the combined state because then aborting the mutation part way will leave the object in an inconsistent state.\n",
                "I would also urge you to think clearly about the reason for the copy being atomic. Specifically, are X and Y coupled to each other in such a way that the object would contain inconsistencies if only X or only the Y was copied or changed? Think about the following:"
            ]
        }
    ],
    [
        {
            "id": "141-1-2",
            "pair": [
                "In all previous versions of Windows, the screen that appears when a user locks the computer AFTER logging in is different than the screen that appears when the user has not yet logged on or has logged off. In Windows 10, both screens look exactly the same, and if the user changes the so-called lock screen (the image that appears BEFORE the login screen or the screen to unlock the computer), it changes for BOTH logging in AND unlocking a computer that is logged in.\n",
                "The actual state of the computer in the two cases is actually different. Many programs that are started during a login session continue to run when the computer is locked, and even Scheduled Tasks that are scheduled to start at a time that the computer is logged in and locked WILL start, but if the computer is logged off, they will not.\n"
            ]
        },
        {
            "id": "141-2-3",
            "pair": [
                "The actual state of the computer in the two cases is actually different. Many programs that are started during a login session continue to run when the computer is locked, and even Scheduled Tasks that are scheduled to start at a time that the computer is logged in and locked WILL start, but if the computer is logged off, they will not.\n",
                "Is there any way to change the appearances of either of the two screen that ask for a username and password (the unlock screen and the login screen), so that someone can look at the screen and determine whether it is logged off or locked?\n"
            ]
        },
        {
            "id": "141-3-4",
            "pair": [
                "Is there any way to change the appearances of either of the two screen that ask for a username and password (the unlock screen and the login screen), so that someone can look at the screen and determine whether it is logged off or locked?\n",
                "There are separate \"lock screens\" and \"logon screens\" in Windows 10. You can set the Lock Screen with \"Lock Screen Settings\" under Personalization in Settings. (See https://support.microsoft.com/en-us/help/17185/windows-10-lock-screen or https://binged.it/2ORvMx8 )\n"
            ]
        },
        {
            "id": "141-4-5",
            "pair": [
                "There are separate \"lock screens\" and \"logon screens\" in Windows 10. You can set the Lock Screen with \"Lock Screen Settings\" under Personalization in Settings. (See https://support.microsoft.com/en-us/help/17185/windows-10-lock-screen or https://binged.it/2ORvMx8 )\n",
                "The Logon Screen can be changed through Group Policy or a Provisioning Package, at least in the enterprise SKUs. See https://docs.microsoft.com/en-us/windows-hardware/customize/enterprise/custom-logon.\n"
            ]
        },
        {
            "id": "141-5-6",
            "pair": [
                "The Logon Screen can be changed through Group Policy or a Provisioning Package, at least in the enterprise SKUs. See https://docs.microsoft.com/en-us/windows-hardware/customize/enterprise/custom-logon.\n",
                "You might also be able to use one screen saver for the \"default\" (signed-out) user and one or more different screen-savers for users.\n"
            ]
        },
        {
            "id": "141-6-7",
            "pair": [
                "You might also be able to use one screen saver for the \"default\" (signed-out) user and one or more different screen-savers for users.\n",
                "I think if you tweak these settings you can get the visual indications you want. "
            ]
        }
    ],
    [
        {
            "id": "142-1-2",
            "pair": [
                "First, as you have, select the preference to mark the original email with a chevron (>).\n",
                "Then, when replying, look at the end of the subject line and click on Plain Text. This will convert the entire email to plain text format and will include \">\" on each line.\n"
            ]
        },
        {
            "id": "142-2-3",
            "pair": [
                "Then, when replying, look at the end of the subject line and click on Plain Text. This will convert the entire email to plain text format and will include \">\" on each line.\n",
                "The link at the end of the subject field will now read Rich Text. Click on that.\n"
            ]
        },
        {
            "id": "142-3-4",
            "pair": [
                "The link at the end of the subject field will now read Rich Text. Click on that.\n",
                "This will convert the email back to HTML formatting, but keep the chevrons. You can now reply with HTML formatting on your text, but still have the original message marked out with \">\" on each line.\n"
            ]
        },
        {
            "id": "142-4-5",
            "pair": [
                "This will convert the email back to HTML formatting, but keep the chevrons. You can now reply with HTML formatting on your text, but still have the original message marked out with \">\" on each line.\n",
                "A side-effect is that it renders the original email without any of the formatting it had originally, like colours, tables and images.\n"
            ]
        },
        {
            "id": "142-5-6",
            "pair": [
                "A side-effect is that it renders the original email without any of the formatting it had originally, like colours, tables and images.\n",
                "I use the \"new\" Yahoo Mail interface (which is actually about 5 years old). When replying or forwarding to a message I would like to mark the original message with '>' so you can easily distinguish the reply from the original like this:\n"
            ]
        },
        {
            "id": "142-6-7",
            "pair": [
                "I use the \"new\" Yahoo Mail interface (which is actually about 5 years old). When replying or forwarding to a message I would like to mark the original message with '>' so you can easily distinguish the reply from the original like this:\n",
                "Although I have this selected, it doesn't appear to make any difference. I guess that in order for it to work, I also need to change my messages to plain text, but I can't figure out how to do this.\n"
            ]
        },
        {
            "id": "142-7-8",
            "pair": [
                "Although I have this selected, it doesn't appear to make any difference. I guess that in order for it to work, I also need to change my messages to plain text, but I can't figure out how to do this.\n",
                "Do I really have to choose between HTML messages and marking my messages with '>', i.e. I can't have both? If that's the case, I'll give up the HTML. Can someone tell me if it's possible to have both, but if not, how do I turn on plain text?"
            ]
        }
    ],
    [
        {
            "id": "143-1-2",
            "pair": [
                "Some routers will be able to be WiFi clients and hosts (at the same time). This is key to your problem.\n",
                "You'd want one network for your RV, so you can connect your laptops with your own router. But your router should connect to the internet through the hotspot of the camp grounds.\n"
            ]
        },
        {
            "id": "143-2-3",
            "pair": [
                "You'd want one network for your RV, so you can connect your laptops with your own router. But your router should connect to the internet through the hotspot of the camp grounds.\n",
                "People are often only used to their router being able to connect to DSL or cable. But WiFi can provide the same service.\n"
            ]
        },
        {
            "id": "143-3-4",
            "pair": [
                "People are often only used to their router being able to connect to DSL or cable. But WiFi can provide the same service.\n",
                "However, setting something like that up can be complicated (depending on the router). And, to my understanding, requires 2 radios in the router. Something that is usually reserved for higher performance equipment. \n"
            ]
        },
        {
            "id": "143-4-5",
            "pair": [
                "However, setting something like that up can be complicated (depending on the router). And, to my understanding, requires 2 radios in the router. Something that is usually reserved for higher performance equipment. \n",
                "I personally own a WNDR3700 which, to my understanding, would support a configuration like that. But, it only does so because I installed a custom firmware on it. Something I would not suspect you'd want to deal with.\n"
            ]
        },
        {
            "id": "143-5-6",
            "pair": [
                "I personally own a WNDR3700 which, to my understanding, would support a configuration like that. But, it only does so because I installed a custom firmware on it. Something I would not suspect you'd want to deal with.\n",
                "...or, as you've already realized yourself, just add WiFi dongles to your laptop so you can connect them to both your own router and the campgrounds network at the same time.\n"
            ]
        },
        {
            "id": "143-6-7",
            "pair": [
                "...or, as you've already realized yourself, just add WiFi dongles to your laptop so you can connect them to both your own router and the campgrounds network at the same time.\n",
                "Having multiple WiFi connections in this context shouldn't be a problem.\n"
            ]
        },
        {
            "id": "143-7-8",
            "pair": [
                "Having multiple WiFi connections in this context shouldn't be a problem.\n",
                "Your printer and router are on a different network than the campground wireless access points.  Unless you set up your computer in such a way that it can connect to multiple wireless networks simultaneously (i.e.: with multiple wireless adapters) you will have to connect to each network on an as-needed basis - connect to the campground Wi-Fi when you need Internet access, and to your RV's router when you need to print.\n"
            ]
        },
        {
            "id": "143-8-9",
            "pair": [
                "Your printer and router are on a different network than the campground wireless access points.  Unless you set up your computer in such a way that it can connect to multiple wireless networks simultaneously (i.e.: with multiple wireless adapters) you will have to connect to each network on an as-needed basis - connect to the campground Wi-Fi when you need Internet access, and to your RV's router when you need to print.\n",
                "Alternately, as @OliverSalzburg suggests, some wireless routers can connect to the Internet through other wireless APs.  This is not a feature available on all routers though, so you'll have to look up your own router's specifications and read the friendly manual."
            ]
        }
    ],
    [
        {
            "id": "144-1-2",
            "pair": [
                "With those changes, your code might look something like:\n",
                "As you are asking for best practices: methods and exceptions.\n"
            ]
        },
        {
            "id": "144-2-3",
            "pair": [
                "As you are asking for best practices: methods and exceptions.\n",
                "First of all, your code does everything in one method, where you could (and should) clearly divide the parts of\n"
            ]
        },
        {
            "id": "144-3-4",
            "pair": [
                "First of all, your code does everything in one method, where you could (and should) clearly divide the parts of\n",
                "As you return with a failure anyway if a single line fails, you might as well throw an exception and stop the complete process. If you want to stick to the interface that returns an error/success condition as a result value, you may well catch and return in the outermost method.\n"
            ]
        },
        {
            "id": "144-4-5",
            "pair": [
                "As you return with a failure anyway if a single line fails, you might as well throw an exception and stop the complete process. If you want to stick to the interface that returns an error/success condition as a result value, you may well catch and return in the outermost method.\n",
                "Here, you have a trivial outer loop (or in fact a stream), which uses the nio.Files methods (which have been around since java 8, i.e. more than 4 years now - time to start using them), and two more methods which can trivially be unit-tested without even creating a file.\n"
            ]
        },
        {
            "id": "144-5-6",
            "pair": [
                "Here, you have a trivial outer loop (or in fact a stream), which uses the nio.Files methods (which have been around since java 8, i.e. more than 4 years now - time to start using them), and two more methods which can trivially be unit-tested without even creating a file.\n",
                "A little twist is extending RuntimeException instead of Exception for the hommade business exception, so that it can be used in a lambda expression. As it extends RuntimeException, the throws declaration is technically not really necessary on the methods, but I like to have them there so that the reader immediately sees that this exception type is to be expected."
            ]
        }
    ],
    [
        {
            "id": "145-1-2",
            "pair": [
                "I would like do some NAT in iptables. So that, all the packets coming to 192.168.12.87 and port 80 will be forwarded to 192.168.12.77 port 80.\n",
                "The reason a seemingly obvious iptables -t nat -A PREROUTING -d 192.168.12.87 -p tcp --dport 80 -j DNAT --to-destination 192.168.12.77 will not work is how the return packets will be routed.\n"
            ]
        },
        {
            "id": "145-2-3",
            "pair": [
                "The reason a seemingly obvious iptables -t nat -A PREROUTING -d 192.168.12.87 -p tcp --dport 80 -j DNAT --to-destination 192.168.12.77 will not work is how the return packets will be routed.\n",
                "You can set up rules that will cause the packets send to 192.168.12.87 to simply be NATted to 192.168.12.77, but 192.168.12.77 will then send replies directly back to the client. Those replies will not go through the host where your iptables rule is doing NAT, hence the packets in one direction are translated, but packets in the other direction are not.\n"
            ]
        },
        {
            "id": "145-3-4",
            "pair": [
                "You can set up rules that will cause the packets send to 192.168.12.87 to simply be NATted to 192.168.12.77, but 192.168.12.77 will then send replies directly back to the client. Those replies will not go through the host where your iptables rule is doing NAT, hence the packets in one direction are translated, but packets in the other direction are not.\n",
                "There are three approaches to solving this problem.\n"
            ]
        },
        {
            "id": "145-4-5",
            "pair": [
                "There are three approaches to solving this problem.\n",
                "Each of those three solutions have drawbacks, so you need to carefully consider, if you really need to do this particular forwarding.\n"
            ]
        },
        {
            "id": "145-5-6",
            "pair": [
                "Each of those three solutions have drawbacks, so you need to carefully consider, if you really need to do this particular forwarding.\n",
                "Of the three approaches I think the first is the one, which is most likely to work. So if you don't need to know the client IP addresses, that is the one I would recommend.\n"
            ]
        },
        {
            "id": "145-6-7",
            "pair": [
                "Of the three approaches I think the first is the one, which is most likely to work. So if you don't need to know the client IP addresses, that is the one I would recommend.\n",
                "You can also choose to forget about NAT altogether and not try to solve the problem on MAC or IP layer. You can go all the way up to the HTTP layer and look for a solution there. In that case the solution you will find is an HTTP proxy. If you install an HTTP proxy on 192.168.12.87 and configure it appropriately, you can have it forward the requests to 192.168.12.77 and forward the answers back. Additionally it can insert an X-Forwarded-For header preserving the original client IP. The server on 192.168.12.77 then need to be configured to trust the X-Forwarded-For header from 192.168.12.87."
            ]
        }
    ],
    [
        {
            "id": "146-1-2",
            "pair": [
                "Disk First Aid might be reporting that the GPT partition table and the MBR partition table disagree with each other about your partition map.  You can confirm this by typing fdisk /dev/disk0 from a terminal window.  See if fdisk reports a different partition layout than diskutil does.\n",
                "Are you trying to recover the former Boot Camp partition or are you just trying to clear the error so you can use it to partition your disk again?\n"
            ]
        },
        {
            "id": "146-2-3",
            "pair": [
                "Are you trying to recover the former Boot Camp partition or are you just trying to clear the error so you can use it to partition your disk again?\n",
                "If your goal is to merely clear the error, use fdisk to remove all partitions and create a single partition, type EE, starting on sector 1 and ending on the last sector of the drive.  This is what a 'normal' GPT partition's MBR protective partition looks like.\n"
            ]
        },
        {
            "id": "146-3-4",
            "pair": [
                "If your goal is to merely clear the error, use fdisk to remove all partitions and create a single partition, type EE, starting on sector 1 and ending on the last sector of the drive.  This is what a 'normal' GPT partition's MBR protective partition looks like.\n",
                "I believe I've found a simple solution for my problem. For anyone experiencing the same issues, here's how I did it:\n"
            ]
        },
        {
            "id": "146-4-5",
            "pair": [
                "I believe I've found a simple solution for my problem. For anyone experiencing the same issues, here's how I did it:\n",
                "It worked flawlessly. I now have a fully-working Macintosh HD partition (500go), and a recovery partition that is only 650 Mb. I am able to boot on both Macintosh HD and Recovery HD, and the recovery tools report no errors. "
            ]
        }
    ],
    [
        {
            "id": "147-1-2",
            "pair": [
                "So, you said it was mounted over the network. Which filesystem, CIFS or NFS? Either way, I suspect you may've been delegated the file. When your client gets a file delegated, it's able to cache writes locally. However, when you use O_DIRECT (that's what oflag=direct means), the writes are sent to the server immediately instead of any caching.\n",
                "Either way, there's something strange going on. You should be getting more than 1.1MB/s unless you're actually using just 10Mbps ethernet. \n"
            ]
        },
        {
            "id": "147-2-3",
            "pair": [
                "Either way, there's something strange going on. You should be getting more than 1.1MB/s unless you're actually using just 10Mbps ethernet. \n",
                "Also, you can get a mid-transfer update for speed by running, in another window, killall -USR1 dd\n"
            ]
        },
        {
            "id": "147-3-4",
            "pair": [
                "Also, you can get a mid-transfer update for speed by running, in another window, killall -USR1 dd\n",
                "If you check the dd man page, you'll see that the USR1 signal does not kill/stop dd but print out I/O statistics. I often do this when waiting for large disk transfers:\n"
            ]
        },
        {
            "id": "147-4-5",
            "pair": [
                "If you check the dd man page, you'll see that the USR1 signal does not kill/stop dd but print out I/O statistics. I often do this when waiting for large disk transfers:\n",
                "Oh, and if you suspect disk caching, use this command to flush the readcache:\n"
            ]
        },
        {
            "id": "147-5-6",
            "pair": [
                "Oh, and if you suspect disk caching, use this command to flush the readcache:\n",
                "dd has direct option which requires kernel to bypass any caching and send data directly to block device. If you need benchmarking your device, not RAM, you have to use it, for e. g.:\n"
            ]
        },
        {
            "id": "147-6-7",
            "pair": [
                "dd has direct option which requires kernel to bypass any caching and send data directly to block device. If you need benchmarking your device, not RAM, you have to use it, for e. g.:\n",
                "dd if=/dev/zero of=direct_output bs=1M count=100 oflag=direct"
            ]
        }
    ],
    [
        {
            "id": "148-1-2",
            "pair": [
                "I've been wondering lately what (if any) are the improvements available in MariaDB over 'conventional' MySQL?\n",
                "I understand that where platform interoperability and/or backwards compatibility may be an issue then sticking with the tried and trusted MySQL is best. But for a stand-alone DB on a stand-alone web site/application are there any benefits to be had by using Maria?\n"
            ]
        },
        {
            "id": "148-2-3",
            "pair": [
                "I understand that where platform interoperability and/or backwards compatibility may be an issue then sticking with the tried and trusted MySQL is best. But for a stand-alone DB on a stand-alone web site/application are there any benefits to be had by using Maria?\n",
                "Will Maria work with common web platforms such as WordPress, Drupal, Joomla etc.?\n"
            ]
        },
        {
            "id": "148-3-4",
            "pair": [
                "Will Maria work with common web platforms such as WordPress, Drupal, Joomla etc.?\n",
                "I expect that some of this is going to come down to choice/preference of storage engines, but to be honest I still don't know half the time if/when I should use MyISAM, InnoDB or any of the others! Which is better or faster or whatever?\n"
            ]
        },
        {
            "id": "148-4-5",
            "pair": [
                "I expect that some of this is going to come down to choice/preference of storage engines, but to be honest I still don't know half the time if/when I should use MyISAM, InnoDB or any of the others! Which is better or faster or whatever?\n",
                "The only thing I get is that if I want 'true' table relationships (i.e. foreign keys etc.) I use InnoDB.\n"
            ]
        },
        {
            "id": "148-5-6",
            "pair": [
                "The only thing I get is that if I want 'true' table relationships (i.e. foreign keys etc.) I use InnoDB.\n",
                "Thanks for any help or clarity people can offer me.\n"
            ]
        },
        {
            "id": "148-6-7",
            "pair": [
                "Thanks for any help or clarity people can offer me.\n",
                "MariaDB developers claim that it's a drop-in replacement, and it's true until version 5.5.\n"
            ]
        },
        {
            "id": "148-7-8",
            "pair": [
                "MariaDB developers claim that it's a drop-in replacement, and it's true until version 5.5.\n",
                "The small \"incompatibility\" issues usually don't apply, however, they are documented here:\n"
            ]
        },
        {
            "id": "148-8-9",
            "pair": [
                "The small \"incompatibility\" issues usually don't apply, however, they are documented here:\n",
                "https://kb.askmonty.org/en/mariadb-vs-mysql-compatibility/\n"
            ]
        },
        {
            "id": "148-9-10",
            "pair": [
                "https://kb.askmonty.org/en/mariadb-vs-mysql-compatibility/\n",
                "There are also a lot of bug fixes and new features, the most important being listed here:\n"
            ]
        },
        {
            "id": "148-10-11",
            "pair": [
                "There are also a lot of bug fixes and new features, the most important being listed here:\n",
                "https://kb.askmonty.org/en/mariadb-vs-mysql-features/\n"
            ]
        },
        {
            "id": "148-11-12",
            "pair": [
                "https://kb.askmonty.org/en/mariadb-vs-mysql-features/\n",
                "The default storage engine, XtraDB, is a Percona's fork of Oracle InnoDB, which includes several bug fixes and some extra features. And you can also replace MyISAM with Aria and Federated with FederatedX. However, Oracle InnoDB can be installed if you experience compatibility problems, as well as Federated, and MyISAM is already there.\n"
            ]
        },
        {
            "id": "148-12-13",
            "pair": [
                "The default storage engine, XtraDB, is a Percona's fork of Oracle InnoDB, which includes several bug fixes and some extra features. And you can also replace MyISAM with Aria and Federated with FederatedX. However, Oracle InnoDB can be installed if you experience compatibility problems, as well as Federated, and MyISAM is already there.\n",
                "MariaDB 10.0 can replace MySQL 5.5 too, and has some features from MySQL 5.6 (some of which have been reimplemented by Monty's team because the code quality was too low). However, some features from 5.6 are not in MariaDB, at the moment. For example, they don't have GET DIAGNOSTICS in stored procedures, and the JSON format for EXPLAIN output.\n"
            ]
        },
        {
            "id": "148-13-14",
            "pair": [
                "MariaDB 10.0 can replace MySQL 5.5 too, and has some features from MySQL 5.6 (some of which have been reimplemented by Monty's team because the code quality was too low). However, some features from 5.6 are not in MariaDB, at the moment. For example, they don't have GET DIAGNOSTICS in stored procedures, and the JSON format for EXPLAIN output.\n",
                "All APIs and clients which work with MySQL should work with MariaDB. If something doesn't work, it's a bug and will be solved. If you use Windows, probably you want to use the HeidiSQL GUI, because it supports some MariaDB specific features like Virtual Columns."
            ]
        }
    ],
    [
        {
            "id": "149-1-2",
            "pair": [
                "If you're asking whether a quantum computer can compute any function that a classical computer can compute without using many more elementary computational steps, then the answer is yes: a quantum computer can perform any reversible classical computation, and if you keep the input around, any classical computation can be made reversible at a cost of multiplying the number of steps by a small constant factor.\n",
                "If you're asking whether a quantum computer can compute any function that a classical computer can without using many more resources, the answer is much less clear. The construction that lets you make a $T$-step computation reversible using $O(T)$ steps also takes $O(T)$ space (i.e., memory cells). You can achieve a smaller blow-up in space at the cost of a superlinear number of steps. See Time/Space Trade-Offs for Reversible Computation by Charles H. Bennett. \n"
            ]
        },
        {
            "id": "149-2-3",
            "pair": [
                "If you're asking whether a quantum computer can compute any function that a classical computer can without using many more resources, the answer is much less clear. The construction that lets you make a $T$-step computation reversible using $O(T)$ steps also takes $O(T)$ space (i.e., memory cells). You can achieve a smaller blow-up in space at the cost of a superlinear number of steps. See Time/Space Trade-Offs for Reversible Computation by Charles H. Bennett. \n",
                "For an actual physical quantum computer, it's very likely that you might also be able to make it faithfully simulate a classical computer by letting it lose coherence, but in this case it's no longer really working as a quantum computer, and if you try to use a quantum computer that is operating this way as a subroutine in a quantum computation, it might not work properly. \n"
            ]
        },
        {
            "id": "149-3-4",
            "pair": [
                "For an actual physical quantum computer, it's very likely that you might also be able to make it faithfully simulate a classical computer by letting it lose coherence, but in this case it's no longer really working as a quantum computer, and if you try to use a quantum computer that is operating this way as a subroutine in a quantum computation, it might not work properly. \n",
                "check it out, that guy explains your question. He starts with the concept of a qbit and around the 5:15 mark he goes into what you asked."
            ]
        }
    ],
    [
        {
            "id": "15-1-2",
            "pair": [
                "I have several storage arrays where a significant number of the drives have been powered on between 25,000 - 30,000 hours (2.8 - 3.4 years). These drives have no other issues or errors. \n",
                "What I want to know: is there a point where drive age alone is a significant enough factor to replace a drive, even if the drive is working fine and has no errors? \n"
            ]
        },
        {
            "id": "15-2-3",
            "pair": [
                "What I want to know: is there a point where drive age alone is a significant enough factor to replace a drive, even if the drive is working fine and has no errors? \n",
                "(I'm curious to see if people tend to run drives until they fail or start throwing errors, or if anyone takes a proactive approach at replacement using Power On Hours as a metric.)\n"
            ]
        },
        {
            "id": "15-3-4",
            "pair": [
                "(I'm curious to see if people tend to run drives until they fail or start throwing errors, or if anyone takes a proactive approach at replacement using Power On Hours as a metric.)\n",
                "Drive manufactures generally quote MTBF on enterprise drives at 1,000,000 to 1,500,000 hours, but these numbers don't really mean much in the real world.\n"
            ]
        },
        {
            "id": "15-4-5",
            "pair": [
                "Drive manufactures generally quote MTBF on enterprise drives at 1,000,000 to 1,500,000 hours, but these numbers don't really mean much in the real world.\n",
                "Disk failures in the real world: What does an MTTF of 1,000,000 hours mean to you?\n"
            ]
        },
        {
            "id": "15-5-6",
            "pair": [
                "Disk failures in the real world: What does an MTTF of 1,000,000 hours mean to you?\n",
                "The study suggests a \"sweet spot\" between 1 year and 5-7 years where you can expect less failures. Drive age before/after these times tended to be considerably higher.\n"
            ]
        },
        {
            "id": "15-6-7",
            "pair": [
                "The study suggests a \"sweet spot\" between 1 year and 5-7 years where you can expect less failures. Drive age before/after these times tended to be considerably higher.\n",
                "I have never met (or heard of) anyone who is replacing drives just because they are 'too old' (while keeping storage/server in production)."
            ]
        }
    ],
    [
        {
            "id": "150-1-2",
            "pair": [
                "The display\u2019s specs (manual page 58) are very clear on this:\n",
                "Some other model can also do it over Dual Link DVI, though it\u2019s not entirely clear whether the G2770PF can. It most certainly won\u2019t work over Single Link DVI or HDMI.\n"
            ]
        },
        {
            "id": "150-2-3",
            "pair": [
                "Some other model can also do it over Dual Link DVI, though it\u2019s not entirely clear whether the G2770PF can. It most certainly won\u2019t work over Single Link DVI or HDMI.\n",
                "I put this question in Arquade but was told to move it here:\n"
            ]
        },
        {
            "id": "150-3-4",
            "pair": [
                "I put this question in Arquade but was told to move it here:\n",
                "I recently bought an AOC G2770PF 144hz 1ms Monitor which you can check out here: https://www.amazon.com/AOC-G2770PF-Freesync-DisplayPort-Speakers/dp/B013YIR5WU\n"
            ]
        },
        {
            "id": "150-4-5",
            "pair": [
                "I recently bought an AOC G2770PF 144hz 1ms Monitor which you can check out here: https://www.amazon.com/AOC-G2770PF-Freesync-DisplayPort-Speakers/dp/B013YIR5WU\n",
                "Currently, i don't have a Desktop Computer so I am running this on my laptop you can check it here: https://www.amazon.com/dp/B015PYZI8E/ref=olp_product_details?_encoding=UTF8&me=\n"
            ]
        },
        {
            "id": "150-5-6",
            "pair": [
                "Currently, i don't have a Desktop Computer so I am running this on my laptop you can check it here: https://www.amazon.com/dp/B015PYZI8E/ref=olp_product_details?_encoding=UTF8&me=\n",
                "Anyways, The Monitors 'Extra' Tab on the OCD shows:\n"
            ]
        },
        {
            "id": "150-6-7",
            "pair": [
                "Anyways, The Monitors 'Extra' Tab on the OCD shows:\n",
                "Which, of course, is odd considering the monitor literally shows 144hz on the Frame. I am using a Male HDMI and Male DVI-D cable to connect the two (I'm not sure if this is the problem or not) I mainly play csgo on 1400x1050 resolution so I thought that since HDMI 1.3+ can (apparently) support 144hz @ 1920x1080 that I could of course play on 144hz on a resolution less than 1920x1080. when I checked the csgo console command \"mat_info\" it said that I was running 1400x1050@60hz what is the problem? Also im not to sure what the version of my laptop's HDMI port supports (1.3,1.4,2.0 etc;) would this mean that i cant run on 144hz on HDMI to HDMI? "
            ]
        }
    ],
    [
        {
            "id": "151-1-2",
            "pair": [
                "I used clustering on my dataset. Now when I'm trying to use a LASSO with cv to predict a response, one of the variables it takes into consideration is which cluster a new point is classified into.(I included the cluster variable as a predictor to see if being in a particular group affects the response) \n",
                "Since the information on all variables is already captured by the cluster variable,using it again in the Lasso model with some other variables,does it become redundant/biased?\n"
            ]
        },
        {
            "id": "151-2-3",
            "pair": [
                "Since the information on all variables is already captured by the cluster variable,using it again in the Lasso model with some other variables,does it become redundant/biased?\n",
                "I think that by doing so, you have expanded the feature space, and I don't think that this additional variable is redundant. \n"
            ]
        },
        {
            "id": "151-3-4",
            "pair": [
                "I think that by doing so, you have expanded the feature space, and I don't think that this additional variable is redundant. \n",
                "The cluster categorical variable is a linear combination of the other variables. When you apply Lasso on the dataset, the categorical variable undergoes one-hot encoding and Lasso would pick a subset of values given a level of regularisation. Suppose in the best performing regularisation parameter, Lasso picked clusters 1,3,7 out of clusters 1 - 10; These pockets of clusters might not be picked up using just Lasso on the other variables without the cluster variable."
            ]
        }
    ],
    [
        {
            "id": "152-1-2",
            "pair": [
                "I think it is choking on 200 threads.  Recommend not using more than twice the number of CPU cores you have.  And even that may be too high...\n",
                "In particular, there is no use adding more threads after saturating the I/O.  (I am not familiar with your configuration, so I cannot be more specific.)\n"
            ]
        },
        {
            "id": "152-2-3",
            "pair": [
                "In particular, there is no use adding more threads after saturating the I/O.  (I am not familiar with your configuration, so I cannot be more specific.)\n",
                "innodb_buffer_pool_size =235G is dangerously high for 244GB of RAM.  If there is swapping, that could be killing performance.\n"
            ]
        },
        {
            "id": "152-3-4",
            "pair": [
                "innodb_buffer_pool_size =235G is dangerously high for 244GB of RAM.  If there is swapping, that could be killing performance.\n",
                "If you have \"indexed every column\", that is another performance killer for loading.  And \"indexing every column\" is usually useless.  Instead, discover what indexes you need, and have only them.  This often includes 'composite' indexes.\n"
            ]
        },
        {
            "id": "152-4-5",
            "pair": [
                "If you have \"indexed every column\", that is another performance killer for loading.  And \"indexing every column\" is usually useless.  Instead, discover what indexes you need, and have only them.  This often includes 'composite' indexes.\n",
                "Also, multiple UNIQUE (including PRIMARY) indexes on a table is costly.\n"
            ]
        },
        {
            "id": "152-5-6",
            "pair": [
                "Also, multiple UNIQUE (including PRIMARY) indexes on a table is costly.\n",
                "Im trying to restore a database which is 1.3TB and its in AWS Aurora. I want to restore a copy of this database to an EC2 instance(32 vCPU, 244GB MEMORY).\n"
            ]
        },
        {
            "id": "152-6-7",
            "pair": [
                "Im trying to restore a database which is 1.3TB and its in AWS Aurora. I want to restore a copy of this database to an EC2 instance(32 vCPU, 244GB MEMORY).\n",
                "I used mydumper to take backup from AWS RDS and it took 6 Hrs. \n"
            ]
        },
        {
            "id": "152-7-8",
            "pair": [
                "I used mydumper to take backup from AWS RDS and it took 6 Hrs. \n",
                "But Im trying to load this backup using myloader. Its running more than 12hrs, but still 30% data only restored.\n"
            ]
        },
        {
            "id": "152-8-9",
            "pair": [
                "But Im trying to load this backup using myloader. Its running more than 12hrs, but still 30% data only restored.\n",
                "Is there any additional settings that I should change for faster restore? "
            ]
        }
    ],
    [
        {
            "id": "153-1-2",
            "pair": [
                "NEVER MERGE that unpartitioned space (in most cases, there will be shown 500 mb).\n",
                " It is kept by system for special cases, and you should not use that space for your needs. \n"
            ]
        },
        {
            "id": "153-2-3",
            "pair": [
                " It is kept by system for special cases, and you should not use that space for your needs. \n",
                "At first, try to re-attach that space again and leave it as unallocated space. Then see if system boots.\n"
            ]
        },
        {
            "id": "153-3-4",
            "pair": [
                "At first, try to re-attach that space again and leave it as unallocated space. Then see if system boots.\n",
                "I had around a 500 MB of unallocated space in my HDD so I decided to marge it with C: drive. I used AOMEI partition toolkit.\n"
            ]
        },
        {
            "id": "153-4-5",
            "pair": [
                "I had around a 500 MB of unallocated space in my HDD so I decided to marge it with C: drive. I used AOMEI partition toolkit.\n",
                "I did a trial run by splitting my D: drive into two (unallocated one of the partitions) and joined them. This is similar to the scenario with C: drive and some unallocated space. The software said restart the computer and the partition will be allocated in a pre OS mode(Same thing happened with D drive) and it did. Midway through the merging it showed an error occurred and restarted. From that time it is showing\n"
            ]
        },
        {
            "id": "153-5-6",
            "pair": [
                "I did a trial run by splitting my D: drive into two (unallocated one of the partitions) and joined them. This is similar to the scenario with C: drive and some unallocated space. The software said restart the computer and the partition will be allocated in a pre OS mode(Same thing happened with D drive) and it did. Midway through the merging it showed an error occurred and restarted. From that time it is showing\n",
                "\"DISK READ ERROR OCCURRED press ctrl+alt+del to restart\" and it goes on in a loop.\n"
            ]
        },
        {
            "id": "153-6-7",
            "pair": [
                "\"DISK READ ERROR OCCURRED press ctrl+alt+del to restart\" and it goes on in a loop.\n",
                "I installed MiniTool Partition wizard so that I could allocate the drive spaces without booting into the OS but it is showing \n"
            ]
        },
        {
            "id": "153-7-8",
            "pair": [
                "I installed MiniTool Partition wizard so that I could allocate the drive spaces without booting into the OS but it is showing \n",
                "\"USB descriptor file not found\" for my mouse(not recognizing it)and not going inside the main partition screen.\n"
            ]
        },
        {
            "id": "153-8-9",
            "pair": [
                "\"USB descriptor file not found\" for my mouse(not recognizing it)and not going inside the main partition screen.\n",
                "I just want to boot into my win 10...How can I do that??Is there any recovery software that I can try or any cmd code that will help me recover my OS????? "
            ]
        }
    ],
    [
        {
            "id": "154-1-2",
            "pair": [
                "If you enter man 5 crontab you will see the following example:-\n",
                "Alternatively, if YourCommand is a script, you can incorporate the time tests into it.\n"
            ]
        },
        {
            "id": "154-2-3",
            "pair": [
                "Alternatively, if YourCommand is a script, you can incorporate the time tests into it.\n",
                "Note that, since the command is invoked at multiples of 5 minutes past the hour, tests for > and >= are equivalent (likewise < and <=); otherwise, you may need to adjust the test boundaries (eg >= 02:18), since test doesn't support >= and <= in string comparisons.\n"
            ]
        },
        {
            "id": "154-3-4",
            "pair": [
                "Note that, since the command is invoked at multiples of 5 minutes past the hour, tests for > and >= are equivalent (likewise < and <=); otherwise, you may need to adjust the test boundaries (eg >= 02:18), since test doesn't support >= and <= in string comparisons.\n",
                "I've seen quite a few questions here related to configuring CronExpressions. But they all seem to have exact hours (no minutes specified) as their time window for running.\n"
            ]
        },
        {
            "id": "154-4-5",
            "pair": [
                "I've seen quite a few questions here related to configuring CronExpressions. But they all seem to have exact hours (no minutes specified) as their time window for running.\n",
                "Is there a way to configure a CronExpression to run say, every 5 minutes of every day between 7:37AM and 9:13PM? Configuring for 7AM - 9PM is simple. But I can't seem to figure out if there is a way (maybe it's not even supported) to run on a more detailed schedule.\n"
            ]
        },
        {
            "id": "154-5-6",
            "pair": [
                "Is there a way to configure a CronExpression to run say, every 5 minutes of every day between 7:37AM and 9:13PM? Configuring for 7AM - 9PM is simple. But I can't seem to figure out if there is a way (maybe it's not even supported) to run on a more detailed schedule.\n",
                "I have an application that allows users to specify custom schedules. Right now I'm rounding up/down to the nearest \"whole hour\" as I cannot seem to figure out how to do specific times.\n"
            ]
        },
        {
            "id": "154-6-7",
            "pair": [
                "I have an application that allows users to specify custom schedules. Right now I'm rounding up/down to the nearest \"whole hour\" as I cannot seem to figure out how to do specific times.\n",
                "0 0/5 2-16 ? * 1-7 //runs every 5 minutes between 2am and 4pm every day of the week\n"
            ]
        },
        {
            "id": "154-7-8",
            "pair": [
                "0 0/5 2-16 ? * 1-7 //runs every 5 minutes between 2am and 4pm every day of the week\n",
                "Or is this just not possible (outside of creating multiple CronExpressions and making it really messy imo)"
            ]
        }
    ],
    [
        {
            "id": "155-1-2",
            "pair": [
                "I have a situation where I need best possible throughput (TCP/IP sockets & SMB3) between two Windows Server 2016 connect with a 1m patch cable.\n",
                "When you connect two Windows Server 2016 machines together directly, without using a switch, you should be able to get better performance since you know there is only two computers on the subnet.\n"
            ]
        },
        {
            "id": "155-2-3",
            "pair": [
                "When you connect two Windows Server 2016 machines together directly, without using a switch, you should be able to get better performance since you know there is only two computers on the subnet.\n",
                "The question is, which parameters to tune, and what set those parameters to?\n"
            ]
        },
        {
            "id": "155-3-4",
            "pair": [
                "The question is, which parameters to tune, and what set those parameters to?\n",
                "I have been looking at the parameters on the drivers for the network cards, but I guess that there are settings in windows as well that affect the network stack and how TCP/IP, RPC, named pipes and so on behave.\n"
            ]
        },
        {
            "id": "155-4-5",
            "pair": [
                "I have been looking at the parameters on the drivers for the network cards, but I guess that there are settings in windows as well that affect the network stack and how TCP/IP, RPC, named pipes and so on behave.\n",
                "Both machines are Dell R430 with Intel Ethernet I350 QP 1Gb Server Adapter.\n"
            ]
        },
        {
            "id": "155-5-6",
            "pair": [
                "Both machines are Dell R430 with Intel Ethernet I350 QP 1Gb Server Adapter.\n",
                "The \"front\" machine is connected the the normal network through one nic, and to the \"backend\" machine through a 1m patch cable on another nic.\n"
            ]
        },
        {
            "id": "155-6-7",
            "pair": [
                "The \"front\" machine is connected the the normal network through one nic, and to the \"backend\" machine through a 1m patch cable on another nic.\n",
                "The \"backend\" machine is only connected to the \"front\" machine.\n"
            ]
        },
        {
            "id": "155-7-8",
            "pair": [
                "The \"backend\" machine is only connected to the \"front\" machine.\n",
                "Using TCP over gigabit Ethernet, there shouldn't be any difference between direct connection or a switch in between (unless the switch is ancient). For the last two decades, switches have been running at \"wire speed\" = forwarding as fast as is physically possible.\n"
            ]
        },
        {
            "id": "155-8-9",
            "pair": [
                "Using TCP over gigabit Ethernet, there shouldn't be any difference between direct connection or a switch in between (unless the switch is ancient). For the last two decades, switches have been running at \"wire speed\" = forwarding as fast as is physically possible.\n",
                "A switch introduces a tiny, additional latency into the connection (in the order of 2 to 10 \u00b5s). Unless the protocol in use is extremely sensitive to latency you won't see any difference. Especially TCP handles latencies well.\n"
            ]
        },
        {
            "id": "155-9-10",
            "pair": [
                "A switch introduces a tiny, additional latency into the connection (in the order of 2 to 10 \u00b5s). Unless the protocol in use is extremely sensitive to latency you won't see any difference. Especially TCP handles latencies well.\n",
                "Also, when using a switch there's next to no penalty for other computers on the network unless their traffic shares a single link that's already running at full capacity.\n"
            ]
        },
        {
            "id": "155-10-11",
            "pair": [
                "Also, when using a switch there's next to no penalty for other computers on the network unless their traffic shares a single link that's already running at full capacity.\n",
                "With a fairly decent network design you can run streams across the entire network and a number of switches that run at close to 100% of the link speed.\n"
            ]
        },
        {
            "id": "155-11-12",
            "pair": [
                "With a fairly decent network design you can run streams across the entire network and a number of switches that run at close to 100% of the link speed.\n",
                "With current Windows versions, the IP stack is already set up or very good performance by default. I haven't had to tweak the parameters in many years.\n"
            ]
        },
        {
            "id": "155-12-13",
            "pair": [
                "With current Windows versions, the IP stack is already set up or very good performance by default. I haven't had to tweak the parameters in many years.\n",
                "Make sure you've got current drivers installed and the offloading features in the NICs are turned on.\n"
            ]
        },
        {
            "id": "155-13-14",
            "pair": [
                "Make sure you've got current drivers installed and the offloading features in the NICs are turned on.\n",
                "When gigabit speed isn't enough you can aggregate multiple links (this is where the fun starts) or replace the NICs with 10GE ones (much simpler but more costly)."
            ]
        }
    ],
    [
        {
            "id": "156-1-2",
            "pair": [
                "The new OLE DB driver, MSOLEDBSQL, was released today. This new driver includes the support for the latest TLS 1.2 standards and is backwards compatible with SQL Server Native Client 11 (SQLNCLI11). See the Microsoft SQLNCLi team blog announcement.\n",
                "This has been going on. At first MS said they will not support TLS1.2 for OLEDB then they said, they will. You can read about it here, where they said, the support comes in March 2018\n"
            ]
        },
        {
            "id": "156-2-3",
            "pair": [
                "This has been going on. At first MS said they will not support TLS1.2 for OLEDB then they said, they will. You can read about it here, where they said, the support comes in March 2018\n",
                "Now it looks like package has arrived. I installed it. Then I ran soft without change to connection string using old style Provider=sqloledb. That was expected to fail and it did. I changed to new style Provider=MSOLEDBSQL (see first link). And now I get \n"
            ]
        },
        {
            "id": "156-3-4",
            "pair": [
                "Now it looks like package has arrived. I installed it. Then I ran soft without change to connection string using old style Provider=sqloledb. That was expected to fail and it did. I changed to new style Provider=MSOLEDBSQL (see first link). And now I get \n",
                "Ok. I didn't install provider after all. But I can't find it either. I thought, may be this is part of Native Client update. But that came out Jan 2018. I installed it, it didn't break anything, it didn't fix anything. Then I came across this provider that was just released. Installed it. This seem to be new ODBC version."
            ]
        }
    ],
    [
        {
            "id": "157-1-2",
            "pair": [
                "Was this for transferring a single file?  Also, is this performance consistent across multiple client machines?  For testing, your best test is to take a very large file (500MB or so) and transfer it.\n",
                "If all of the above yield 10Mb speeds, the issue is your router.  If the performance is only slow when transferring to/from the NAS, then the issue is your Ubuntu setup (I'd guess network config, bad driver, etc).  If the issue is only from your current test machine, but the borrowed one works fine, then it's likely a driver issue or config (e.g. duplex setting).  \n"
            ]
        },
        {
            "id": "157-2-3",
            "pair": [
                "If all of the above yield 10Mb speeds, the issue is your router.  If the performance is only slow when transferring to/from the NAS, then the issue is your Ubuntu setup (I'd guess network config, bad driver, etc).  If the issue is only from your current test machine, but the borrowed one works fine, then it's likely a driver issue or config (e.g. duplex setting).  \n",
                "I recently built a NAS box on top of ubuntu server.  I have an average transfer of about 10mb/sec over gigabit wire to my computer.  I was hoping you guys could give recommendations on how to pinpoint where the bottleneck is in this system.  To my knowledge the router is only rated at 100mb/sec, and so are the nics (both server, and client), but shouldn't my files be transferring faster then 10mb/sec?  I made sure to shutdown all other traffic when getting the 10mb/sec baseline."
            ]
        }
    ],
    [
        {
            "id": "158-1-2",
            "pair": [
                "No, you can't (I think). You can verify if the email was correctly delivered to your SMTP server, but the message will probably go through a few other smtp servers too, and one of them can mark your message as spam without you ever knowing about it.\n",
                "I think the best way to make sure that an internet mail gets delivered, is to make sure your message does not get tagged as spam. There a few simple ways to do this:\n"
            ]
        },
        {
            "id": "158-2-3",
            "pair": [
                "I think the best way to make sure that an internet mail gets delivered, is to make sure your message does not get tagged as spam. There a few simple ways to do this:\n",
                "Still, there is always a chance your mail with not get trough. That's the nature of Internet email.\n"
            ]
        },
        {
            "id": "158-3-4",
            "pair": [
                "Still, there is always a chance your mail with not get trough. That's the nature of Internet email.\n",
                "I am trying to send mails in large numbers, say 1000, using a java test application and a mail server, but though the application confirms that it has sent all the 1000 mails without any exceptions, I am not receiving the exact number (1000) of the same in the in box of the recipient's mail box(all the mails are sent to the same email recipient for testing the application's reliability). That is few of them goes missing each time I execute the application. Is there any means for the java application to verify whether each mail that is being sent using the mail server has reached the destination? And why do they go missing in the first place? It would be of great help if some one could provide tips/suggestions on how to tackle this."
            ]
        }
    ],
    [
        {
            "id": "159-1-2",
            "pair": [
                "Application (pod) presently running in GLB<->GKE Europe (say Netherlands). I got requirement to scale the  application to serve customer in US and ASIA (say SFO & Japan).\n",
                "Please help to clarify whether whether GKE need to duplicate in US & Asia or Europe is sufficient.\n"
            ]
        },
        {
            "id": "159-2-3",
            "pair": [
                "Please help to clarify whether whether GKE need to duplicate in US & Asia or Europe is sufficient.\n",
                "Please share some best practice/recommendation for this scenario.\n"
            ]
        },
        {
            "id": "159-3-4",
            "pair": [
                "Please share some best practice/recommendation for this scenario.\n",
                "I might be few weeks too late but this might be still relevant for someone.\n"
            ]
        },
        {
            "id": "159-4-5",
            "pair": [
                "I might be few weeks too late but this might be still relevant for someone.\n",
                "I don't know your architecture but if applicable use container native load balancing with a global load balancer.\n"
            ]
        },
        {
            "id": "159-5-6",
            "pair": [
                "I don't know your architecture but if applicable use container native load balancing with a global load balancer.\n",
                "Simple container native load balancer setup where you can do everything with Kubernetes resources doesn't give you global load balancing. You would need to use standalone network endpoint groups(NEG). Create load balancer manually (or with whatever tooling you use), create backend service that includes NEGs for the same service in different clusters and add backend service to the load balancer. You will get the benefits of the premium network tier (lower latency for clients globally) and traffic spillover if the service is overloaded or down in one of the regions."
            ]
        }
    ],
    [
        {
            "id": "16-1-2",
            "pair": [
                "Is there a way to connect an external driver to my lan network (direct connection, without having any PC opened) and then access it from any device from lan? Like a network drive?\n",
                "If its not possible, can you please recommend a sollution? I am asking this so all my development files will be easily accessed by all my lan devices.\n"
            ]
        },
        {
            "id": "16-2-3",
            "pair": [
                "If its not possible, can you please recommend a sollution? I am asking this so all my development files will be easily accessed by all my lan devices.\n",
                "You might want look into what is called NAS - Network-attached storage. Quote:\n"
            ]
        },
        {
            "id": "16-3-4",
            "pair": [
                "You might want look into what is called NAS - Network-attached storage. Quote:\n",
                "Ready-made products come in all shapes and sizes, depending on what exactly your needs are (redundancy, computational and I/O power, etc.). Some manufacturers might be Synology, Qnap and Buffalo.\n"
            ]
        },
        {
            "id": "16-4-5",
            "pair": [
                "Ready-made products come in all shapes and sizes, depending on what exactly your needs are (redundancy, computational and I/O power, etc.). Some manufacturers might be Synology, Qnap and Buffalo.\n",
                "As others have pointed out, a NAS with a single bay might already be enough for your purposes. You would install your hard drive, hook it up to the network and you'd pretty much be good to go. The setup and user interfaces of these products tend to be very user friendly.\n"
            ]
        },
        {
            "id": "16-5-6",
            "pair": [
                "As others have pointed out, a NAS with a single bay might already be enough for your purposes. You would install your hard drive, hook it up to the network and you'd pretty much be good to go. The setup and user interfaces of these products tend to be very user friendly.\n",
                "In the end, most of them run (sometimes more, sometimes less heavily) modified versions of Linux under the hood. It can be great fun to tinker with them, too."
            ]
        }
    ],
    [
        {
            "id": "160-1-2",
            "pair": [
                "So you're using either Debian or Ubuntu, which enable AppArmor by default and restrict which locations the named daemon may read from, or write to, or mmap, or execute. Any attempts to load modules from the \"wrong\" locations will be denied by the kernel and logged in dmesg.\n",
                "The default policy is at /etc/apparmor.d/usr.sbin.named and only allows two locations:\n"
            ]
        },
        {
            "id": "160-2-3",
            "pair": [
                "The default policy is at /etc/apparmor.d/usr.sbin.named and only allows two locations:\n",
                "Custom additions can be made in /etc/apparmor.d/local/usr.sbin.named (as the #include statement at the bottom of the file indicates). That file doesn't have any begin/end delimiters, just the additional rules themselves such as:\n"
            ]
        },
        {
            "id": "160-3-4",
            "pair": [
                "Custom additions can be made in /etc/apparmor.d/local/usr.sbin.named (as the #include statement at the bottom of the file indicates). That file doesn't have any begin/end delimiters, just the additional rules themselves such as:\n",
                "BIND9 allows you to link dynamically loadable zone (DLZ) modules at runtime using the dlopen driver. The unit test for this functionality passes for my environment, but when I try to run named with the compiled shared object file linked, I get this error:\n"
            ]
        },
        {
            "id": "160-4-5",
            "pair": [
                "BIND9 allows you to link dynamically loadable zone (DLZ) modules at runtime using the dlopen driver. The unit test for this functionality passes for my environment, but when I try to run named with the compiled shared object file linked, I get this error:\n",
                "I already opened a ticket in the BIND9 GitLab here, which includes detailed information about my specific problem.\n"
            ]
        },
        {
            "id": "160-5-6",
            "pair": [
                "I already opened a ticket in the BIND9 GitLab here, which includes detailed information about my specific problem.\n",
                "More generally, I'm wondering if folks have any insight into common problems when trying to load shared objects, or experience using DLZ modules. My hunch is there is something I'm not understanding about how they work, and there's some silly misconfiguration that's causing the issue. Of course, debugging tips are also appreciated.\n"
            ]
        },
        {
            "id": "160-6-7",
            "pair": [
                "More generally, I'm wondering if folks have any insight into common problems when trying to load shared objects, or experience using DLZ modules. My hunch is there is something I'm not understanding about how they work, and there's some silly misconfiguration that's causing the issue. Of course, debugging tips are also appreciated.\n",
                "ISC Knowledge Base page \"Using DLZ in BIND\": https://kb.isc.org/docs/aa-00995"
            ]
        }
    ],
    [
        {
            "id": "161-1-2",
            "pair": [
                "For a domain or workgroup to be seen on a given subnet (really, broadcast domain of a LAN, not necessarily a single IPv4 subnet per se), it must have at least one member on that subnet, and that member (or \"one of those members\" if there are more than one) must act as a master browser for that domain or workgroup on that subnet.\n",
                "So you should expect to see one master browser for each domain or workgroup that has members present on that subnet. So if you have a subnet with 4 machines and each is a member of a different domain or workgroup, then each will be the master browser for its own domain/workgroup on that subnet, so you will have 4 master browsers on that subnet.\n"
            ]
        },
        {
            "id": "161-2-3",
            "pair": [
                "So you should expect to see one master browser for each domain or workgroup that has members present on that subnet. So if you have a subnet with 4 machines and each is a member of a different domain or workgroup, then each will be the master browser for its own domain/workgroup on that subnet, so you will have 4 master browsers on that subnet.\n",
                "I don't know the internals of the exact registrations well enough to tell you how that maps to registrations of the group name \\x01\\x02__MSBROWSE__\\x02\\x01.\n"
            ]
        },
        {
            "id": "161-3-4",
            "pair": [
                "I don't know the internals of the exact registrations well enough to tell you how that maps to registrations of the group name \\x01\\x02__MSBROWSE__\\x02\\x01.\n",
                "On a SMB/CIFS network using NetBIOS over TCP, the group name \\x01\\x02__MSBROWSE__\\x02\\x01 is registered, which is the master browser for the subnet.\n"
            ]
        },
        {
            "id": "161-4-5",
            "pair": [
                "On a SMB/CIFS network using NetBIOS over TCP, the group name \\x01\\x02__MSBROWSE__\\x02\\x01 is registered, which is the master browser for the subnet.\n",
                "If I have different domains/workgroups on my subnet, should I expect to see one of these registrations per group, or is it simply per subnet only?"
            ]
        }
    ],
    [
        {
            "id": "162-1-2",
            "pair": [
                "I want to use HAProxy as a load balancer. I want to put two rabbitmq server behind haproxy. Both the rabbitmq server are on different instance of EC2. I have configure HAProxy server by following this reference. I works but the problem is messages are not published in roundrobin pattern. Messages are publish only on one server. Is there any different configuration for my requirement?\n",
                "I have made some R&D on this and found that HAProxy is round robin the connection on the rabbitmq server. for ex: if i request for 10 connections then it will round robin the 10 connection over my 2 rabbitmq servers and publish the message. \n"
            ]
        },
        {
            "id": "162-2-3",
            "pair": [
                "I have made some R&D on this and found that HAProxy is round robin the connection on the rabbitmq server. for ex: if i request for 10 connections then it will round robin the 10 connection over my 2 rabbitmq servers and publish the message. \n",
                "But the problem is I want to round robin the messages, not connection. i.e if i send 1000 msg at a time from 1 connection then 500 msg should go to rabbit server1 and 500 msg should go to rabbit server2. What should be the configuration that i have to follow?\n"
            ]
        },
        {
            "id": "162-3-4",
            "pair": [
                "But the problem is I want to round robin the messages, not connection. i.e if i send 1000 msg at a time from 1 connection then 500 msg should go to rabbit server1 and 500 msg should go to rabbit server2. What should be the configuration that i have to follow?\n",
                "As long as your client keeps open the connection to HAProxy, the connection from HAProxy to RabbitMQ connection won't move.  HAProxy is a connection-based load balancer, not message based (as far as I know).  It does not know when a message starts or ends (it would have to know the internal protocols for it to successfully do this).\n"
            ]
        },
        {
            "id": "162-4-5",
            "pair": [
                "As long as your client keeps open the connection to HAProxy, the connection from HAProxy to RabbitMQ connection won't move.  HAProxy is a connection-based load balancer, not message based (as far as I know).  It does not know when a message starts or ends (it would have to know the internal protocols for it to successfully do this).\n",
                "In order to round robin the RabbitMQ servers, you'll need to have your client connect to HAProxy, send the message, then disconnect (don't keep the connection open).  Reconnect to send the next message, then disconnect.\n"
            ]
        },
        {
            "id": "162-5-6",
            "pair": [
                "In order to round robin the RabbitMQ servers, you'll need to have your client connect to HAProxy, send the message, then disconnect (don't keep the connection open).  Reconnect to send the next message, then disconnect.\n",
                "Each time you reconnect, HAProxy will/should move you to a different server."
            ]
        }
    ],
    [
        {
            "id": "163-1-2",
            "pair": [
                "To get from X to Y, the public key of X has to be in the authorized_keys file of Y\n",
                "Run the ssh command to get from A to B where it asks for a password e.g.\n"
            ]
        },
        {
            "id": "163-2-3",
            "pair": [
                "Run the ssh command to get from A to B where it asks for a password e.g.\n",
                "Then do Ctrl-C,  so you just know you have the ssh command right so it prompts for a password.\n"
            ]
        },
        {
            "id": "163-3-4",
            "pair": [
                "Then do Ctrl-C,  so you just know you have the ssh command right so it prompts for a password.\n",
                "It will prompt for a password. Enter the password. Now Exit.\n"
            ]
        },
        {
            "id": "163-4-5",
            "pair": [
                "It will prompt for a password. Enter the password. Now Exit.\n",
                "Then the next time you ssh in, it should go in automatically.\n"
            ]
        },
        {
            "id": "163-5-6",
            "pair": [
                "Then the next time you ssh in, it should go in automatically.\n",
                "If you don't already have keys on A, then run  $ssh-keygen\n"
            ]
        },
        {
            "id": "163-6-7",
            "pair": [
                "If you don't already have keys on A, then run  $ssh-keygen\n",
                "In connecting from A to B, All commands mentioned here so, ssh or ssh-copy-id or ssh-keygen, are run from A.\n"
            ]
        },
        {
            "id": "163-7-8",
            "pair": [
                "In connecting from A to B, All commands mentioned here so, ssh or ssh-copy-id or ssh-keygen, are run from A.\n",
                "I have two RH linux hosts, same build 'n everything, call them A & B.\n"
            ]
        },
        {
            "id": "163-8-9",
            "pair": [
                "I have two RH linux hosts, same build 'n everything, call them A & B.\n",
                "I can ssh happily from A to B using a public key, but I simply cannot login to A from B (or from my desktop using putty & pageant for that matter) without being prompted for a password. It seems there must be something slightly different with A, but I can't fathom what.\n"
            ]
        },
        {
            "id": "163-9-10",
            "pair": [
                "I can ssh happily from A to B using a public key, but I simply cannot login to A from B (or from my desktop using putty & pageant for that matter) without being prompted for a password. It seems there must be something slightly different with A, but I can't fathom what.\n",
                "The authorized_keys file and .ssh directory permissions are correct & PubkeyAuthentication is 'yes' in sshd_config.\n"
            ]
        },
        {
            "id": "163-10-11",
            "pair": [
                "The authorized_keys file and .ssh directory permissions are correct & PubkeyAuthentication is 'yes' in sshd_config.\n",
                "Is there anything else that could be missing or anything that could have been changed from base build that would cause this to happen? I have checked and double checked the above and regenerated keys several times. I can also see it tries using the key when running ssh -v A, but fails."
            ]
        }
    ],
    [
        {
            "id": "164-1-2",
            "pair": [
                "I think Brendan is right. Chrome seems to look for a field whose name is \"username\" along with the password field. If the username field is missing, nothing gets filled in. Also, if the username field has some other name, such as \"ID\", or if the username field is on a separate page from the password field, it probably won't fill in the name.\n",
                "Firefox does a much better job of recognizing when username and or password fields need to be filled in or at least completed. This is certainly an area where Chrome could be improved.\n"
            ]
        },
        {
            "id": "164-2-3",
            "pair": [
                "Firefox does a much better job of recognizing when username and or password fields need to be filled in or at least completed. This is certainly an area where Chrome could be improved.\n",
                "Chrome's basic AutoFill feature will be sufficient for most people, but for some people (like myself) it was sorely lacking. Hence, I created my own Autofill form filler extension. This extension has been working for a lot of people, so maybe it'll work for you also:\n"
            ]
        },
        {
            "id": "164-3-4",
            "pair": [
                "Chrome's basic AutoFill feature will be sufficient for most people, but for some people (like myself) it was sorely lacking. Hence, I created my own Autofill form filler extension. This extension has been working for a lot of people, so maybe it'll work for you also:\n",
                "https://chrome.google.com/extensions/detail/nlmmgnhgdeffjkdckmikfpnddkbbfkkk\n"
            ]
        },
        {
            "id": "164-4-5",
            "pair": [
                "https://chrome.google.com/extensions/detail/nlmmgnhgdeffjkdckmikfpnddkbbfkkk\n",
                "You will definitely have more power and control than Chrome's built-in autofill, and it will require one less click to autofill (zero clicks, actually -- no more having to select from a menu to initiate the autofill!)."
            ]
        }
    ],
    [
        {
            "id": "165-1-2",
            "pair": [
                "I always think that Principal component analysis is a very interesting tool. Many know its applications, few knows exactly what is going on.\n",
                "If you care about the math, you need to study eigen values and eigen vectors, but here let me explain pca in a very simple way. Imagine that you have a point in a 2-d space (x, y), this same point can be represented by using another two values, let us say (r, theta), where r is the distance from the origin and theta is the angle with respect to the +v x-axis. \n"
            ]
        },
        {
            "id": "165-2-3",
            "pair": [
                "If you care about the math, you need to study eigen values and eigen vectors, but here let me explain pca in a very simple way. Imagine that you have a point in a 2-d space (x, y), this same point can be represented by using another two values, let us say (r, theta), where r is the distance from the origin and theta is the angle with respect to the +v x-axis. \n",
                "Given any point (x,y), you can find the point (r, theta) corresponding to the point (x,y). You should be able to move between the two coordinates easily. Now let us ask the following question: \n"
            ]
        },
        {
            "id": "165-3-4",
            "pair": [
                "Given any point (x,y), you can find the point (r, theta) corresponding to the point (x,y). You should be able to move between the two coordinates easily. Now let us ask the following question: \n",
                "If I decide to ignore the theta dimension (for example set it to zero) how many points in the (x,y) you still can recover? All the points across the +ve x-axis can be recovered correctly. Now what about the r dimension, if we set it to zero, we can recover only one point which is the origin point. \n"
            ]
        },
        {
            "id": "165-4-5",
            "pair": [
                "If I decide to ignore the theta dimension (for example set it to zero) how many points in the (x,y) you still can recover? All the points across the +ve x-axis can be recovered correctly. Now what about the r dimension, if we set it to zero, we can recover only one point which is the origin point. \n",
                "Principal component analysis helps find this new coordinate system depending on the data, number of dimensions and variation across each dimension. The higher variation across dimension the more important to reserve this dimension. Very important to know that pca proposes both new coordinates and a measure of how much each dimension is important to recover the points in the original coordinate.\n"
            ]
        },
        {
            "id": "165-5-6",
            "pair": [
                "Principal component analysis helps find this new coordinate system depending on the data, number of dimensions and variation across each dimension. The higher variation across dimension the more important to reserve this dimension. Very important to know that pca proposes both new coordinates and a measure of how much each dimension is important to recover the points in the original coordinate.\n",
                "Both x and y dimensions are important and they experience the same variation.\n"
            ]
        },
        {
            "id": "165-6-7",
            "pair": [
                "Both x and y dimensions are important and they experience the same variation.\n",
                "If we transform, these data points into a new coordinate system (k,m), such that\n"
            ]
        },
        {
            "id": "165-7-8",
            "pair": [
                "If we transform, these data points into a new coordinate system (k,m), such that\n",
                "Then. K is very important 100% and m is not important at all 0% \n"
            ]
        },
        {
            "id": "165-8-9",
            "pair": [
                "Then. K is very important 100% and m is not important at all 0% \n",
                "You can recover the original points by using, x=k. And y=k. Note that the variation across the m dimension is zero and it is high across the k dimension.\n"
            ]
        },
        {
            "id": "165-9-10",
            "pair": [
                "You can recover the original points by using, x=k. And y=k. Note that the variation across the m dimension is zero and it is high across the k dimension.\n",
                "How much you want to reserve depends mainly on the application, usually > 95% is good. \n"
            ]
        },
        {
            "id": "165-10-11",
            "pair": [
                "How much you want to reserve depends mainly on the application, usually > 95% is good. \n",
                "For the second question, it is possible to recover less than 100% when you use less dimensions, but when you use 200 of 200 you should be able to recover 100%.\n"
            ]
        },
        {
            "id": "165-11-12",
            "pair": [
                "For the second question, it is possible to recover less than 100% when you use less dimensions, but when you use 200 of 200 you should be able to recover 100%.\n",
                "In the following code run after PCA i can see that X number of components explain Y % of cumulative explained variance (CEV). \n"
            ]
        },
        {
            "id": "165-12-13",
            "pair": [
                "In the following code run after PCA i can see that X number of components explain Y % of cumulative explained variance (CEV). \n",
                "1- What percentage of the CEV is typically acceptable e.g. 95% or 99%? (Or is it a case by case basis?)\n"
            ]
        },
        {
            "id": "165-13-14",
            "pair": [
                "1- What percentage of the CEV is typically acceptable e.g. 95% or 99%? (Or is it a case by case basis?)\n",
                "2- If 20 out of 200 components explain 95% of the CEV, what does this say about my data, what about when 200 out of 200 explain 95% ?"
            ]
        }
    ],
    [
        {
            "id": "166-1-2",
            "pair": [
                "I recommend using the Hyper-V management console to manage your virtual machine. I haven't had the privilege of using Windows 8 Hyper-V, but I manage a couple of Hyper-V servers. You can download the Windows 8 RSAT tools from Microsoft, which comes with the Hyper V management console. Once you have that installed point it to your localhost.\n",
                "Now, make sure you external network is set to the proper network card of your hyper-v server. In my example my external network is connected to the gigabit ethernet nic of my physical machine. To get to this menu, right click on your hyper v server from the management console, and select 'Virtual Network Manager'.\n"
            ]
        },
        {
            "id": "166-2-3",
            "pair": [
                "Now, make sure you external network is set to the proper network card of your hyper-v server. In my example my external network is connected to the gigabit ethernet nic of my physical machine. To get to this menu, right click on your hyper v server from the management console, and select 'Virtual Network Manager'.\n",
                "After that, for your VM settings, select the same Network that you configured for external access in the previous step.\n"
            ]
        },
        {
            "id": "166-3-4",
            "pair": [
                "After that, for your VM settings, select the same Network that you configured for external access in the previous step.\n",
                "And last (or first?) make sure That the physical computer has access to the network.\n"
            ]
        },
        {
            "id": "166-4-5",
            "pair": [
                "And last (or first?) make sure That the physical computer has access to the network.\n",
                "I successfully installed Ubuntu 12.10 server into the Hyper-V, but until now way I'm not able to connect it to my network. I created internal and external virtual switch, both pointing to the the physical adapter on the host machine, and nothing.\n"
            ]
        },
        {
            "id": "166-5-6",
            "pair": [
                "I successfully installed Ubuntu 12.10 server into the Hyper-V, but until now way I'm not able to connect it to my network. I created internal and external virtual switch, both pointing to the the physical adapter on the host machine, and nothing.\n",
                "Maybe there is some step I didn't do, or software i need to install. "
            ]
        }
    ],
    [
        {
            "id": "167-1-2",
            "pair": [
                "Indeed the problem that you speak of exists and makes up for a lot of questions, like \"for how long should I cache an answer? What if I have two projects whose commits rates are very different?\". There are some proprietary solutions which do what you are looking form i.e. if you use Atlassian Stash, it has a built-in plugin which manages checkout answers caching in order to lower the load on the server.\n",
                "The best solution anyway is different from what you want to do. The best and recommended solution is to use post-commit hooks, they exist in git, svn and I think in other vcs as well. Just have your repository trigger the build on your CI system, rather than the CI jobs polling continuosly. As you mentioned Jenkins (Hudson), the Git plugin for example already provides urls to perform this kind of activity.\n"
            ]
        },
        {
            "id": "167-2-3",
            "pair": [
                "The best solution anyway is different from what you want to do. The best and recommended solution is to use post-commit hooks, they exist in git, svn and I think in other vcs as well. Just have your repository trigger the build on your CI system, rather than the CI jobs polling continuosly. As you mentioned Jenkins (Hudson), the Git plugin for example already provides urls to perform this kind of activity.\n",
                "I would like to offer a continuous integration service (I'm planning to use hudson, but the solution should work for others as well) with a web interface where a user will define a SCM URL (e.g. a git URL) and the workspace/source root which is used for building should be cleaned (at least optionally) before building. This requires are lot of repeated checkouts which I would like to cache (i.e. make them be read from local storage instead of being fetched from a remote resource).\n"
            ]
        },
        {
            "id": "167-3-4",
            "pair": [
                "I would like to offer a continuous integration service (I'm planning to use hudson, but the solution should work for others as well) with a web interface where a user will define a SCM URL (e.g. a git URL) and the workspace/source root which is used for building should be cleaned (at least optionally) before building. This requires are lot of repeated checkouts which I would like to cache (i.e. make them be read from local storage instead of being fetched from a remote resource).\n",
                "Different SCMs (git, svn and mercurial/hg) use different protocols (HTTP, HTTPS, git, etc.), some of them can be cached (HTTP), others generally not (HTTPS without using a man-in-the-middle which is inacceptable for a trustworthy service imo - which I want to provide) or specifically not (I didn't find any git protocol cache servers).\n"
            ]
        },
        {
            "id": "167-4-5",
            "pair": [
                "Different SCMs (git, svn and mercurial/hg) use different protocols (HTTP, HTTPS, git, etc.), some of them can be cached (HTTP), others generally not (HTTPS without using a man-in-the-middle which is inacceptable for a trustworthy service imo - which I want to provide) or specifically not (I didn't find any git protocol cache servers).\n",
                "Caching HTTP isn't a problem, but few git hoster support it or redirect to HTTPS. I would like to support one protocol which reliably caches checkouts and suggest the user to use it.\n"
            ]
        },
        {
            "id": "167-5-6",
            "pair": [
                "Caching HTTP isn't a problem, but few git hoster support it or redirect to HTTPS. I would like to support one protocol which reliably caches checkouts and suggest the user to use it.\n",
                "Redirection via a SOCKS proxy can be achieved for HTTP and git protocol, but that doesn't allow caching. Other protocols like IGD can't be used for caching neither."
            ]
        }
    ],
    [
        {
            "id": "168-1-2",
            "pair": [
                "Hey, I know Flash works using Facebook, and I think I've seen some Java stuff on there too, but does anyone know if the Unity Player can be used in Facebook?\n",
                "This is kind of a nonsensical question based on a poor understanding of how Facebook apps work. Facebook doesn't really have anything to do with what technology exactly you are using; they simply embed your app as an iframe on their site. You build your app using whatever technology you want on your own server, and then simply tell Facebook what URL to point to.\n"
            ]
        },
        {
            "id": "168-2-3",
            "pair": [
                "This is kind of a nonsensical question based on a poor understanding of how Facebook apps work. Facebook doesn't really have anything to do with what technology exactly you are using; they simply embed your app as an iframe on their site. You build your app using whatever technology you want on your own server, and then simply tell Facebook what URL to point to.\n",
                "So basically anything that can be done in a web browser can by done as a Facebook app. Now there are certain technologies that get used more often for Facebook apps, but those trends are due to reasons that have little to do with Facebook, like the fact that most browsers have the Flash plugin.\n"
            ]
        },
        {
            "id": "168-3-4",
            "pair": [
                "So basically anything that can be done in a web browser can by done as a Facebook app. Now there are certain technologies that get used more often for Facebook apps, but those trends are due to reasons that have little to do with Facebook, like the fact that most browsers have the Flash plugin.\n",
                "Also, while you can build your app with pretty much any web technology, you will need to work with PHP or JavaScript (does Facebook provide an API for any other languages?) if you need to interface with their API (eg. to retrieve the player's friends list). That doesn't mean you actually have to build the entire app using PHP or JavaScript, but that you need to have at least a small script acting as the middle-man between your app and Facebook's API.\n"
            ]
        },
        {
            "id": "168-4-5",
            "pair": [
                "Also, while you can build your app with pretty much any web technology, you will need to work with PHP or JavaScript (does Facebook provide an API for any other languages?) if you need to interface with their API (eg. to retrieve the player's friends list). That doesn't mean you actually have to build the entire app using PHP or JavaScript, but that you need to have at least a small script acting as the middle-man between your app and Facebook's API.\n",
                "(I'm not criticizing you btw; until recently I didn't understand how Facebook apps work either, and thought they were hosted on Facebook's servers or something.)"
            ]
        }
    ],
    [
        {
            "id": "169-1-2",
            "pair": [
                "It would make sense that mouseButton won't have proper values except for events that involve the mouse button - MouseButtonReleased and MouseButtonPressed.\n",
                "If for example you wanted the mouse position for MouseMoved events, you should be looking at the mouseMove member, which is of type MouseMoveEvent.\n"
            ]
        },
        {
            "id": "169-2-3",
            "pair": [
                "If for example you wanted the mouse position for MouseMoved events, you should be looking at the mouseMove member, which is of type MouseMoveEvent.\n",
                "Always use the right event data for the event type. A mapping can be found in source code:\n"
            ]
        },
        {
            "id": "169-3-4",
            "pair": [
                "Always use the right event data for the event type. A mapping can be found in source code:\n",
                "  223         SizeEvent             size;              \n"
            ]
        },
        {
            "id": "169-4-5",
            "pair": [
                "  223         SizeEvent             size;              \n",
                "  224         KeyEvent              key;               \n"
            ]
        },
        {
            "id": "169-5-6",
            "pair": [
                "  224         KeyEvent              key;               \n",
                "  225         TextEvent             text;              \n"
            ]
        },
        {
            "id": "169-6-7",
            "pair": [
                "  225         TextEvent             text;              \n",
                "  226         MouseMoveEvent        mouseMove;         \n"
            ]
        },
        {
            "id": "169-7-8",
            "pair": [
                "  226         MouseMoveEvent        mouseMove;         \n",
                "  227         MouseButtonEvent      mouseButton;       \n"
            ]
        },
        {
            "id": "169-8-9",
            "pair": [
                "  227         MouseButtonEvent      mouseButton;       \n",
                "  228         MouseWheelEvent       mouseWheel;        \n"
            ]
        },
        {
            "id": "169-9-10",
            "pair": [
                "  228         MouseWheelEvent       mouseWheel;        \n",
                "  229         MouseWheelScrollEvent mouseWheelScroll;  \n"
            ]
        },
        {
            "id": "169-10-11",
            "pair": [
                "  229         MouseWheelScrollEvent mouseWheelScroll;  \n",
                "  230         JoystickMoveEvent     joystickMove;      \n"
            ]
        },
        {
            "id": "169-11-12",
            "pair": [
                "  230         JoystickMoveEvent     joystickMove;      \n",
                "  231         JoystickButtonEvent   joystickButton;    \n"
            ]
        },
        {
            "id": "169-12-13",
            "pair": [
                "  231         JoystickButtonEvent   joystickButton;    \n",
                "  232         JoystickConnectEvent  joystickConnect;   \n"
            ]
        },
        {
            "id": "169-13-14",
            "pair": [
                "  232         JoystickConnectEvent  joystickConnect;   \n",
                "  233         TouchEvent            touch;             \n"
            ]
        },
        {
            "id": "169-14-15",
            "pair": [
                "  233         TouchEvent            touch;             \n",
                "  234         SensorEvent           sensor;            \n"
            ]
        },
        {
            "id": "169-15-16",
            "pair": [
                "  234         SensorEvent           sensor;            \n",
                "FYI -858993460 in hexadecimal is CCCCCCCC. It is a garbage value used by microsoft compilers for uninitialised values.\n"
            ]
        },
        {
            "id": "169-16-17",
            "pair": [
                "FYI -858993460 in hexadecimal is CCCCCCCC. It is a garbage value used by microsoft compilers for uninitialised values.\n",
                "Using SFML, every event type will produce a large negative value for x and y of -858993460. \n"
            ]
        },
        {
            "id": "169-17-18",
            "pair": [
                "Using SFML, every event type will produce a large negative value for x and y of -858993460. \n",
                "I am wondering if this is an intended result or am I attempting to use these events incorrectly? \n"
            ]
        },
        {
            "id": "169-18-19",
            "pair": [
                "I am wondering if this is an intended result or am I attempting to use these events incorrectly? \n",
                "It seems to me there is a use case for knowing the position of the mouse cursor on MouseMoved, for example so you can have a button change to a different sprite as if you are hovering over it."
            ]
        }
    ],
    [
        {
            "id": "17-1-2",
            "pair": [
                "Thank you for all the comments above which enlighten me.\n",
                "ps -ew contains no BSD options, so I get the default output, which get the process name from /proc/<pid>/stat, which is no more than 15 characters.\n"
            ]
        },
        {
            "id": "17-2-3",
            "pair": [
                "ps -ew contains no BSD options, so I get the default output, which get the process name from /proc/<pid>/stat, which is no more than 15 characters.\n",
                "The -w option is indeed in effect, it does show wide output. If I shrink the window very narrow, ps -ew will try to wrap the lines so that I still get the full 15 characters in COMMAND column. But with out -w, like ps -e, long lines are are not seen, but they are still there since I can not see them but can grep them.\n"
            ]
        },
        {
            "id": "17-3-4",
            "pair": [
                "The -w option is indeed in effect, it does show wide output. If I shrink the window very narrow, ps -ew will try to wrap the lines so that I still get the full 15 characters in COMMAND column. But with out -w, like ps -e, long lines are are not seen, but they are still there since I can not see them but can grep them.\n",
                "As of ps -w <pid>, I find the answer from parser.c of procps source code. It implies BSD style output if there are PID arguments, which is not said in the man page: \n"
            ]
        },
        {
            "id": "17-4-5",
            "pair": [
                "As of ps -w <pid>, I find the answer from parser.c of procps source code. It implies BSD style output if there are PID arguments, which is not said in the man page: \n",
                "So ps -w <pid> will show BSD-style output, which will show command args (COMMAND) instead of the executable name (CMD). -w is in effect similar to the above example.\n"
            ]
        },
        {
            "id": "17-5-6",
            "pair": [
                "So ps -w <pid> will show BSD-style output, which will show command args (COMMAND) instead of the executable name (CMD). -w is in effect similar to the above example.\n",
                "I don't think \"wide output\" means what you want.  I created a script named foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo.sh and get the following\n"
            ]
        },
        {
            "id": "17-6-7",
            "pair": [
                "I don't think \"wide output\" means what you want.  I created a script named foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo_foo.sh and get the following\n",
                "The only thing w and -w seem to do is let lines wrap if I shrink the window so it won't fit on a line:"
            ]
        }
    ],
    [
        {
            "id": "170-1-2",
            "pair": [
                "I am working on gesture recognition in images and the best way that I am aware of, is whether using end to end approaches with deep neural networks or extracting body joint positions in an image and then doing some pre-processing on these joints positions and finally, feeding them to a classifier. I wanted to now what is the state of the art method for full body gesture recognition in images and if there are some good papers or a blog post reviewing different methods, as I have problem finding one.\n",
                "If you are planing to train a model which is using images taken by a single camera, I encourage you to take a look at this paper. They use a camera to track a person and recognize gestures involving arm motion. In this paper, two alternative methods for gesture recognition are compared: a template based approach and a neural network approach. \n"
            ]
        },
        {
            "id": "170-2-3",
            "pair": [
                "If you are planing to train a model which is using images taken by a single camera, I encourage you to take a look at this paper. They use a camera to track a person and recognize gestures involving arm motion. In this paper, two alternative methods for gesture recognition are compared: a template based approach and a neural network approach. \n",
                "However, another way which I suggest is using depth cameras such as Intel Realsense or Microsoft Kinect cameras. This paper uses depth cameras for full body gesture recognition. I think using these cameras take your project a step ahead and more accurate in initial states."
            ]
        }
    ],
    [
        {
            "id": "171-1-2",
            "pair": [
                "EncFS is a good non-container (not a fixed size like TrueCrypt or dm-crypt/LUKS) variable size folder/file encryption system. It's available on any Debian system too.\n",
                "Similar to eCryptFS, but EncFS keeps it's config file inside the encrypted folder and can be mounted rather \"portably\" and easily, whereas eCryptFS would require knowing the mount options or copying some extra config files to special folders before mounting.\n"
            ]
        },
        {
            "id": "171-2-3",
            "pair": [
                "Similar to eCryptFS, but EncFS keeps it's config file inside the encrypted folder and can be mounted rather \"portably\" and easily, whereas eCryptFS would require knowing the mount options or copying some extra config files to special folders before mounting.\n",
                "Also EncFS does not need root privileges to run (like eCryptFS does), so EncFS is more \"portable\" running as a regular user only.\n"
            ]
        },
        {
            "id": "171-3-4",
            "pair": [
                "Also EncFS does not need root privileges to run (like eCryptFS does), so EncFS is more \"portable\" running as a regular user only.\n",
                "The venerable TrueCrypt has versions for most OS's, including Linux distros and Windows. It is simple to create a folder as a TrueCrypt container. Caveat: questions were rised about the trustworthiness of that encryption, and VeraCrypt might be an alternative, though I have seen no independent audit of VeraCrypt yet.\n"
            ]
        },
        {
            "id": "171-4-5",
            "pair": [
                "The venerable TrueCrypt has versions for most OS's, including Linux distros and Windows. It is simple to create a folder as a TrueCrypt container. Caveat: questions were rised about the trustworthiness of that encryption, and VeraCrypt might be an alternative, though I have seen no independent audit of VeraCrypt yet.\n",
                "Another choice is simply to archive the folder with 7-Zip or PeaZip and use a password. Again, these applications are available for many OS's."
            ]
        }
    ],
    [
        {
            "id": "172-1-2",
            "pair": [
                "http://www.eightforums.com/virtualization/5137-how-add-virtual-network-editor-vmware-player.html\n",
                "Check the above link, read from the beginning but vmware player 5.0 procedure is further below the post on the above link.\n"
            ]
        },
        {
            "id": "172-2-3",
            "pair": [
                "Check the above link, read from the beginning but vmware player 5.0 procedure is further below the post on the above link.\n",
                "Instead of getting into all of the above and or vmware workstation 9.0, why don't you use virtualbox ?\n"
            ]
        },
        {
            "id": "172-3-4",
            "pair": [
                "Instead of getting into all of the above and or vmware workstation 9.0, why don't you use virtualbox ?\n",
                "http://www.oracle.com/technetwork/server-storage/virtualbox/downloads/index.html\n"
            ]
        },
        {
            "id": "172-4-5",
            "pair": [
                "http://www.oracle.com/technetwork/server-storage/virtualbox/downloads/index.html\n",
                "It's free, very rich in features, extensive command-line. have never had any problems with it.\n"
            ]
        },
        {
            "id": "172-5-6",
            "pair": [
                "It's free, very rich in features, extensive command-line. have never had any problems with it.\n",
                "Just use bridged adapter in virtualbox, if you want to ssh from your local lan provided you had it configured on your ubuntu vm, that should work, if you want to allow it over the internet / external access then open the appropriate ssh port on your router and forward it to your vm ip address in bridged mode.\n"
            ]
        },
        {
            "id": "172-6-7",
            "pair": [
                "Just use bridged adapter in virtualbox, if you want to ssh from your local lan provided you had it configured on your ubuntu vm, that should work, if you want to allow it over the internet / external access then open the appropriate ssh port on your router and forward it to your vm ip address in bridged mode.\n",
                "I would recommend using virtualbox instead of modifying vmware player 5.0, unless there is a specific feature of vmware which you want to utilize.\n"
            ]
        },
        {
            "id": "172-7-8",
            "pair": [
                "I would recommend using virtualbox instead of modifying vmware player 5.0, unless there is a specific feature of vmware which you want to utilize.\n",
                "I installed ubuntu on my laptop in vmware player 5.0.0 build-812388) under windows 7.\n"
            ]
        },
        {
            "id": "172-8-9",
            "pair": [
                "I installed ubuntu on my laptop in vmware player 5.0.0 build-812388) under windows 7.\n",
                "I do coding in ubuntu and would like to connect to ubuntu remotely with ssh. From My understanding, I need to use Bridged connection."
            ]
        }
    ],
    [
        {
            "id": "173-1-2",
            "pair": [
                "Laptops \"ramp up/down\", and as a result, there are lags. This greatly affects your overall performance.\n",
                "As to being a better CPU that is largely subjective. The performance points are very specific. So ultimately you will want to choose a CPU that better caters to the areas of performance you're looking for -- video, audio, (cache) bandwidth, \"lanes\", etc. Truth is, an i5 may actually turn out to be a better choice than an i7 depending upon your needs. Generation is also another factor. Attempting to compare PC CPU to Laptop CPU is much the same as comparing an automobile to a boat. Both have an Engine (CPU). But have very different intended goals/uses. Two very separate classes. :)\n"
            ]
        },
        {
            "id": "173-2-3",
            "pair": [
                "As to being a better CPU that is largely subjective. The performance points are very specific. So ultimately you will want to choose a CPU that better caters to the areas of performance you're looking for -- video, audio, (cache) bandwidth, \"lanes\", etc. Truth is, an i5 may actually turn out to be a better choice than an i7 depending upon your needs. Generation is also another factor. Attempting to compare PC CPU to Laptop CPU is much the same as comparing an automobile to a boat. Both have an Engine (CPU). But have very different intended goals/uses. Two very separate classes. :)\n",
                "Cooling enhancement(s) will improve your overall performance on a Laptop. Because, as you noted above, the CPU(s) are throttled when hot. To prevent failure/death. :)\n"
            ]
        },
        {
            "id": "173-3-4",
            "pair": [
                "Cooling enhancement(s) will improve your overall performance on a Laptop. Because, as you noted above, the CPU(s) are throttled when hot. To prevent failure/death. :)\n",
                "Speaking as someone who used to game on a laptop a cooling mat is definitely worth it. Even if it doesn't make a difference in performance laptops heat up very quickly while gaming and I have yet to see one that has enough airflow to properly cool itself under high load.\n"
            ]
        },
        {
            "id": "173-4-5",
            "pair": [
                "Speaking as someone who used to game on a laptop a cooling mat is definitely worth it. Even if it doesn't make a difference in performance laptops heat up very quickly while gaming and I have yet to see one that has enough airflow to properly cool itself under high load.\n",
                "As far as performance goes temperature may be making a difference. Another issue could be your power settings; by default most laptops are set to sacrifice some performance in exchange for battery life. Assuming that you are using Windows you can find those settings if you go to the control panel > power options > change plan settings > change advanced power settings. It is also entirely possible that your laptop is under-clocked slightly from the factory to keep people from melting their computer and extend battery life.\n"
            ]
        },
        {
            "id": "173-5-6",
            "pair": [
                "As far as performance goes temperature may be making a difference. Another issue could be your power settings; by default most laptops are set to sacrifice some performance in exchange for battery life. Assuming that you are using Windows you can find those settings if you go to the control panel > power options > change plan settings > change advanced power settings. It is also entirely possible that your laptop is under-clocked slightly from the factory to keep people from melting their computer and extend battery life.\n",
                "Unfortunately as far as performance goes a desktop is better in most circumstances just due to the cooling and power supply capabilities."
            ]
        }
    ],
    [
        {
            "id": "174-1-2",
            "pair": [
                "Taking a look at the source code (for Icecat 9.0.1), the URL you give does not appear to be present anywhere in the code explicitly. In fact, the string \"icecat/addons/\" does not appear anywhere in the source code. \n",
                "Given the fact that the incorrect URL does not include this string, though, it doesn't make much sense to be search for it. Instead, it would make more sense to search for \"/icecat/themes/\".\n"
            ]
        },
        {
            "id": "174-2-3",
            "pair": [
                "Given the fact that the incorrect URL does not include this string, though, it doesn't make much sense to be search for it. Instead, it would make more sense to search for \"/icecat/themes/\".\n",
                "To answer the question of where the incorrect URL may be in the source code, the file ./browser/app/firefox.js includes the line pref(\"extensions.getMoreThemesURL\", \"http://www.gnu.org/software/gnuzilla/addons.html#themes\");(on line 203). This is the correct URL for the themes page, right? If you are working with an older version of the source code, I would expect that the problem lies in that file. The file ./testing/mozmill/tests/shared-modules also contains the line {name: \"extensions.getMoreThemesURL\", old: \"addons.mozilla.org\", new: \"preview.addons.mozilla.org\"}, so that may also be of interest.\n"
            ]
        },
        {
            "id": "174-3-4",
            "pair": [
                "To answer the question of where the incorrect URL may be in the source code, the file ./browser/app/firefox.js includes the line pref(\"extensions.getMoreThemesURL\", \"http://www.gnu.org/software/gnuzilla/addons.html#themes\");(on line 203). This is the correct URL for the themes page, right? If you are working with an older version of the source code, I would expect that the problem lies in that file. The file ./testing/mozmill/tests/shared-modules also contains the line {name: \"extensions.getMoreThemesURL\", old: \"addons.mozilla.org\", new: \"preview.addons.mozilla.org\"}, so that may also be of interest.\n",
                "I've been using GNU Icecat Browser (libre Firefox version made by Gnuzilla) for some weeks. I've been diving through its menus and I found an error:\n"
            ]
        },
        {
            "id": "174-4-5",
            "pair": [
                "I've been using GNU Icecat Browser (libre Firefox version made by Gnuzilla) for some weeks. I've been diving through its menus and I found an error:\n",
                "When I get into the 'Customize Mode' (the one that lets you organize your toolbars and stuff), I find a little drop-down menu saying 'Themes', and an option called 'Get More Themes' inside it. \n"
            ]
        },
        {
            "id": "174-5-6",
            "pair": [
                "When I get into the 'Customize Mode' (the one that lets you organize your toolbars and stuff), I find a little drop-down menu saying 'Themes', and an option called 'Get More Themes' inside it. \n",
                "If I click it, it will redirect me here, which will return an error. I want to modify the line of code of that redirection, in order to fix that URL, but I can't find it with grep -Rli /icecat/addons/ /usr/share/. Maybe am I searching in the wrong folder? Maybe the command is wrong?..."
            ]
        }
    ],
    [
        {
            "id": "175-1-2",
            "pair": [
                "Have your declaration at the top.. Variables are declared in the body of a batch or procedure with the DECLARE statement and are assigned values by using either a SET or SELECT statement.\n",
                "In my mind there are a few ways you can achieve this, none of which are simple solutions. Which ever way you approach this, I think you will need to make two passes in to the database:\n"
            ]
        },
        {
            "id": "175-2-3",
            "pair": [
                "In my mind there are a few ways you can achieve this, none of which are simple solutions. Which ever way you approach this, I think you will need to make two passes in to the database:\n",
                "2) to get all the contacts associated with chosen company.\n"
            ]
        },
        {
            "id": "175-3-4",
            "pair": [
                "2) to get all the contacts associated with chosen company.\n",
                "First up you need to decide where/how you are going to store your company names. You could either store them in a local table, and provide a lookup via the CompanyName column using a Data Validation list option. This will mean that only the names in the list can be selected. Good for data integrity.\n"
            ]
        },
        {
            "id": "175-4-5",
            "pair": [
                "First up you need to decide where/how you are going to store your company names. You could either store them in a local table, and provide a lookup via the CompanyName column using a Data Validation list option. This will mean that only the names in the list can be selected. Good for data integrity.\n",
                "Or, if you don't want to store a local table then you will need to execute some vba code to connect to the database and download the company names. Iterate through the list of names and deliver the options to the user in whatever format you like.\n"
            ]
        },
        {
            "id": "175-5-6",
            "pair": [
                "Or, if you don't want to store a local table then you will need to execute some vba code to connect to the database and download the company names. Iterate through the list of names and deliver the options to the user in whatever format you like.\n",
                "Alternatively, you could manually write a list of names, but this will require additional maintenance when new company names are added (not ideal). Or, if you are really brave/mad, let the user write the company name in (I don't recommend this!!!!!).\n"
            ]
        },
        {
            "id": "175-6-7",
            "pair": [
                "Alternatively, you could manually write a list of names, but this will require additional maintenance when new company names are added (not ideal). Or, if you are really brave/mad, let the user write the company name in (I don't recommend this!!!!!).\n",
                "Back to your original question, if you are using MS Query you should be able to parameterise your query in the query editor (if I remember correctly?!). Parameterise it such that the parameter value is taken from the cell where the user selects the company name. MS Query should do the rest for you... provided the query is parametrised client-side (i.e. in excel). Alternatively, you could once again go back to vba and dynamically create the query string in the code, passing in the CompanyName to replace @CompanyName and executing a full un-parametrised query.\n"
            ]
        },
        {
            "id": "175-7-8",
            "pair": [
                "Back to your original question, if you are using MS Query you should be able to parameterise your query in the query editor (if I remember correctly?!). Parameterise it such that the parameter value is taken from the cell where the user selects the company name. MS Query should do the rest for you... provided the query is parametrised client-side (i.e. in excel). Alternatively, you could once again go back to vba and dynamically create the query string in the code, passing in the CompanyName to replace @CompanyName and executing a full un-parametrised query.\n",
                "If your query is stored server-side then perhaps create a stored procedure to accept @CompanyName as a parameter (you will need to write your DECLARE in your usp, as previously mentioned). Then, again using vba code, you can dynamically create the EXEC statement to execute against the connection.\n"
            ]
        },
        {
            "id": "175-8-9",
            "pair": [
                "If your query is stored server-side then perhaps create a stored procedure to accept @CompanyName as a parameter (you will need to write your DECLARE in your usp, as previously mentioned). Then, again using vba code, you can dynamically create the EXEC statement to execute against the connection.\n",
                "These are just a couple of options, I'm sure there are others. But if you are looking for code as an answer, then I think you may be out of luck as there is more going on here than just \"declare the scalar variable @CompanyName\"."
            ]
        }
    ],
    [
        {
            "id": "176-1-2",
            "pair": [
                "I'm fairly new to shaders and OpenGL so bare with me please, I just want to make sure I'm doing it correctly! Now I'm using LibGDX in order to create a simple 3d diagram for my company. I have a few models that I need to display as grey until they are active, where they will fade to a brighter blue, probably gain some emission value so that they're brighter.\n",
                "Now, when I export my models from Blender to .fbx, and then run Fbx-Conv to generate .g3dj files, I get some output like the following:\n"
            ]
        },
        {
            "id": "176-2-3",
            "pair": [
                "Now, when I export my models from Blender to .fbx, and then run Fbx-Conv to generate .g3dj files, I get some output like the following:\n",
                "Which is all good and displays what I want, but I've been reading about shaders the past few days and see that in a lot of cases LibGDX suggests extending the default shader and making tweaks that way.\n"
            ]
        },
        {
            "id": "176-3-4",
            "pair": [
                "Which is all good and displays what I want, but I've been reading about shaders the past few days and see that in a lot of cases LibGDX suggests extending the default shader and making tweaks that way.\n",
                "From a computational point of view, what is the best way to go about switching colors of models? Extending the default shader with some logic about if x is inactive, change shader to render as greyscale, else render as normal blue material? Or should I create a greyscale version of the material and switch them out? ( I'm not quite sure how to do this, also, because I apply the material in the g3dj file \u2014\u00a0could I do this in Java instead of the g3dj file? Does it cause a performance hit? If anyone could point me in the direction of some documentation somewhere.. )\n"
            ]
        },
        {
            "id": "176-4-5",
            "pair": [
                "From a computational point of view, what is the best way to go about switching colors of models? Extending the default shader with some logic about if x is inactive, change shader to render as greyscale, else render as normal blue material? Or should I create a greyscale version of the material and switch them out? ( I'm not quite sure how to do this, also, because I apply the material in the g3dj file \u2014\u00a0could I do this in Java instead of the g3dj file? Does it cause a performance hit? If anyone could point me in the direction of some documentation somewhere.. )\n",
                "To preserve performance, you should change the material setting in java rather than a \"if\" statement of the shader.\n"
            ]
        },
        {
            "id": "176-5-6",
            "pair": [
                "To preserve performance, you should change the material setting in java rather than a \"if\" statement of the shader.\n",
                "You can access the material the objects after the config file has been loaded using the functions described here"
            ]
        }
    ],
    [
        {
            "id": "177-1-2",
            "pair": [
                "In the attached screenshot you can see that I have three small partitions called \"Healthy (EFI System/OEM Partition)\".\n",
                "It's been quite a while since I've been in Disk Management, but the last time I was, I'm quite sure those weren't there. I recently started a Windows reinstallation a couple of times but stopped it before I was committed to the operation, so I'm wondering if those were created as a result of that. Just a guess...\n"
            ]
        },
        {
            "id": "177-2-3",
            "pair": [
                "It's been quite a while since I've been in Disk Management, but the last time I was, I'm quite sure those weren't there. I recently started a Windows reinstallation a couple of times but stopped it before I was committed to the operation, so I'm wondering if those were created as a result of that. Just a guess...\n",
                "OEM partition is used for recovery purpose when your system fails to boot up. When you insert your recovery disk it will fetch recovery points from this OEM partition and try to restore to the previous restoration points which will be mentioned in this partition.\n"
            ]
        },
        {
            "id": "177-3-4",
            "pair": [
                "OEM partition is used for recovery purpose when your system fails to boot up. When you insert your recovery disk it will fetch recovery points from this OEM partition and try to restore to the previous restoration points which will be mentioned in this partition.\n",
                "You can surely delete this partition without facing any issues. If you manually want to create such partition again then you can do it by booting with Windows Bootable disk.\n"
            ]
        },
        {
            "id": "177-4-5",
            "pair": [
                "You can surely delete this partition without facing any issues. If you manually want to create such partition again then you can do it by booting with Windows Bootable disk.\n",
                "Reference: https://www.urtech.ca/2018/12/everything-you-need-to-know-about-healthy-oem-partitions-and-how-to-easily-remove-them/"
            ]
        }
    ],
    [
        {
            "id": "178-1-2",
            "pair": [
                "There is no \"standard\" load balancing set up for MS SQL Server that you can run via a wizard.\n",
                "This would be a database architecture decision and implemented at the database level not the server level. Techniques would be:\n"
            ]
        },
        {
            "id": "178-2-3",
            "pair": [
                "This would be a database architecture decision and implemented at the database level not the server level. Techniques would be:\n",
                "If anyone disagrees, then I'd like to see an article by a respected known MS SQL figure saying clustering is load balancing. The articles quoted above do not mention load balancing. For example, A Microsoftie (Chas Boyd) says it is not here.\n"
            ]
        },
        {
            "id": "178-3-4",
            "pair": [
                "If anyone disagrees, then I'd like to see an article by a respected known MS SQL figure saying clustering is load balancing. The articles quoted above do not mention load balancing. For example, A Microsoftie (Chas Boyd) says it is not here.\n",
                "My question to the OP would be what kind of load do you expect?\n"
            ]
        },
        {
            "id": "178-4-5",
            "pair": [
                "My question to the OP would be what kind of load do you expect?\n",
                "Database servers are usually IO and memory bound, so proper disk configuration (with appropriate filegroups) and as much RAM as possible will go a lot further than any solution above.\n"
            ]
        },
        {
            "id": "178-5-6",
            "pair": [
                "Database servers are usually IO and memory bound, so proper disk configuration (with appropriate filegroups) and as much RAM as possible will go a lot further than any solution above.\n",
                "Don't forget: SQL Server 2005/Windows 2003 Enterprise 32-bit goes to 32GB RAM (of which you'd have 26-28GB data cache) and you are not limited by drive letters because of NTFS mount points. As for x64...\n"
            ]
        },
        {
            "id": "178-6-7",
            "pair": [
                "Don't forget: SQL Server 2005/Windows 2003 Enterprise 32-bit goes to 32GB RAM (of which you'd have 26-28GB data cache) and you are not limited by drive letters because of NTFS mount points. As for x64...\n",
                "New technologies for database load balancing, separate from SQL Server, are available. These software solutions integrate with Always On in 2012 or 2014 and support automatic read/write split and other load balancing techniques. Look for NetScaler DataStream or ScaleArc for SQL Server as two examples of this transparent SQL load balancing software."
            ]
        }
    ],
    [
        {
            "id": "179-1-2",
            "pair": [
                "It seems that most of the posts I can find show me how to take a VMDK and convert it to an FTK Image for processing. I'd like to go the other way, and get a bootable VMWare image.\n",
                "I've found the Virtual Forensic Computing tool, but I'm just a hobbyist and cannot afford to buy it. I was hoping LiveView would have been my savior, but it doesn't like the FTK image format.\n"
            ]
        },
        {
            "id": "179-2-3",
            "pair": [
                "I've found the Virtual Forensic Computing tool, but I'm just a hobbyist and cannot afford to buy it. I was hoping LiveView would have been my savior, but it doesn't like the FTK image format.\n",
                "Is there a tool that will do this for me for free with a little work? For example, I know that if I could get it to just a raw \"dd\" style dump, then I could use qemu-img covert to make it a VMDK.\n"
            ]
        },
        {
            "id": "179-3-4",
            "pair": [
                "Is there a tool that will do this for me for free with a little work? For example, I know that if I could get it to just a raw \"dd\" style dump, then I could use qemu-img covert to make it a VMDK.\n",
                "Your question is confusing because you refer to the \"FTK Image\" format which doesn't exist.  Perhaps you are referring to AccessData's AD1 format, which is a logical image and doesn't include things like unallocated space?\n"
            ]
        },
        {
            "id": "179-4-5",
            "pair": [
                "Your question is confusing because you refer to the \"FTK Image\" format which doesn't exist.  Perhaps you are referring to AccessData's AD1 format, which is a logical image and doesn't include things like unallocated space?\n",
                "FTK Imager is a free tool that can create and convert disk images between many formats including the common ones like Encase E01, RAW dd, SMART S01, and Advanced Forensic Format AFF.  It sounds like your problem will be solved if you can convert your file to a RAW/dd image since you can use qemu at that point.  FTK Imager should be able to convert the format you are calling \"FTK Image\" into a RAW/dd and it meets your requirements of being free.  Use the following steps:\n"
            ]
        },
        {
            "id": "179-5-6",
            "pair": [
                "FTK Imager is a free tool that can create and convert disk images between many formats including the common ones like Encase E01, RAW dd, SMART S01, and Advanced Forensic Format AFF.  It sounds like your problem will be solved if you can convert your file to a RAW/dd image since you can use qemu at that point.  FTK Imager should be able to convert the format you are calling \"FTK Image\" into a RAW/dd and it meets your requirements of being free.  Use the following steps:\n",
                "Install FTK Imager or use the portable version and launch the application.  File -> Create Disk Image -> Select Source = Image File -> Select your original file -> Finish -> Add Image Destination -> Raw (dd) -> Next -> Next -> Select a destination folder and filename -> Image Fragment Size = 0 -> Finish."
            ]
        }
    ],
    [
        {
            "id": "18-1-2",
            "pair": [
                "I'm creating a new HPE configuration for a Hyper-V cluster.\n",
                "Need also to consider Veeam and Off-Host Backup Proxy. \n"
            ]
        },
        {
            "id": "18-2-3",
            "pair": [
                "Need also to consider Veeam and Off-Host Backup Proxy. \n",
                "This might be a silly question, but considering HPE MSA 2052, does it make any difference if it's SAS or SAN to Veeam in this subject (Hyper-V cluster / Off-Host Backup Proxy)\n"
            ]
        },
        {
            "id": "18-3-4",
            "pair": [
                "This might be a silly question, but considering HPE MSA 2052, does it make any difference if it's SAS or SAN to Veeam in this subject (Hyper-V cluster / Off-Host Backup Proxy)\n",
                "With a SAS storage array you need to connect the backup proxy to the array directly (at least one link per controller). You could use a SAS expander (\"SAS switch\") to share ports between hosts and proxy but that is very costly as are fiber links with extended reach.\n"
            ]
        },
        {
            "id": "18-4-5",
            "pair": [
                "With a SAS storage array you need to connect the backup proxy to the array directly (at least one link per controller). You could use a SAS expander (\"SAS switch\") to share ports between hosts and proxy but that is very costly as are fiber links with extended reach.\n",
                "Using the SAN/iSCSI model allows you to use a more budget-friendly (but adequate) Ethernet switch to share the storage ports between hosts and proxy. Additionally, you can run the iSCSI-over-Ethernet connection across multiple switches and could potentially even route it. Technically, it could even work over a VPN link (but likely iSCSI won't perform that well over WAN).\n"
            ]
        },
        {
            "id": "18-5-6",
            "pair": [
                "Using the SAN/iSCSI model allows you to use a more budget-friendly (but adequate) Ethernet switch to share the storage ports between hosts and proxy. Additionally, you can run the iSCSI-over-Ethernet connection across multiple switches and could potentially even route it. Technically, it could even work over a VPN link (but likely iSCSI won't perform that well over WAN).\n",
                "Overall, the SAN/iSCSI variant scales more easily - when you need to run multiple hosts and storage arrays. If you don't need that SAS is fine is is a bit easier to handle and configure."
            ]
        }
    ],
    [
        {
            "id": "180-1-2",
            "pair": [
                "Okay, first thing, I need to pick at this a little bit:\n",
                "Using READ UNCOMMITTED/NOLOCK should only be considered when the accuracy of the results is not critical, because that's what the transaction isolation level affects. I understand that the operation is not mission-critical, but that should not be the driver behind selecting a less restrictive isolation level.\n"
            ]
        },
        {
            "id": "180-2-3",
            "pair": [
                "Using READ UNCOMMITTED/NOLOCK should only be considered when the accuracy of the results is not critical, because that's what the transaction isolation level affects. I understand that the operation is not mission-critical, but that should not be the driver behind selecting a less restrictive isolation level.\n",
                "If you continue to use READ UNCOMMITTED, the locking overhead will be the same between the two methods (i.e., no locks will be taken). However, running multiple statements instead of one can affect other things, such as query compilation (which uses CPU), network chattiness, and the complexity of your application. In this case, I think the operation of sending a mailer would be relatively infrequent (I could be wrong), so the latter factor may be the most important to consider.\n"
            ]
        },
        {
            "id": "180-3-4",
            "pair": [
                "If you continue to use READ UNCOMMITTED, the locking overhead will be the same between the two methods (i.e., no locks will be taken). However, running multiple statements instead of one can affect other things, such as query compilation (which uses CPU), network chattiness, and the complexity of your application. In this case, I think the operation of sending a mailer would be relatively infrequent (I could be wrong), so the latter factor may be the most important to consider.\n",
                "If you choose to switch back to a higher isolation level for accuracy reasons, that is an entirely different ball game in terms of locking.\n"
            ]
        },
        {
            "id": "180-4-5",
            "pair": [
                "If you choose to switch back to a higher isolation level for accuracy reasons, that is an entirely different ball game in terms of locking.\n",
                "20k records is not that many. Are you SELECTing other data from other tables as part of your mailout which is slowing things down? Are you keeping your transaction open longer than you need?\n"
            ]
        },
        {
            "id": "180-5-6",
            "pair": [
                "20k records is not that many. Are you SELECTing other data from other tables as part of your mailout which is slowing things down? Are you keeping your transaction open longer than you need?\n",
                "Otherwise, can you use one of the opimistic concurrency isolation levels: Snapshot or Read Committed Snapshot? They should allow you to read without blocking."
            ]
        }
    ],
    [
        {
            "id": "181-1-2",
            "pair": [
                "I'm in the middle of migrating some applications from IIS 7.5 (2008R2) to IIS 10 (2016) and I'm beating my head against a wall over something.\n",
                "The applications use a backend virtual folder that points to a NAS for common file storage (in a load balanced bundle). I know on 2008 the trick of creating a local username and password that matches a username and password on the NAS and then, set the application pool to run as that user followed by creating the virtual folder pointed to the NAS as the same user.  All of that works great on 7.5 but on 10.0 it fails saying invalid username and password.  Even after trying 10 times one finger typing the username and password. \n"
            ]
        },
        {
            "id": "181-2-3",
            "pair": [
                "The applications use a backend virtual folder that points to a NAS for common file storage (in a load balanced bundle). I know on 2008 the trick of creating a local username and password that matches a username and password on the NAS and then, set the application pool to run as that user followed by creating the virtual folder pointed to the NAS as the same user.  All of that works great on 7.5 but on 10.0 it fails saying invalid username and password.  Even after trying 10 times one finger typing the username and password. \n",
                "I even tried changing it from the EqualLogic NAS to another windows machine to test and got the same issue so I am pretty sure that there has been a security change in IIS 10.0 but I'm not sure what.  \n"
            ]
        },
        {
            "id": "181-3-4",
            "pair": [
                "I even tried changing it from the EqualLogic NAS to another windows machine to test and got the same issue so I am pretty sure that there has been a security change in IIS 10.0 but I'm not sure what.  \n",
                "I've compared the settings on the application pools, virtual folders, etc side by side and they are all identical but no joy.  \n"
            ]
        },
        {
            "id": "181-4-5",
            "pair": [
                "I've compared the settings on the application pools, virtual folders, etc side by side and they are all identical but no joy.  \n",
                "Has anyone hit this hurdle in an IIS newer than 7.5 ? I don't have access to an 2012R2 server to test it with so I can't confirm if the problem started in 2012R2 or 2016. \n"
            ]
        },
        {
            "id": "181-5-6",
            "pair": [
                "Has anyone hit this hurdle in an IIS newer than 7.5 ? I don't have access to an 2012R2 server to test it with so I can't confirm if the problem started in 2012R2 or 2016. \n",
                "without knowing more about your environment I would try the following:\n"
            ]
        },
        {
            "id": "181-6-7",
            "pair": [
                "without knowing more about your environment I would try the following:\n",
                "The version of SMB used between a client and the server will be the highest dialect supported by both the client and server.\n"
            ]
        },
        {
            "id": "181-7-8",
            "pair": [
                "The version of SMB used between a client and the server will be the highest dialect supported by both the client and server.\n",
                "Maybe your NAS is using an SMB version that is incompatible with the current version of your Windows server. If so, you could fix that on both ends either the NAS or allow the server to use a lower version of SMB (which I do not recommend).\n"
            ]
        },
        {
            "id": "181-8-9",
            "pair": [
                "Maybe your NAS is using an SMB version that is incompatible with the current version of your Windows server. If so, you could fix that on both ends either the NAS or allow the server to use a lower version of SMB (which I do not recommend).\n",
                "In September 2016, security bulletin MS16-114 was released. It described a fatal security flaw in SMB1.\n"
            ]
        },
        {
            "id": "181-9-10",
            "pair": [
                "In September 2016, security bulletin MS16-114 was released. It described a fatal security flaw in SMB1.\n",
                "Of course your problem could be totally different but I would just check the SMB version used.\n"
            ]
        },
        {
            "id": "181-10-11",
            "pair": [
                "Of course your problem could be totally different but I would just check the SMB version used.\n",
                "You wrote \"changing it from the EqualLogic NAS to another windows machine to test\" did not help. What OS was that Windows machine running, what SMB version did it use?"
            ]
        }
    ],
    [
        {
            "id": "182-1-2",
            "pair": [
                "I am planning to use m1.small in Elastic Beanstalk for the image processing of my app. It can scale behind a load balancer and provide more processing power when the app needs to based on specific metrics.\n",
                "I also considered using m3.medium which is more optimized for compute intensive application like image processing. The problem is that my application runs on ASP.NET 4.5 and when running on Windows 2012, I assume that the m3.medium 4GB SSD drive won't work for this type of instance.\n"
            ]
        },
        {
            "id": "182-2-3",
            "pair": [
                "I also considered using m3.medium which is more optimized for compute intensive application like image processing. The problem is that my application runs on ASP.NET 4.5 and when running on Windows 2012, I assume that the m3.medium 4GB SSD drive won't work for this type of instance.\n",
                "The other option is m1.medium that has 160GB of storage space but less powerful than the m3.\n"
            ]
        },
        {
            "id": "182-3-4",
            "pair": [
                "The other option is m1.medium that has 160GB of storage space but less powerful than the m3.\n",
                "Which server considering that I need to run my application on Windows and I am running it behind a load balancer (Elastic Beanstalk) and can add remove servers using auto scale, which one do you recommend for image processing?\n"
            ]
        },
        {
            "id": "182-4-5",
            "pair": [
                "Which server considering that I need to run my application on Windows and I am running it behind a load balancer (Elastic Beanstalk) and can add remove servers using auto scale, which one do you recommend for image processing?\n",
                "Why don't you try a couple out. You can always launch a different one, with the same image. If you use amazons build-in loadbalancer, it's very easy to do so. I have found out, that using a cluster of micro instances works well for image processing."
            ]
        }
    ],
    [
        {
            "id": "183-1-2",
            "pair": [
                "I would suggest that you do NOT try to create a mathematical formula for this, since you will sooner or later find that the curve needs special shapes - it will be very tricky to achieve these with an f(age).\n",
                "Instead write the value pairs in a table and linearly interpolate between two age values and grab the corresponding other value. If you have for example only 10 rows, for ages 0...90 years, with 10 year intervals (ie. age in column 1 and for example strength in column 2), the curve will have a bit of sharpness, but otoh it is easy to insert more points AND you can use for example  Newton interpolation (C code sample) to smooth between the points; this will create the nice roundness you'd get when drawing between the points with a pen, by hand.\n"
            ]
        },
        {
            "id": "183-2-3",
            "pair": [
                "Instead write the value pairs in a table and linearly interpolate between two age values and grab the corresponding other value. If you have for example only 10 rows, for ages 0...90 years, with 10 year intervals (ie. age in column 1 and for example strength in column 2), the curve will have a bit of sharpness, but otoh it is easy to insert more points AND you can use for example  Newton interpolation (C code sample) to smooth between the points; this will create the nice roundness you'd get when drawing between the points with a pen, by hand.\n",
                "It is easy to create and adhoc-view a suitable table in Excel or OpenOffice, then copy/paste it. Or simply write it into a .txt file.\n"
            ]
        },
        {
            "id": "183-3-4",
            "pair": [
                "It is easy to create and adhoc-view a suitable table in Excel or OpenOffice, then copy/paste it. Or simply write it into a .txt file.\n",
                "Above all, you'll open for further development and maintenance this way. Doing it with some polynomial formulas seems to me like harakiri.\n"
            ]
        },
        {
            "id": "183-4-5",
            "pair": [
                "Above all, you'll open for further development and maintenance this way. Doing it with some polynomial formulas seems to me like harakiri.\n",
                "My workerclass has physical attributes such as physical_fitness and sexual_fitness which are intended to be functions of their age (the attribute is, unsurprisingly, called age). When the class is initialized, the initial values of XYZ_fitness are computed by a randomized normal distribution. \n"
            ]
        },
        {
            "id": "183-5-6",
            "pair": [
                "My workerclass has physical attributes such as physical_fitness and sexual_fitness which are intended to be functions of their age (the attribute is, unsurprisingly, called age). When the class is initialized, the initial values of XYZ_fitness are computed by a randomized normal distribution. \n",
                "In each game tick, worker's grow a year older. I want to appreciate/depreciate the values of their physical attributes such that, for example, they increase until they are at their maximum value around age 35, and decline with age after that. What is an efficient way of defining this function?\n"
            ]
        },
        {
            "id": "183-6-7",
            "pair": [
                "In each game tick, worker's grow a year older. I want to appreciate/depreciate the values of their physical attributes such that, for example, they increase until they are at their maximum value around age 35, and decline with age after that. What is an efficient way of defining this function?\n",
                "Would this involve something like running their age through a quadratic function with its maxima at the age value 35? I'm sure there's some elegant math/statistics here that's escaping me. "
            ]
        }
    ],
    [
        {
            "id": "184-1-2",
            "pair": [
                "It's possible that you hit SQL's size limit for batch size. Check out https://msdn.microsoft.com/en-us/library/ms143432.aspx.\n",
                "If your file is just line after line of INSERT INTO [Table] (field, field) VALUES (value, value); then you have some better options than splitting the file.\n"
            ]
        },
        {
            "id": "184-2-3",
            "pair": [
                "If your file is just line after line of INSERT INTO [Table] (field, field) VALUES (value, value); then you have some better options than splitting the file.\n",
                "I am currently doing an internship and I received a 42 gb ms sql-server import script.\n"
            ]
        },
        {
            "id": "184-3-4",
            "pair": [
                "I am currently doing an internship and I received a 42 gb ms sql-server import script.\n",
                "I tried to import the script using the ms SQLCMD tool all at once.\n"
            ]
        },
        {
            "id": "184-4-5",
            "pair": [
                "I tried to import the script using the ms SQLCMD tool all at once.\n",
                "It ran for 30 minutes and then I got the following error:\n"
            ]
        },
        {
            "id": "184-5-6",
            "pair": [
                "It ran for 30 minutes and then I got the following error:\n",
                "So naturally I checked line 37 and searched for the value 7500 but I could not find it.\n"
            ]
        },
        {
            "id": "184-6-7",
            "pair": [
                "So naturally I checked line 37 and searched for the value 7500 but I could not find it.\n",
                "Then I tried to find the last executed query in my server log.\n"
            ]
        },
        {
            "id": "184-7-8",
            "pair": [
                "Then I tried to find the last executed query in my server log.\n",
                "Having found the last inserted query, I knew what line in my script was causing the error.\n"
            ]
        },
        {
            "id": "184-8-9",
            "pair": [
                "Having found the last inserted query, I knew what line in my script was causing the error.\n",
                "Unfortunately I could not find the value 7500 in the error causing line.\n"
            ]
        },
        {
            "id": "184-9-10",
            "pair": [
                "Unfortunately I could not find the value 7500 in the error causing line.\n",
                "So I cleared my database, and tried to import the script a second time with the exact same error as result.\n"
            ]
        },
        {
            "id": "184-10-11",
            "pair": [
                "So I cleared my database, and tried to import the script a second time with the exact same error as result.\n",
                "So I tried to split the 42 gb script into 3000 smaller scripts.\n"
            ]
        },
        {
            "id": "184-11-12",
            "pair": [
                "So I tried to split the 42 gb script into 3000 smaller scripts.\n",
                "I am now running the 3000 scripts sequentially and it seems to be running just fine.\n"
            ]
        },
        {
            "id": "184-12-13",
            "pair": [
                "I am now running the 3000 scripts sequentially and it seems to be running just fine.\n",
                "It has been running for 4 hours now and it is past the point of the previous error.\n"
            ]
        },
        {
            "id": "184-13-14",
            "pair": [
                "It has been running for 4 hours now and it is past the point of the previous error.\n",
                "My question is: Did I find a bug in SQLCMD or is this a known problem with larger import scripts? If this is a know problem what is the best way to counter this (Splitting the file took a long time)"
            ]
        }
    ],
    [
        {
            "id": "185-1-2",
            "pair": [
                "I'm trying to transfer the contents of some VERY old laptops which have Windows 3.1 on them. The only way I can manage this is via a serial cable (no network on the laptops).\n",
                "I've got Kermit on the W3.1 end but it is playing up and won't run nicely, apart from being a pig of a UI which I can't get my head around.\n"
            ]
        },
        {
            "id": "185-2-3",
            "pair": [
                "I've got Kermit on the W3.1 end but it is playing up and won't run nicely, apart from being a pig of a UI which I can't get my head around.\n",
                "So I'm after some alternatives for transferring the contents of a 2gb directory from the laptop over a serial link to the Vista PC.\n"
            ]
        },
        {
            "id": "185-3-4",
            "pair": [
                "So I'm after some alternatives for transferring the contents of a 2gb directory from the laptop over a serial link to the Vista PC.\n",
                "Ideally, something open source would be great. I've looked at ripping out the HD from the laptops but they are so ancient that they are non-standard and won't fit in a PC any more.\n"
            ]
        },
        {
            "id": "185-4-5",
            "pair": [
                "Ideally, something open source would be great. I've looked at ripping out the HD from the laptops but they are so ancient that they are non-standard and won't fit in a PC any more.\n",
                "A lightweight file transfer tool is available with Zip. It provides a command line tool that is run on the server end to wait for a connection and on the client to send and receive files. There are many options available and the software supports both serial and parallel cables (pinouts included).\n"
            ]
        },
        {
            "id": "185-5-6",
            "pair": [
                "A lightweight file transfer tool is available with Zip. It provides a command line tool that is run on the server end to wait for a connection and on the client to send and receive files. There are many options available and the software supports both serial and parallel cables (pinouts included).\n",
                "As a bonus, the application includes a tool and instructions to copy itself to another machine with just an OS. Great tool to bootstrap communication to a factory controller with no slot for a network card, and routine floppy drive access is not practical."
            ]
        }
    ],
    [
        {
            "id": "186-1-2",
            "pair": [
                "Find out what port you are using to stream to the X-Box and configure the windows firewall to only allow traffic over that port. \n",
                "You can open the firewall settings by typing in wf.msc into the search box on the start menu.\n"
            ]
        },
        {
            "id": "186-2-3",
            "pair": [
                "You can open the firewall settings by typing in wf.msc into the search box on the start menu.\n",
                "For Xbox to work, these are the known ports for its usage:\n"
            ]
        },
        {
            "id": "186-3-4",
            "pair": [
                "For Xbox to work, these are the known ports for its usage:\n",
                "Now, these are ports that are reliant on your router, and not you're PC that you're streaming with however, the data streaming is most likely occurring on one of these.\n"
            ]
        },
        {
            "id": "186-4-5",
            "pair": [
                "Now, these are ports that are reliant on your router, and not you're PC that you're streaming with however, the data streaming is most likely occurring on one of these.\n",
                "To find which port your PC is actually using to stream content to xbox, you'll want to run an nmap scan on your PC's local address, and your Xbox's local address.\n"
            ]
        },
        {
            "id": "186-5-6",
            "pair": [
                "To find which port your PC is actually using to stream content to xbox, you'll want to run an nmap scan on your PC's local address, and your Xbox's local address.\n",
                "a) nmap -sS -sU -p 1-65535 -T4 -A -v 192.168.1.ip_of_your_xbox\n"
            ]
        },
        {
            "id": "186-6-7",
            "pair": [
                "a) nmap -sS -sU -p 1-65535 -T4 -A -v 192.168.1.ip_of_your_xbox\n",
                "b) nmap -sS -sU -p 1-65535 -T4 -A -v 192.168.1.ip_of_your_pc\n"
            ]
        },
        {
            "id": "186-7-8",
            "pair": [
                "b) nmap -sS -sU -p 1-65535 -T4 -A -v 192.168.1.ip_of_your_pc\n",
                "When the scans have finished and you have identified the ports necessary, its time to close off the firewall.\n"
            ]
        },
        {
            "id": "186-8-9",
            "pair": [
                "When the scans have finished and you have identified the ports necessary, its time to close off the firewall.\n",
                "To close off all connections, inbound and outbound on your computer, you must execute the following command strings in an elevated command prompt window.\n"
            ]
        },
        {
            "id": "186-9-10",
            "pair": [
                "To close off all connections, inbound and outbound on your computer, you must execute the following command strings in an elevated command prompt window.\n",
                "This command will reset all changes made this process:\n"
            ]
        },
        {
            "id": "186-10-11",
            "pair": [
                "This command will reset all changes made this process:\n",
                "This command will delete all rules on your firewall (Not that good of an idea)\n"
            ]
        },
        {
            "id": "186-11-12",
            "pair": [
                "This command will delete all rules on your firewall (Not that good of an idea)\n",
                "Delete all rules: netsh advfirewall firewall delete rule all"
            ]
        }
    ],
    [
        {
            "id": "187-1-2",
            "pair": [
                "If brctl doesn't give you what you need (perhaps the software layer adds too much latency) the next best bet is to use a span or monitor port on your switch. The traffic isn't bridged, it's merely copied to the destination port where whatever monitoring you need to do can happen. The down side is that you can't filter on that traffic, but you can monitor it.\n",
                "Yes, such a thing exists. They're really popular inside home WiFi routers -- that's how your average Linksys router works: the four \"LAN\" ports are connected to a single bridged adapter, which is then bridged in software to the 802.11 adapter, and then that is NAT'ed to the \"WAN\" port.\n"
            ]
        },
        {
            "id": "187-2-3",
            "pair": [
                "Yes, such a thing exists. They're really popular inside home WiFi routers -- that's how your average Linksys router works: the four \"LAN\" ports are connected to a single bridged adapter, which is then bridged in software to the 802.11 adapter, and then that is NAT'ed to the \"WAN\" port.\n",
                "However, it's tough to find one you can just plug into any computer -- the demand just isn't there. Instead, I'd recommend doing simple software bridging using brctl -- that'll create a \"virtual\" adapter, usually called \"br0\", which represents the bridge of the two adapters. You can treat that virtual adapter just like any physical one.\n"
            ]
        },
        {
            "id": "187-3-4",
            "pair": [
                "However, it's tough to find one you can just plug into any computer -- the demand just isn't there. Instead, I'd recommend doing simple software bridging using brctl -- that'll create a \"virtual\" adapter, usually called \"br0\", which represents the bridge of the two adapters. You can treat that virtual adapter just like any physical one.\n",
                "The overhead of bridging the adapters in software is negligible, even with mediocre hardware."
            ]
        }
    ],
    [
        {
            "id": "188-1-2",
            "pair": [
                "All the above answers will work perfectly well, but may cause false positives if you've got the string in more than just source code. Say you are looking for where you use the FooBar class in a java project, you can search for all files named *.java and grep just those.\n",
                "The section in between the $() will expand to the output of that command. The -H prints out the filename. It's the default where there is more than one file name, but we explicitly add it here in case the expansion only returns one file name. The -n prints the line number of the match.\n"
            ]
        },
        {
            "id": "188-2-3",
            "pair": [
                "The section in between the $() will expand to the output of that command. The -H prints out the filename. It's the default where there is more than one file name, but we explicitly add it here in case the expansion only returns one file name. The -n prints the line number of the match.\n",
                "This will work for small projects, but if you're dealing with a larger project, there the potential for the find to expand to a string longer than the command line limit. It's therefore safer to use:\n"
            ]
        },
        {
            "id": "188-3-4",
            "pair": [
                "This will work for small projects, but if you're dealing with a larger project, there the potential for the find to expand to a string longer than the command line limit. It's therefore safer to use:\n",
                "xargs will take input from standard in and split it up so it fits on the command line, running the command multiple times if necessary. The -print0 and -0 are required to deal with files with spaces in their name.\n"
            ]
        },
        {
            "id": "188-4-5",
            "pair": [
                "xargs will take input from standard in and split it up so it fits on the command line, running the command multiple times if necessary. The -print0 and -0 are required to deal with files with spaces in their name.\n",
                "You can also look for all the filesnames with a match in using:\n"
            ]
        },
        {
            "id": "188-5-6",
            "pair": [
                "You can also look for all the filesnames with a match in using:\n",
                "The -l will just display the file name and stop looking in a particular file as soon as it finds a match.\n"
            ]
        },
        {
            "id": "188-6-7",
            "pair": [
                "The -l will just display the file name and stop looking in a particular file as soon as it finds a match.\n",
                "If you're going to be doing this sort of thing frequently, you may want to look at a tool like ctags or exuberant tags, which maintain a search index of your source code and can answer these sorts of questions quickly. Both vim and emacs have support for these tools. You may also like to look at an IDE like eclipse, which can do cross referencing very well.\n"
            ]
        },
        {
            "id": "188-7-8",
            "pair": [
                "If you're going to be doing this sort of thing frequently, you may want to look at a tool like ctags or exuberant tags, which maintain a search index of your source code and can answer these sorts of questions quickly. Both vim and emacs have support for these tools. You may also like to look at an IDE like eclipse, which can do cross referencing very well.\n",
                "(I know you were looking for a class definition, but in Java a class will be defined in a file with the same name as the class. If you were doing cpp, you could do something like:\n"
            ]
        },
        {
            "id": "188-8-9",
            "pair": [
                "(I know you were looking for a class definition, but in Java a class will be defined in a file with the same name as the class. If you were doing cpp, you could do something like:\n",
                "This will also print the line number next to each match."
            ]
        }
    ],
    [
        {
            "id": "189-1-2",
            "pair": [
                "So, what changes if I choose to \"watch\" a thread? Due to the notifications, I have to look through everything that's not ignored anyway.\n",
                "As best I can tell you is that the 'watch' mark puts an eyeball icon on a message and then any subsequent replies to that message come in also get the eyeball icon.  This can be useful for threads of messages you want to 'keep an eye on'. Originally this was a newsgroups-only feature for keeping an eye on interesting threads but around TB 24 they added it for mail as well.\n"
            ]
        },
        {
            "id": "189-2-3",
            "pair": [
                "As best I can tell you is that the 'watch' mark puts an eyeball icon on a message and then any subsequent replies to that message come in also get the eyeball icon.  This can be useful for threads of messages you want to 'keep an eye on'. Originally this was a newsgroups-only feature for keeping an eye on interesting threads but around TB 24 they added it for mail as well.\n",
                "Putting a 'watch' eye is different from putting a 'star' on a message in that when you star a message, only that message is starred.  Any subsequent replies/messages in that thread don't automatically get starred so you'd have to investigate your starred messages in some kind of threaded view to see if something new came in that was related.  This is not always easy/convenient - not everyone uses threaded views daily for example (I don't) so this means another tab left open or remembering to explicitly check another view.  Watched will show in any view (so long as the column for that icon hasn't been hidden/removed.)\n"
            ]
        },
        {
            "id": "189-3-4",
            "pair": [
                "Putting a 'watch' eye is different from putting a 'star' on a message in that when you star a message, only that message is starred.  Any subsequent replies/messages in that thread don't automatically get starred so you'd have to investigate your starred messages in some kind of threaded view to see if something new came in that was related.  This is not always easy/convenient - not everyone uses threaded views daily for example (I don't) so this means another tab left open or remembering to explicitly check another view.  Watched will show in any view (so long as the column for that icon hasn't been hidden/removed.)\n",
                "Also, some people (myself included) want to use the star for other purposes.  I  use them for actionable items or those that I delegate to my assistant's attention which is only really possible because I don't need them for 'watch'.  Yes I could do it with tags (and back when I did) but the star is more convenient and noticeable and frees up tags for our other (more appropriate imo) purposes.\n"
            ]
        },
        {
            "id": "189-4-5",
            "pair": [
                "Also, some people (myself included) want to use the star for other purposes.  I  use them for actionable items or those that I delegate to my assistant's attention which is only really possible because I don't need them for 'watch'.  Yes I could do it with tags (and back when I did) but the star is more convenient and noticeable and frees up tags for our other (more appropriate imo) purposes.\n",
                "There may be other advantages too that I'm not yet aware of but so far for me it's been a great feature addition."
            ]
        }
    ],
    [
        {
            "id": "19-1-2",
            "pair": [
                "My understanding is that the Linear module is essentially linear regression. So if you say \u201csimple Linear\u201d I assume if there is any significant slope, then going negative is somewhat unavoidable if your testing data is beyond the range of the training data.  I say all that really to say, perhaps the Linear model is not the best choice. \n",
                "Regarding ReLU, it might not make much difference as assuming you are using sigmoid activation. Both return a value between 0 and 1.\n"
            ]
        },
        {
            "id": "19-2-3",
            "pair": [
                "Regarding ReLU, it might not make much difference as assuming you are using sigmoid activation. Both return a value between 0 and 1.\n",
                "The other thing you should consider is whether to normalize your input between 0 and 1.\n"
            ]
        },
        {
            "id": "19-3-4",
            "pair": [
                "The other thing you should consider is whether to normalize your input between 0 and 1.\n",
                "If you know that your output are positive, I think it makes more sense to enforce the positivity in your neural network by applying relu function or softplus $\\ln(1. + \\exp(x))$. You could also have a look at Generalized models which extend linear regresssion to cases where the variable to predict is only positive (Gamma regression) or between 0 and 1 (logistic regression). If you are predicting a categorical variable, you could also perform one hot encoding and transform your regression problem in a classification. Last but not least, as suggested in the last question it might be interesting to normalise your output between 0 and 1 and have a logistic regression in the last layer. Hope this help "
            ]
        }
    ],
    [
        {
            "id": "190-1-2",
            "pair": [
                "It find it extremely important to teach them the principles behind those (or similar) data structures first, and I'd go so far as to pick any one of the more complex ones as well (0/1-trees or even B-Trees etc.), just for fun (maybe skipping some details, if time is an issue). I'd also stress some \"outliers\" like ones that have O(1) in some of their operations; or at least one that is good not only for fast random access, but also for storage on slow media.\n",
                "You did not tell us which kind of students you have (high school? University? CS or \"Programming\"? I'll assume CS at a beginner level.). But they will be able to find out how to use the API of their language of choice just by reading the reference documentation (or, alas, these days, more likely, some online \"tutorial\", which leaves away all but the barest syntactical information). The reference documents will also tell them a little bit about when to use which, but not quite in a way that compares all of the (theoretically possible) structures side-to-side.\n"
            ]
        },
        {
            "id": "190-2-3",
            "pair": [
                "You did not tell us which kind of students you have (high school? University? CS or \"Programming\"? I'll assume CS at a beginner level.). But they will be able to find out how to use the API of their language of choice just by reading the reference documentation (or, alas, these days, more likely, some online \"tutorial\", which leaves away all but the barest syntactical information). The reference documents will also tell them a little bit about when to use which, but not quite in a way that compares all of the (theoretically possible) structures side-to-side.\n",
                "That's your job: show them what the world has to offer, and have them get a gut feeling for when to use which. There are several aspects: O-complexities of the operations (insert, delete, search, ...); space-usage aspects; aspects related to storage (i.e., is random access cheap or not?) and so on. \n"
            ]
        },
        {
            "id": "190-3-4",
            "pair": [
                "That's your job: show them what the world has to offer, and have them get a gut feeling for when to use which. There are several aspects: O-complexities of the operations (insert, delete, search, ...); space-usage aspects; aspects related to storage (i.e., is random access cheap or not?) and so on. \n",
                "I find it very important that you are able to teach them on a board or paper; i.e., draw boxes, draw lines and so on, so they really get an intuitive feeling about all of this. After that, the actual implementation of most structures and operations should be a snap; and using pre-made libraries even more so.\n"
            ]
        },
        {
            "id": "190-4-5",
            "pair": [
                "I find it very important that you are able to teach them on a board or paper; i.e., draw boxes, draw lines and so on, so they really get an intuitive feeling about all of this. After that, the actual implementation of most structures and operations should be a snap; and using pre-made libraries even more so.\n",
                "Whatever you teach them in this way will stay valid and useful until the end of their lives. It gives a good foundation and reinforces structural thinking, branching out to the algorithms later working on those data structures. It's the same reason why one would rather teach a handful of abstract sort algorithms instead of the Java SDK functions for sorting.\n"
            ]
        },
        {
            "id": "190-5-6",
            "pair": [
                "Whatever you teach them in this way will stay valid and useful until the end of their lives. It gives a good foundation and reinforces structural thinking, branching out to the algorithms later working on those data structures. It's the same reason why one would rather teach a handful of abstract sort algorithms instead of the Java SDK functions for sorting.\n",
                "For me, a solid knowledge about data structures is one of the few parts of my CS education (which is a few decades gone) that stays with me until today, and has almost everyday applications in both software development, analysis, design, architectural work etc.; and I have certainly, in the past, implemented complicated data structures without having to resolve to libraries (when there were none in the language at hand) with confidence, having a sound background about these topics.\n"
            ]
        },
        {
            "id": "190-6-7",
            "pair": [
                "For me, a solid knowledge about data structures is one of the few parts of my CS education (which is a few decades gone) that stays with me until today, and has almost everyday applications in both software development, analysis, design, architectural work etc.; and I have certainly, in the past, implemented complicated data structures without having to resolve to libraries (when there were none in the language at hand) with confidence, having a sound background about these topics.\n",
                "I see no disadvantage with teaching how to implement those structures, whatsoever.  I'd rather have someone teach only structures with not a single line of code; than a lot of code in a language or library which is likely obsolete in short time anyway, and no solid foundation for it. Have them find out how to use the API of their choice in their homework assignments; they should definitely be able to do that with any modern, well documented language (and if they are very young, give them a few handy pointers, like the names of the relevant packages/modules in the library or something like that).\n"
            ]
        },
        {
            "id": "190-7-8",
            "pair": [
                "I see no disadvantage with teaching how to implement those structures, whatsoever.  I'd rather have someone teach only structures with not a single line of code; than a lot of code in a language or library which is likely obsolete in short time anyway, and no solid foundation for it. Have them find out how to use the API of their choice in their homework assignments; they should definitely be able to do that with any modern, well documented language (and if they are very young, give them a few handy pointers, like the names of the relevant packages/modules in the library or something like that).\n",
                "Let me answer from the perspective of someone who recently completed a Data Structures and Algorithms course.\n"
            ]
        },
        {
            "id": "190-8-9",
            "pair": [
                "Let me answer from the perspective of someone who recently completed a Data Structures and Algorithms course.\n",
                "My Data Structure course was taught using java applets two years ago. My design pattern course last year was the first class many of my peers were exposed to Git (or any version control for that matter), and SOLID design principles were multiple choice questions to answer, rather than industry paradigms which we would be expected to use going forward; in fact in my current courses applying these design patterns would be suicide due to the strict time requirements. In my 400-level Operating Systems course, our first assignment was fibonacci in C, and I can promise you your students are going to go on and have incompetent professors who fail to prepare them for the realities of software development.\n"
            ]
        },
        {
            "id": "190-9-10",
            "pair": [
                "My Data Structure course was taught using java applets two years ago. My design pattern course last year was the first class many of my peers were exposed to Git (or any version control for that matter), and SOLID design principles were multiple choice questions to answer, rather than industry paradigms which we would be expected to use going forward; in fact in my current courses applying these design patterns would be suicide due to the strict time requirements. In my 400-level Operating Systems course, our first assignment was fibonacci in C, and I can promise you your students are going to go on and have incompetent professors who fail to prepare them for the realities of software development.\n",
                "It is imperative that you teach these students how to use industry standards and built-ins side by side with theory i.e. IMO, design and implementation go hand in hand. I would argue that AnoE's assertion that \"they will be able to find out how to use the API of their language of choice just by reading the reference documentation\" makes a very dangerous assumption: that it is not your job to teach these students to be good programmers, rather just good computer scientists. Many students who enter into CS programs do so not aware that Computer Science =/= Software Engineering (although some design patterns and modern day paradigms are included in the curriculum). I will however agree with AnoE vehemently that knowing how these technologies work is infinitely more beneficial to them than the syntax of how to use them, but not teaching them how to use these constructs which are so widely used, which may very well cost you your time in grading and critiquing, is instrumental in producing quality programmers as far as I am concerned. \n"
            ]
        },
        {
            "id": "190-10-11",
            "pair": [
                "It is imperative that you teach these students how to use industry standards and built-ins side by side with theory i.e. IMO, design and implementation go hand in hand. I would argue that AnoE's assertion that \"they will be able to find out how to use the API of their language of choice just by reading the reference documentation\" makes a very dangerous assumption: that it is not your job to teach these students to be good programmers, rather just good computer scientists. Many students who enter into CS programs do so not aware that Computer Science =/= Software Engineering (although some design patterns and modern day paradigms are included in the curriculum). I will however agree with AnoE vehemently that knowing how these technologies work is infinitely more beneficial to them than the syntax of how to use them, but not teaching them how to use these constructs which are so widely used, which may very well cost you your time in grading and critiquing, is instrumental in producing quality programmers as far as I am concerned. \n",
                "My personal advice? Have them write their own Linked Lists, Stacks and what-have-yous, then for part two of the assignment modify their existing codebase to use standard library built-ins, whilst adding some other functionality (threading, pipes, etc., what the assignment calls for). Alternatively, splitting these up into two different assignments works just as well, although may crunch for time."
            ]
        }
    ],
    [
        {
            "id": "191-1-2",
            "pair": [
                "I have a java app that accesses an NFS share with hard mount (soft mount does troubles i get many RPC timeout's for some reason).\n",
                "If i turn off (svcadm -v disable to the nfs server) then my java app gets stuck\n"
            ]
        },
        {
            "id": "191-2-3",
            "pair": [
                "If i turn off (svcadm -v disable to the nfs server) then my java app gets stuck\n",
                "returns nothing as my java process hangs - strange, why even kill -3 to get thread dump does not return?\n"
            ]
        },
        {
            "id": "191-3-4",
            "pair": [
                "returns nothing as my java process hangs - strange, why even kill -3 to get thread dump does not return?\n",
                "also i even had problems killing my process - its not something i want to do, i don't want my app to get stuck if the mount is down\n"
            ]
        },
        {
            "id": "191-4-5",
            "pair": [
                "also i even had problems killing my process - its not something i want to do, i don't want my app to get stuck if the mount is down\n",
                "Anyone has a solution or can recommend what I should be doing?\n"
            ]
        },
        {
            "id": "191-5-6",
            "pair": [
                "Anyone has a solution or can recommend what I should be doing?\n",
                "I would like to use maybe soft mounts the problem is that i get many RPC timeout while I try to copy files from one server to an nfs share.\n"
            ]
        },
        {
            "id": "191-6-7",
            "pair": [
                "I would like to use maybe soft mounts the problem is that i get many RPC timeout while I try to copy files from one server to an nfs share.\n",
                "so because i'm getting these errors with soft mounts and it looks unreliable (its over tcp) then i wished to test hard mount but this makes my app stuck if the share is down for some reason.\n"
            ]
        },
        {
            "id": "191-7-8",
            "pair": [
                "so because i'm getting these errors with soft mounts and it looks unreliable (its over tcp) then i wished to test hard mount but this makes my app stuck if the share is down for some reason.\n",
                "Blocking-and-retrying-indefinitely is the very definition of a hard-mount. Passing the intr option to the mount command ought to at least allow you to interrupt the blockage when the mount is down, if I'm reading the docs correctly."
            ]
        }
    ],
    [
        {
            "id": "192-1-2",
            "pair": [
                "This is an administration task, and the sql standard does not define such things; so a DBMS is not required to have this. But since it is obviously very useful to be able to make a backup, most systems do have similar capabilities, although they might be implemented or named in a different way.\n",
                "The corresponding function to the \"simple\" recovery model is a mysqldump or a pg_dump.\n"
            ]
        },
        {
            "id": "192-2-3",
            "pair": [
                "The corresponding function to the \"simple\" recovery model is a mysqldump or a pg_dump.\n",
                "An equivalent to the transaction log backups which will be created in the mssql-\"full\" recovery model are the mysql binary log and the WAL-log for postgres. All methods will allow you to continuously backup and (more or less comfortably) recover a database to a point after the last complete backup. Many technical details, e.g. on how to recover, if the file created by the \"simple\" backup can be used for it, the file formats, or how or if the files used internally (e.g. after a crash) differ.\n"
            ]
        },
        {
            "id": "192-3-4",
            "pair": [
                "An equivalent to the transaction log backups which will be created in the mssql-\"full\" recovery model are the mysql binary log and the WAL-log for postgres. All methods will allow you to continuously backup and (more or less comfortably) recover a database to a point after the last complete backup. Many technical details, e.g. on how to recover, if the file created by the \"simple\" backup can be used for it, the file formats, or how or if the files used internally (e.g. after a crash) differ.\n",
                "These methods also resemble each other in that they will be used (and are required) for replication purposes in all three database systems. For such a functionality, a different server needs to be continuously informed about all the changes that oocured, which is exactly what these logs contain.\n"
            ]
        },
        {
            "id": "192-4-5",
            "pair": [
                "These methods also resemble each other in that they will be used (and are required) for replication purposes in all three database systems. For such a functionality, a different server needs to be continuously informed about all the changes that oocured, which is exactly what these logs contain.\n",
                "For the MSSQL-\"Bulk logged\" recovery model, there is no direct equivalent; it is a special version of the \"full\" model that does not log some bulk operations.\n"
            ]
        },
        {
            "id": "192-5-6",
            "pair": [
                "For the MSSQL-\"Bulk logged\" recovery model, there is no direct equivalent; it is a special version of the \"full\" model that does not log some bulk operations.\n",
                "I've been reading about different recovery modes you can set on sql databases, for example here: https://www.mssqltips.com/sqlservertutorial/2/sql-server-recovery-models/\n"
            ]
        },
        {
            "id": "192-6-7",
            "pair": [
                "I've been reading about different recovery modes you can set on sql databases, for example here: https://www.mssqltips.com/sqlservertutorial/2/sql-server-recovery-models/\n",
                "So I've been trying to find the option to set this recovery mode or check what recovery mode I am using in mysql but I cannot find it anywhere. I've been googling around as well, and I also cannot find an answer anywhere. \n"
            ]
        },
        {
            "id": "192-7-8",
            "pair": [
                "So I've been trying to find the option to set this recovery mode or check what recovery mode I am using in mysql but I cannot find it anywhere. I've been googling around as well, and I also cannot find an answer anywhere. \n",
                "Is this related only to certain database platforms that use sql (Microsoft only?) that's not mysql or can I use this or a similar function in mysql and postgresql?"
            ]
        }
    ],
    [
        {
            "id": "193-1-2",
            "pair": [
                "By default the resource agent IPaddr2 only check if the ip address is configured, you can check this reading https://github.com/ClusterLabs/resource-agents/blob/master/heartbeat/IPaddr2\n",
                "If you want to monitor the connectivity using icmp protocol, you can read this link http://clusterlabs.org/doc/en-US/Pacemaker/1.1/html/Pacemaker_Explained/_moving_resources_due_to_connectivity_changes.html \n"
            ]
        },
        {
            "id": "193-2-3",
            "pair": [
                "If you want to monitor the connectivity using icmp protocol, you can read this link http://clusterlabs.org/doc/en-US/Pacemaker/1.1/html/Pacemaker_Explained/_moving_resources_due_to_connectivity_changes.html \n",
                "I am running Corosync and Pacemaker via a cman stack, in an active-active setup delivering web pages, and I've hit a brick wall. I'm using the IPaddr2 resource agent to have an IP that is used simoultaneously by both nodes, and after about 15  minutes, it stops working. It is still running according to PCS, the ClusterIP rule is still in iptables, but the IP becomes unreachable. If I restart iptables, the cluster ip works for another fifteen minutes, then stops again. Here is my CIB config:\n"
            ]
        },
        {
            "id": "193-3-4",
            "pair": [
                "I am running Corosync and Pacemaker via a cman stack, in an active-active setup delivering web pages, and I've hit a brick wall. I'm using the IPaddr2 resource agent to have an IP that is used simoultaneously by both nodes, and after about 15  minutes, it stops working. It is still running according to PCS, the ClusterIP rule is still in iptables, but the IP becomes unreachable. If I restart iptables, the cluster ip works for another fifteen minutes, then stops again. Here is my CIB config:\n",
                "I've been searching all over and I can't seem to figure it out. Any idea what is causing my issue or how to fix it?"
            ]
        }
    ],
    [
        {
            "id": "194-1-2",
            "pair": [
                "The other day I had what I thought was a great idea - I could buy up a bunch of cheap USB 2.0 drives and fill up the spaces in my 7-port USB hub for a super-fast RAID device!  But in the light of day it isn't looking so good.  I think that this would give me faster read times at least, but how would write times fare?  Which RAID level would be best suited for this purpose? (I am trying to optimize for speed, any data doesn't need to be particularly safe.)\n",
                "If this is a \"good idea\", or at least not completely foolhardy, how would I go about setting this up?  I run Ubuntu 12.10 and Windows 8.\n"
            ]
        },
        {
            "id": "194-2-3",
            "pair": [
                "If this is a \"good idea\", or at least not completely foolhardy, how would I go about setting this up?  I run Ubuntu 12.10 and Windows 8.\n",
                "Well, it looks like the limitations of a single USB 2.0 bus would cripple this idea.  For some speed gain, I could use two flash drives per bus and use multiple buses (there are two on my laptop) for a total of 70 MB/s (Wikipedia says maximum throughput per bus is 35 MB/s, and each flash drive gives me roughly 20MB/s).  However, this isn't that much - it looks like it was more feasible back in the days of floppy drives.\n"
            ]
        },
        {
            "id": "194-3-4",
            "pair": [
                "Well, it looks like the limitations of a single USB 2.0 bus would cripple this idea.  For some speed gain, I could use two flash drives per bus and use multiple buses (there are two on my laptop) for a total of 70 MB/s (Wikipedia says maximum throughput per bus is 35 MB/s, and each flash drive gives me roughly 20MB/s).  However, this isn't that much - it looks like it was more feasible back in the days of floppy drives.\n",
                "Building a raid out of a bunch of cheap usb thumb drives is a great idea. A 'must read' on this topic can be found here: http://analogbit.com/node/4\n"
            ]
        },
        {
            "id": "194-4-5",
            "pair": [
                "Building a raid out of a bunch of cheap usb thumb drives is a great idea. A 'must read' on this topic can be found here: http://analogbit.com/node/4\n",
                "To summarize, up to six USB flash drives are needed to saturate usb bandwidth in most of the use cases. Lots of USB flash drive with low write speed are actually doing wear leveling internally (ex: Sandisk Cruzer Blade 4GB stuck at 4MB/s for this purpose). With only one flash drive, write speed is about 4 to 10 MB/s. With six flash drives in RAID 0, the write speed is as high as an external hard disk: no tradeoff is made between data safety versus write speed performance with that kind of setup.\n"
            ]
        },
        {
            "id": "194-5-6",
            "pair": [
                "To summarize, up to six USB flash drives are needed to saturate usb bandwidth in most of the use cases. Lots of USB flash drive with low write speed are actually doing wear leveling internally (ex: Sandisk Cruzer Blade 4GB stuck at 4MB/s for this purpose). With only one flash drive, write speed is about 4 to 10 MB/s. With six flash drives in RAID 0, the write speed is as high as an external hard disk: no tradeoff is made between data safety versus write speed performance with that kind of setup.\n",
                "PS: I personally use a RAID0 made of seven Sandisk Cruzer Blade 32GB and a D-Link HUB H7 plugged on a OpenWrt router that runs 24/7. Total cost lower than a 256GB SSD and a 2.5\" external HDD enclosure."
            ]
        }
    ],
    [
        {
            "id": "195-1-2",
            "pair": [
                "Calling this effect \"the table\" is misleading, like talking about \"the BIOS\". Each videocard has a BIOS, but they're not the same. Each manufacturer has its own, and will likely evolve add functionality to it for new products. VESA just descibes some common parts, not everything.\n",
                "A bit late but I think I have found something that you or others finding this page could find useful.\n"
            ]
        },
        {
            "id": "195-2-3",
            "pair": [
                "A bit late but I think I have found something that you or others finding this page could find useful.\n",
                "In a driver for 64-bit for GMA 3150 I found a Vbios.zip which contains three files with the same name.\n"
            ]
        },
        {
            "id": "195-3-4",
            "pair": [
                "In a driver for 64-bit for GMA 3150 I found a Vbios.zip which contains three files with the same name.\n",
                "An executable that runs only in 32-bit windows (probably the Vbios flashing tool), a .dat file that if opened with a hex editor is clearly the Vbios as it states such in the first line, and a .bsf file that if opened with notepad explains what the numbers at each offset are.\n"
            ]
        },
        {
            "id": "195-4-5",
            "pair": [
                "An executable that runs only in 32-bit windows (probably the Vbios flashing tool), a .dat file that if opened with a hex editor is clearly the Vbios as it states such in the first line, and a .bsf file that if opened with notepad explains what the numbers at each offset are.\n",
                "This is way over my head as I don't understand low-level hardware coding, but someone might find this interesting. Especially as this info does not seem to be GMA-3150-specific, although it is a bit outdated (as the GMA's architecture for that matter).\n"
            ]
        },
        {
            "id": "195-5-6",
            "pair": [
                "This is way over my head as I don't understand low-level hardware coding, but someone might find this interesting. Especially as this info does not seem to be GMA-3150-specific, although it is a bit outdated (as the GMA's architecture for that matter).\n",
                "I'm hosting it on my dropbox, but spread the news and rehost this if you can.\n"
            ]
        },
        {
            "id": "195-6-7",
            "pair": [
                "I'm hosting it on my dropbox, but spread the news and rehost this if you can.\n",
                "I'm including the official support page where they give you driver and this package (and say they do)\n"
            ]
        },
        {
            "id": "195-7-8",
            "pair": [
                "I'm including the official support page where they give you driver and this package (and say they do)\n",
                "DISCLAIMER: THIS STUFF IS VERY DANGEROUS FOR YOUR DEVICE, YOU PLAY WITH IT AT YOUR OWN RISK.\n"
            ]
        },
        {
            "id": "195-8-9",
            "pair": [
                "DISCLAIMER: THIS STUFF IS VERY DANGEROUS FOR YOUR DEVICE, YOU PLAY WITH IT AT YOUR OWN RISK.\n",
                "Dropbox Download link: https://dl.dropboxusercontent.com/u/47541136/GMA_3150_vbios%2Btools.zip\n"
            ]
        },
        {
            "id": "195-9-10",
            "pair": [
                "Dropbox Download link: https://dl.dropboxusercontent.com/u/47541136/GMA_3150_vbios%2Btools.zip\n",
                "Official Download link: https://downloadcenter.intel.com/Detail_Desc.aspx?lang=eng&DwnldID=18478"
            ]
        }
    ],
    [
        {
            "id": "196-1-2",
            "pair": [
                "Normally the cases in which you use a Regression model is when you want to predict a continuous value from a set of given independent variables.\n",
                "E.g :  Let the following values be of the type [independent_variable, dependent_variable] or simply $[marks,height]$ and the values be $[2,0],[3,2],[4,5],[1,1]$. You fit a line or curve through these values ($[2,0],[3,2]$ etc.) and then see the case when a value of $[10,y]$ is given or marks of $10$ are obtained, what can be the $y$ (height) value from the fitted line or curve you had modeled.\n"
            ]
        },
        {
            "id": "196-2-3",
            "pair": [
                "E.g :  Let the following values be of the type [independent_variable, dependent_variable] or simply $[marks,height]$ and the values be $[2,0],[3,2],[4,5],[1,1]$. You fit a line or curve through these values ($[2,0],[3,2]$ etc.) and then see the case when a value of $[10,y]$ is given or marks of $10$ are obtained, what can be the $y$ (height) value from the fitted line or curve you had modeled.\n",
                "Take a look at Linear Regression for the above type.\n"
            ]
        },
        {
            "id": "196-3-4",
            "pair": [
                "Take a look at Linear Regression for the above type.\n",
                "Classification model is used in the case when you got a set of independent variables as in the previous case but the dependent value used in training is not continuous value but tells what class the value belong to.\n"
            ]
        },
        {
            "id": "196-4-5",
            "pair": [
                "Classification model is used in the case when you got a set of independent variables as in the previous case but the dependent value used in training is not continuous value but tells what class the value belong to.\n",
                "E.g : $([2,1],fail),([3,2],fail),([4,5],pass),([1,2],fail)$. Here [2,1] belongs to class $fail$ etc. So later time when a point say [7,8] is given, you will be finding which class (pass or fail) it might probably belong to.\n"
            ]
        },
        {
            "id": "196-5-6",
            "pair": [
                "E.g : $([2,1],fail),([3,2],fail),([4,5],pass),([1,2],fail)$. Here [2,1] belongs to class $fail$ etc. So later time when a point say [7,8] is given, you will be finding which class (pass or fail) it might probably belong to.\n",
                "For example SVM's for this case create a hyperplane(a multidimensional plane) and based on where the points falls in the space, it is going to find the class with some probability.\n"
            ]
        },
        {
            "id": "196-6-7",
            "pair": [
                "For example SVM's for this case create a hyperplane(a multidimensional plane) and based on where the points falls in the space, it is going to find the class with some probability.\n",
                "Simply, choose Regression if the dependent value is continuous else choose Classification if the dependent value is a class.\n"
            ]
        },
        {
            "id": "196-7-8",
            "pair": [
                "Simply, choose Regression if the dependent value is continuous else choose Classification if the dependent value is a class.\n",
                "Learning from a set of examples x mapping to y can be conceptualised as finding function f such that:\n"
            ]
        },
        {
            "id": "196-8-9",
            "pair": [
                "Learning from a set of examples x mapping to y can be conceptualised as finding function f such that:\n",
                "if y is continuous, the problem is a regression problem\n"
            ]
        },
        {
            "id": "196-9-10",
            "pair": [
                "if y is continuous, the problem is a regression problem\n",
                "else if y is discrete, the problem is a classification problem\n"
            ]
        },
        {
            "id": "196-10-11",
            "pair": [
                "else if y is discrete, the problem is a classification problem\n",
                "Continuous implies y can take any value [i, j] on Real scale and discrete implies y can take a value from set of {a, b, ..., d}"
            ]
        }
    ],
    [
        {
            "id": "197-1-2",
            "pair": [
                "I have two desktop computers, both Windows 7, and they are connected via a gigabit network connection. I share a folder via Windows file sharing (samba). When I browse a folder on a remote computer, it's still quite slow: if I open a folder with ~100 photos of 3 MiB, I can't view them as easily as on a local disk.\n",
                "How can this be improved? Should I use 10 gigabit networking?\n"
            ]
        },
        {
            "id": "197-2-3",
            "pair": [
                "How can this be improved? Should I use 10 gigabit networking?\n",
                "Network speed vs local speed is so different, speeding up the network to extremes is going to be a very heavy costing thing and is definitely not worth the cost and effort.\n"
            ]
        },
        {
            "id": "197-3-4",
            "pair": [
                "Network speed vs local speed is so different, speeding up the network to extremes is going to be a very heavy costing thing and is definitely not worth the cost and effort.\n",
                "You will want to look into how big the data is on the network share. IF its not too big (You have enough free space on your C drive with extra space to spare) consider setting the network share available offline. This will in essence copy all the files to your C:\\Windows\\CSC folder and instead of reading from the network, it reads from your local drive. It will sync new files in the background.\n"
            ]
        },
        {
            "id": "197-4-5",
            "pair": [
                "You will want to look into how big the data is on the network share. IF its not too big (You have enough free space on your C drive with extra space to spare) consider setting the network share available offline. This will in essence copy all the files to your C:\\Windows\\CSC folder and instead of reading from the network, it reads from your local drive. It will sync new files in the background.\n",
                "A similar approach can be done using dropbox and similar clients, with the advantage of not being forced to use your C drive. The downside is that these online storage services have limited capacity.\n"
            ]
        },
        {
            "id": "197-5-6",
            "pair": [
                "A similar approach can be done using dropbox and similar clients, with the advantage of not being forced to use your C drive. The downside is that these online storage services have limited capacity.\n",
                "Lastly, there are programs such as Free Sync which you can setup to automatically sync two folders so they remain the same. You will have to keep this program running in the background and then sync the entire folder to a local folder of your choice. You can then open the files locally. Because changes are not detected locally until the next sync is performed, its less reliable to use if you also need to edit the files and other people work in them at the same time.\n"
            ]
        },
        {
            "id": "197-6-7",
            "pair": [
                "Lastly, there are programs such as Free Sync which you can setup to automatically sync two folders so they remain the same. You will have to keep this program running in the background and then sync the entire folder to a local folder of your choice. You can then open the files locally. Because changes are not detected locally until the next sync is performed, its less reliable to use if you also need to edit the files and other people work in them at the same time.\n",
                "A last alternative is to go about it the other way.\n"
            ]
        },
        {
            "id": "197-7-8",
            "pair": [
                "A last alternative is to go about it the other way.\n",
                "Lets assume for a second that the files are stored on a server. If you use a remote desktop connection to open the server, you can browse the files locally on the server with much greater speeds than downloading every single file all the time. All it needs to transmit is the visual output of what you see. For quickly browsing it doesn't matter if the compression performed on the video stream makes the video quality less good (its marginal though), depending on your settings."
            ]
        }
    ],
    [
        {
            "id": "198-1-2",
            "pair": [
                "The connectors themself arn't actually specific to a protocol - It simply refers to the number of pins and that its sub miniature. You could in theory wire up any suitable connector to the right electrical connections and to use it.\n",
                "In most systems I've seen, serial ports had 9 pins (since they didn't implement the whole 25 pin standard). I could also have, in theory the same 9 pins on a mini DIN connector or used the same 9 or 25 pins for another protocol.\n"
            ]
        },
        {
            "id": "198-2-3",
            "pair": [
                "In most systems I've seen, serial ports had 9 pins (since they didn't implement the whole 25 pin standard). I could also have, in theory the same 9 pins on a mini DIN connector or used the same 9 or 25 pins for another protocol.\n",
                "On the other hand, printers often had a centronics port rather than a DB25, and once again, the physical connector simply had to support the minimum amount of pins the electrical standard needed.\n"
            ]
        },
        {
            "id": "198-3-4",
            "pair": [
                "On the other hand, printers often had a centronics port rather than a DB25, and once again, the physical connector simply had to support the minimum amount of pins the electrical standard needed.\n",
                "At the end of the day the smart thing is to check before you plug anything in ;p\n"
            ]
        },
        {
            "id": "198-4-5",
            "pair": [
                "At the end of the day the smart thing is to check before you plug anything in ;p\n",
                "I would like to know whether the DB-25 port is a serial or parallel port. These ports are otherwise known as printer ports, aka LPT port. \n"
            ]
        },
        {
            "id": "198-5-6",
            "pair": [
                "I would like to know whether the DB-25 port is a serial or parallel port. These ports are otherwise known as printer ports, aka LPT port. \n",
                "I'm very confused about this since, according to Wikipedia, this port is considered part of the D-Subminiature connectors, which are mostly used for RS-232 serial communication. \n"
            ]
        },
        {
            "id": "198-6-7",
            "pair": [
                "I'm very confused about this since, according to Wikipedia, this port is considered part of the D-Subminiature connectors, which are mostly used for RS-232 serial communication. \n",
                "On the other hand, we know that this port is known as the parallel printer port. So, why all this confusion? Is this confusion only associated with DB-25 ports on computers? Then how about other devices that have DB-25 connectors (mostly male DB-25)? "
            ]
        }
    ],
    [
        {
            "id": "199-1-2",
            "pair": [
                "Try This. this changes the default sound device for Windows and these type of softwares often take settings like this from here. \n",
                "Start Menu -> Control Panel -> Sounds and Audio Devices -> Audio tab -> Change Default Device for Sound Playback.\n"
            ]
        },
        {
            "id": "199-2-3",
            "pair": [
                "Start Menu -> Control Panel -> Sounds and Audio Devices -> Audio tab -> Change Default Device for Sound Playback.\n",
                "I have dual monitors on an XP SP2 system: one connected to the VGA port, the other on the HDMI port. Normally analogue sound is chosen but I can fiddle with the device drivers to get sound on the HDMI channel. But when the system is rebooted, it reverts to analogue. I would like HDMI to be the default. How do you do this?\n"
            ]
        },
        {
            "id": "199-3-4",
            "pair": [
                "I have dual monitors on an XP SP2 system: one connected to the VGA port, the other on the HDMI port. Normally analogue sound is chosen but I can fiddle with the device drivers to get sound on the HDMI channel. But when the system is rebooted, it reverts to analogue. I would like HDMI to be the default. How do you do this?\n",
                "The two drivers involved are (1) AMD High Definition Audio Device which appears to control HDMI and (2) Realtek High Definition Audio controlling the analogue.\n"
            ]
        },
        {
            "id": "199-4-5",
            "pair": [
                "The two drivers involved are (1) AMD High Definition Audio Device which appears to control HDMI and (2) Realtek High Definition Audio controlling the analogue.\n",
                "The controlling software is the Catalyst Control Center\n"
            ]
        },
        {
            "id": "199-5-6",
            "pair": [
                "The controlling software is the Catalyst Control Center\n",
                "The solution to the problem was surprising and had little to do with the computer or its operating system.\n"
            ]
        },
        {
            "id": "199-6-7",
            "pair": [
                "The solution to the problem was surprising and had little to do with the computer or its operating system.\n",
                "The culprit was the HDMI monitor switching itself off after a certain amount of time and although the vision came on when it was switched on again, the sound connection was not re-established. The solution was therefore to set it up so that it no longer switched itself off.\n"
            ]
        },
        {
            "id": "199-7-8",
            "pair": [
                "The culprit was the HDMI monitor switching itself off after a certain amount of time and although the vision came on when it was switched on again, the sound connection was not re-established. The solution was therefore to set it up so that it no longer switched itself off.\n",
                "This monitor has both VGA and HDMI inputs and although the default can be set to HDMI as far as vision is concerned, the default for sound remains analogue and cannot be set to digital. So, if the HDMI monitor is manually switched off and on again, the sound returns analogue on the VGA monitor.  However, when both monitors are switched on and the computer is booted or re-booted, sound appears on the HDMI channel.\n"
            ]
        },
        {
            "id": "199-8-9",
            "pair": [
                "This monitor has both VGA and HDMI inputs and although the default can be set to HDMI as far as vision is concerned, the default for sound remains analogue and cannot be set to digital. So, if the HDMI monitor is manually switched off and on again, the sound returns analogue on the VGA monitor.  However, when both monitors are switched on and the computer is booted or re-booted, sound appears on the HDMI channel.\n",
                "Monitor#1 is on the VGA  channel and is used as the primary monitor\n"
            ]
        },
        {
            "id": "199-9-10",
            "pair": [
                "Monitor#1 is on the VGA  channel and is used as the primary monitor\n",
                "Monitor#2 is on the HDMI channel and extends the Windows desktop"
            ]
        }
    ],
    [
        {
            "id": "2-1-2",
            "pair": [
                "I figured i'd share my method which worked to hide VM from Windows/Task MGR (It's also got less needless stuff and is easier to read). I was not trying to activate Solidworks but IMO this thread is more about hiding your running in a VM and maybe should be edited as such. Anyway...\n",
                "Add the following to your domain.xml and make sure to completley remove the CPU Section\n"
            ]
        },
        {
            "id": "2-2-3",
            "pair": [
                "Add the following to your domain.xml and make sure to completley remove the CPU Section\n",
                "The official way of activating Solidworks server on KVM is to apply for a \"Activation Exempt licensing for SolidWorks\".  We do this through the reseller, but possibly could be done directly through Solidworks support.\n"
            ]
        },
        {
            "id": "2-3-4",
            "pair": [
                "The official way of activating Solidworks server on KVM is to apply for a \"Activation Exempt licensing for SolidWorks\".  We do this through the reseller, but possibly could be done directly through Solidworks support.\n",
                "Solidworks sends a license file that Solidworks License Manager loads through the Advanced Options of the Server Administration tab.\n"
            ]
        },
        {
            "id": "2-4-5",
            "pair": [
                "Solidworks sends a license file that Solidworks License Manager loads through the Advanced Options of the Server Administration tab.\n",
                "Given that the last two years it has taken more than 2 weeks for Solidworks to send the license file, it would be nice to have them not blacklist KVM.\n"
            ]
        },
        {
            "id": "2-5-6",
            "pair": [
                "Given that the last two years it has taken more than 2 weeks for Solidworks to send the license file, it would be nice to have them not blacklist KVM.\n",
                "Solidworks has an \"Enhancement Request\" open with the title: \"Provide SNL Server support for VM KVM Qemu Libvirt.\" ER # 1-9482749288  I found it by logging in to Solidworks portal, finding the Enhancement Request link, and typing KVM.\n"
            ]
        },
        {
            "id": "2-6-7",
            "pair": [
                "Solidworks has an \"Enhancement Request\" open with the title: \"Provide SNL Server support for VM KVM Qemu Libvirt.\" ER # 1-9482749288  I found it by logging in to Solidworks portal, finding the Enhancement Request link, and typing KVM.\n",
                "Please leave a comment on this Enhancement Request!  Hopefully they'll decide KVM is a legitimate virtualization environment. "
            ]
        }
    ],
    [
        {
            "id": "20-1-2",
            "pair": [
                "Note the i modifier for regex, that makes searching case insensitive.\n",
                "You are vulnerable to persistent XSS attacks via the logs, which may or may not currently be a problem, depending on whether or not the log input is user-supplied. I definitely wouldn't trust that they never contain user input, so you should definitely defend against this. \n"
            ]
        },
        {
            "id": "20-2-3",
            "pair": [
                "You are vulnerable to persistent XSS attacks via the logs, which may or may not currently be a problem, depending on whether or not the log input is user-supplied. I definitely wouldn't trust that they never contain user input, so you should definitely defend against this. \n",
                "You are also vulnerable to reflected XSS via service and host.\n"
            ]
        },
        {
            "id": "20-3-4",
            "pair": [
                "You are also vulnerable to reflected XSS via service and host.\n",
                "An attacker could for example exploit the command injection issue via XSS, even if you would have CSRF protection.\n"
            ]
        },
        {
            "id": "20-4-5",
            "pair": [
                "An attacker could for example exploit the command injection issue via XSS, even if you would have CSRF protection.\n",
                "You are vulnerable to command injection via service as well as host, meaning anyone using your script can execute arbitrary commands.\n"
            ]
        },
        {
            "id": "20-5-6",
            "pair": [
                "You are vulnerable to command injection via service as well as host, meaning anyone using your script can execute arbitrary commands.\n",
                "Even if only people who should be allowed to execute arbitrary commands ever have access to this script - a big if - it should be secure against this, as it may be exploited in combination with other issues, eg XSS or CSRF.\n"
            ]
        },
        {
            "id": "20-6-7",
            "pair": [
                "Even if only people who should be allowed to execute arbitrary commands ever have access to this script - a big if - it should be secure against this, as it may be exploited in combination with other issues, eg XSS or CSRF.\n",
                "Your code doesn't have CSRF protection. If you don't have that in some other file that you didn't post, you should add it. Without CSRF protection, an attacker can get you to submit a request they define if you visit an attacker-controlled website while authenticated at your site.\n"
            ]
        },
        {
            "id": "20-7-8",
            "pair": [
                "Your code doesn't have CSRF protection. If you don't have that in some other file that you didn't post, you should add it. Without CSRF protection, an attacker can get you to submit a request they define if you visit an attacker-controlled website while authenticated at your site.\n",
                "GET and POST shouldn't be treated interchangeably. Accepting POST data via GET makes exploitation of some issues such as CSRF easier.\n"
            ]
        },
        {
            "id": "20-8-9",
            "pair": [
                "GET and POST shouldn't be treated interchangeably. Accepting POST data via GET makes exploitation of some issues such as CSRF easier.\n",
                "If you extract the code to a function, you could get rid of quite a couple of lines, and avoid possible bugs that may happen because of copy-pasting.\n"
            ]
        },
        {
            "id": "20-9-10",
            "pair": [
                "If you extract the code to a function, you could get rid of quite a couple of lines, and avoid possible bugs that may happen because of copy-pasting.\n",
                "(But as I said above, you shouldn't actually do this at all.)\n"
            ]
        },
        {
            "id": "20-10-11",
            "pair": [
                "(But as I said above, you shouldn't actually do this at all.)\n",
                "I would get rid of this variable. Just check at the beginning if either value you need is set, and if not, return.\n"
            ]
        },
        {
            "id": "20-11-12",
            "pair": [
                "I would get rid of this variable. Just check at the beginning if either value you need is set, and if not, return.\n",
                "Don't abbreviate variable names, it makes code harder to read. Without proper context, I have no idea what comm or curr are. \n"
            ]
        },
        {
            "id": "20-12-13",
            "pair": [
                "Don't abbreviate variable names, it makes code harder to read. Without proper context, I have no idea what comm or curr are. \n",
                "Your HTML isn't valid, you can check this yourself, eg here.\n"
            ]
        },
        {
            "id": "20-13-14",
            "pair": [
                "Your HTML isn't valid, you can check this yourself, eg here.\n",
                "You are also using elements such as &nbsp to style your document, which you shouldn't. The same is true of using <br> for styling (it should only be used if it has a semantic meaning). "
            ]
        }
    ],
    [
        {
            "id": "200-1-2",
            "pair": [
                "Yes, this approach is feasible; it just requires the right setup.  I'm going to assume you'll be setting up replication via the wizard because any other way is basically like trying to navigate your way out of a series of caverns with a terrible flashlight.  This answer is a bit wordy and light on instructions because it combines links to a few walkthroughs that are just better documented.  I'm only going to draw attention to the key items you need to adjust in order to achieve your desired setup.\n",
                "First, you must define publications at each individual server.  When specifying your articles in each publication, you need to be on the lookout for the Destination Object Owner setting.  You can specify a new schema for your articles to get pushed to.  I assume you'll want this to be different for each publication.\n"
            ]
        },
        {
            "id": "200-2-3",
            "pair": [
                "First, you must define publications at each individual server.  When specifying your articles in each publication, you need to be on the lookout for the Destination Object Owner setting.  You can specify a new schema for your articles to get pushed to.  I assume you'll want this to be different for each publication.\n",
                "The above screenshot was taken from a SQL Server Central Walkthrough on how to set this up within the publication, which can be found here.\n"
            ]
        },
        {
            "id": "200-3-4",
            "pair": [
                "The above screenshot was taken from a SQL Server Central Walkthrough on how to set this up within the publication, which can be found here.\n",
                "After you have completed configuring your publications and distributor, you now setup what I've heard referred to as a central subscriber.  To do this, you basically setup subscriptions to point to the same subscriber database.  This SQL Server Central Article walks through the process, but because you've setup your publications to remap the tables to different schemas (unlike the article), I believe you can initialize each publication via snapshot, which should hopefully save you some time on initial setup and anytime down the line you need to reset the subscription.  It's only when you are combining tables within the subscriber that one publisher snapshot will overwrite the data from a different publisher's snapshot.  I don't believe this will occur in your setup, but test to be certain.\n"
            ]
        },
        {
            "id": "200-4-5",
            "pair": [
                "After you have completed configuring your publications and distributor, you now setup what I've heard referred to as a central subscriber.  To do this, you basically setup subscriptions to point to the same subscriber database.  This SQL Server Central Article walks through the process, but because you've setup your publications to remap the tables to different schemas (unlike the article), I believe you can initialize each publication via snapshot, which should hopefully save you some time on initial setup and anytime down the line you need to reset the subscription.  It's only when you are combining tables within the subscriber that one publisher snapshot will overwrite the data from a different publisher's snapshot.  I don't believe this will occur in your setup, but test to be certain.\n",
                "wondering if it's possible to replicat into a different schema using Transactional Replication? \n"
            ]
        },
        {
            "id": "200-5-6",
            "pair": [
                "wondering if it's possible to replicat into a different schema using Transactional Replication? \n",
                "My situation is that I have 3 DB servers, each hosting it's own instance of our software, in three different locations. I would like to grab the data from each database and replicate them to a central server for reporting in Azure. I'm seeing if it's possible to combine them into a single database as opposed to having to pay for 3.\n"
            ]
        },
        {
            "id": "200-6-7",
            "pair": [
                "My situation is that I have 3 DB servers, each hosting it's own instance of our software, in three different locations. I would like to grab the data from each database and replicate them to a central server for reporting in Azure. I'm seeing if it's possible to combine them into a single database as opposed to having to pay for 3.\n",
                "I'm not worried if it's not possible, more of a curiosity thing."
            ]
        }
    ],
    [
        {
            "id": "201-1-2",
            "pair": [
                "With 2 x 6870s in crossfire, 4x2 DDR3 DIMMS, i7 and a possible water cooling you should be looking at a 850W PSU. And if you can stretch it - a 1000W would be ideal.  A good PSU can help your system and can cope up with any possible future upgrades.\n",
                "I am building a new computer and would like to double check on the power supply. I am using Building a PC, Part VI: Rebuilding as a guideline, updating parts to newer ones available, but am staying within the lines so to speak (aside from the GPUs).\n"
            ]
        },
        {
            "id": "201-2-3",
            "pair": [
                "I am building a new computer and would like to double check on the power supply. I am using Building a PC, Part VI: Rebuilding as a guideline, updating parts to newer ones available, but am staying within the lines so to speak (aside from the GPUs).\n",
                "Overall this will be a gaming rig with 2 x 6870s in crossfire, 4x2 DDR3 DIMMS, i7, etc.  (I am still looking into water cooling units and don't know how that will impact psu requirements.)\n"
            ]
        },
        {
            "id": "201-3-4",
            "pair": [
                "Overall this will be a gaming rig with 2 x 6870s in crossfire, 4x2 DDR3 DIMMS, i7, etc.  (I am still looking into water cooling units and don't know how that will impact psu requirements.)\n",
                "I have churned the expected setup though the psu calcs on newegg, outervision, and used ibuypower as a reference point.  These break down as:\n"
            ]
        },
        {
            "id": "201-4-5",
            "pair": [
                "I have churned the expected setup though the psu calcs on newegg, outervision, and used ibuypower as a reference point.  These break down as:\n",
                "I understand these are approximations/best guesses.  Based on the Jeff's article above and the reviews on newegg I believe I am going to go with the CORSAIR HX Series (approximate current prices):\n"
            ]
        },
        {
            "id": "201-5-6",
            "pair": [
                "I understand these are approximations/best guesses.  Based on the Jeff's article above and the reviews on newegg I believe I am going to go with the CORSAIR HX Series (approximate current prices):\n",
                "It's probably silly to even be concerned about going up to the 850W at this point based on what I want to do with the box.  Should I even be looking at the 1000W or is that just totally unnecessary?  (not that that's a bad thing)\n"
            ]
        },
        {
            "id": "201-6-7",
            "pair": [
                "It's probably silly to even be concerned about going up to the 850W at this point based on what I want to do with the box.  Should I even be looking at the 1000W or is that just totally unnecessary?  (not that that's a bad thing)\n",
                "I am really enjoying this process and appreciate input, thanks."
            ]
        }
    ],
    [
        {
            "id": "202-1-2",
            "pair": [
                "In sql 2016, you still can create user who can create a databas and become db_owner on the databas he creates but not be an sysadmin.\n",
                "That user can login with [user1] and create database and do all CRUD operations in that database (only)\n"
            ]
        },
        {
            "id": "202-2-3",
            "pair": [
                "That user can login with [user1] and create database and do all CRUD operations in that database (only)\n",
                "I have run into something i can't find any answer to and maybe you can help me.\n"
            ]
        },
        {
            "id": "202-3-4",
            "pair": [
                "I have run into something i can't find any answer to and maybe you can help me.\n",
                "In SQL 2012 and 2014, if a user have the server role dbcreator and public and create a databas that user got db_owner db role on the created database and could do what ever he needed to with that database.\n"
            ]
        },
        {
            "id": "202-4-5",
            "pair": [
                "In SQL 2012 and 2014, if a user have the server role dbcreator and public and create a databas that user got db_owner db role on the created database and could do what ever he needed to with that database.\n",
                "In SQL 2016 this does not happen and the database you create gets inaccessable because the user is added to the database only as a user and not as a db_owner and sa is set as dbo.\n"
            ]
        },
        {
            "id": "202-5-6",
            "pair": [
                "In SQL 2016 this does not happen and the database you create gets inaccessable because the user is added to the database only as a user and not as a db_owner and sa is set as dbo.\n",
                "Does anyone know of any changes in the roles between 2012/2014 and 2016 regarding this and how do i solve it, what server role does, the lowest possible, a user need to have go become db_owner on the database it creates?\n"
            ]
        },
        {
            "id": "202-6-7",
            "pair": [
                "Does anyone know of any changes in the roles between 2012/2014 and 2016 regarding this and how do i solve it, what server role does, the lowest possible, a user need to have go become db_owner on the database it creates?\n",
                "I need a user, that can create a databas and become db_owner on the databas he creates but not be an sysadmin.\n"
            ]
        },
        {
            "id": "202-7-8",
            "pair": [
                "I need a user, that can create a databas and become db_owner on the databas he creates but not be an sysadmin.\n",
                "Well the problem is not that u can't create the user with that server role, the problem is that it does not work like in 2014.\n"
            ]
        },
        {
            "id": "202-8-9",
            "pair": [
                "Well the problem is not that u can't create the user with that server role, the problem is that it does not work like in 2014.\n",
                "What db role does that user you created the database with got in that database?\n"
            ]
        },
        {
            "id": "202-9-10",
            "pair": [
                "What db role does that user you created the database with got in that database?\n",
                "I have tested this on 2 versions, Standard and Express, both did work the same.\n"
            ]
        },
        {
            "id": "202-10-11",
            "pair": [
                "I have tested this on 2 versions, Standard and Express, both did work the same.\n",
                "So have the dbcreate server role changed in how it works in SQL 2016 or is it just a setting or installation feature that i have missed?\n"
            ]
        },
        {
            "id": "202-11-12",
            "pair": [
                "So have the dbcreate server role changed in how it works in SQL 2016 or is it just a setting or installation feature that i have missed?\n",
                "I got a TSQL script for creating a User with dbcreate server role, then using that  user to create the database and the user got the db_owner db role.\n"
            ]
        },
        {
            "id": "202-12-13",
            "pair": [
                "I got a TSQL script for creating a User with dbcreate server role, then using that  user to create the database and the user got the db_owner db role.\n",
                "After that i ran that script i had no problem creating users and set them as Dbcreator and create database with the user and it then got to be a db_owner on the database.\n"
            ]
        },
        {
            "id": "202-13-14",
            "pair": [
                "After that i ran that script i had no problem creating users and set them as Dbcreator and create database with the user and it then got to be a db_owner on the database.\n",
                "I'll be testing on customer server now and see if i get the same result there."
            ]
        }
    ],
    [
        {
            "id": "203-1-2",
            "pair": [
                "It's only when there's an error, though, and it was a 404; this suggests that the request generating the error wasn't part of the namespace served by your app pool. Cos a 404 is Not Found, and that implies that the URL wasn't correct; psychic debugging tells us that this didn't fall within your app's /path/ .\n",
                "If the Default Web Site runs as DefaultAppPool at the root, any misrouted requests that land outside your specific app path (which runs in Your40AppPool) will be serviced by the DefaultAppPool.\n"
            ]
        },
        {
            "id": "203-2-3",
            "pair": [
                "If the Default Web Site runs as DefaultAppPool at the root, any misrouted requests that land outside your specific app path (which runs in Your40AppPool) will be serviced by the DefaultAppPool.\n",
                "will show you the URL namespaces defined for each App Pool (just stare at it a while until you get the hang of it). If any request targets anything outside the /path/ mapped to Your40AppPool, it'll be served by whatever's registered at the site (or box) level, which most times is DefAppPool.\n"
            ]
        },
        {
            "id": "203-3-4",
            "pair": [
                "will show you the URL namespaces defined for each App Pool (just stare at it a while until you get the hang of it). If any request targets anything outside the /path/ mapped to Your40AppPool, it'll be served by whatever's registered at the site (or box) level, which most times is DefAppPool.\n",
                "I have an aspx.net website running on the .net framework version 4.0, the website uses forms authentication, except for a small portion of the system, where the web services are kept, which has been configured to be accessible by all (using the location setting in the web.config file).\n"
            ]
        },
        {
            "id": "203-4-5",
            "pair": [
                "I have an aspx.net website running on the .net framework version 4.0, the website uses forms authentication, except for a small portion of the system, where the web services are kept, which has been configured to be accessible by all (using the location setting in the web.config file).\n",
                "the application is setup on a windows 2008 server, IIS7.5, with it's own memory pool, configured for .NET 4.0.  There is also a DefaultAppPool also configured, which I originally left configured for .NET 2.0.\n"
            ]
        },
        {
            "id": "203-5-6",
            "pair": [
                "the application is setup on a windows 2008 server, IIS7.5, with it's own memory pool, configured for .NET 4.0.  There is also a DefaultAppPool also configured, which I originally left configured for .NET 2.0.\n",
                "I was investigating a problem with running one of my web services, in which an error 404 was returned whenever a function was called, I resolved this problem, however I noticed whenever I received the error, the .NET version returning the error was version 2.0, rather than version 4.0, that of the application pool it should be running in.  I confirmed this behaviour by changing the settings of the DefaultAppPool to 4.0 as well, and the error was then being generated by the appropriate version.\n"
            ]
        },
        {
            "id": "203-6-7",
            "pair": [
                "I was investigating a problem with running one of my web services, in which an error 404 was returned whenever a function was called, I resolved this problem, however I noticed whenever I received the error, the .NET version returning the error was version 2.0, rather than version 4.0, that of the application pool it should be running in.  I confirmed this behaviour by changing the settings of the DefaultAppPool to 4.0 as well, and the error was then being generated by the appropriate version.\n",
                "IIS appears to have made a decision about which memory pool it should use, bypassing what I had configured for the aspx.net website.\n"
            ]
        },
        {
            "id": "203-7-8",
            "pair": [
                "IIS appears to have made a decision about which memory pool it should use, bypassing what I had configured for the aspx.net website.\n",
                "has anybody else experienced a similar problem, am I going mad, is there a configuration option hidden somewhere that could justify this behaviour? (i.e. somewhere in machine.config)"
            ]
        }
    ],
    [
        {
            "id": "204-1-2",
            "pair": [
                "In road network it is better to have cost in edges, not in nodes.\n",
                "When I was doing road networks (for navigation, not for games) related programming, I used Node that contained set of incoming and outgoing edges (and other data) and edge with both ends, reference to edge in oposite direction and again, other data.\n"
            ]
        },
        {
            "id": "204-2-3",
            "pair": [
                "When I was doing road networks (for navigation, not for games) related programming, I used Node that contained set of incoming and outgoing edges (and other data) and edge with both ends, reference to edge in oposite direction and again, other data.\n",
                "Some of this you will probubly not need (depends on what you want to do with this graph.\n"
            ]
        },
        {
            "id": "204-3-4",
            "pair": [
                "Some of this you will probubly not need (depends on what you want to do with this graph.\n",
                "For A* I would guess that you need: cost of edge, cost to reach node from start, outgoing edges and used incoming edges (for creating actual path once you find cost of path to goal).\n"
            ]
        },
        {
            "id": "204-4-5",
            "pair": [
                "For A* I would guess that you need: cost of edge, cost to reach node from start, outgoing edges and used incoming edges (for creating actual path once you find cost of path to goal).\n",
                "Another complicating thing (that can significantly reduce size of graph):\n"
            ]
        },
        {
            "id": "204-5-6",
            "pair": [
                "Another complicating thing (that can significantly reduce size of graph):\n",
                "Nodes with 2 neighbours are not interesting, unless they are start or goal, theefore you can create simplified graph where they are romeved and replaced with edges connecting their neighbours (if they were originaly connected by removed node)\n"
            ]
        },
        {
            "id": "204-6-7",
            "pair": [
                "Nodes with 2 neighbours are not interesting, unless they are start or goal, theefore you can create simplified graph where they are romeved and replaced with edges connecting their neighbours (if they were originaly connected by removed node)\n",
                "Nodes with just one neighbours are not interesting too, so they again can be removed.\n"
            ]
        },
        {
            "id": "204-7-8",
            "pair": [
                "Nodes with just one neighbours are not interesting too, so they again can be removed.\n",
                "Applying both these rules can in some cases reduce graph size multiple times (on some islands it reduced our graph to single node which was not so usefull ;) ), but in grid like road network it will help only little.\n"
            ]
        },
        {
            "id": "204-8-9",
            "pair": [
                "Applying both these rules can in some cases reduce graph size multiple times (on some islands it reduced our graph to single node which was not so usefull ;) ), but in grid like road network it will help only little.\n",
                "How excatly I represented this I don't remember (proprietary work I did few years ago).\n"
            ]
        },
        {
            "id": "204-9-10",
            "pair": [
                "How excatly I represented this I don't remember (proprietary work I did few years ago).\n",
                "And my experience was not with games, but with preprocessing graphs, so I had lot of memeory available (eg. Vieana needed computer with more than 4GB ram)\n"
            ]
        },
        {
            "id": "204-10-11",
            "pair": [
                "And my experience was not with games, but with preprocessing graphs, so I had lot of memeory available (eg. Vieana needed computer with more than 4GB ram)\n",
                "I have been reading A* path finding to apply on a road like network structure. Most of the code I saw is about A* on tile based maps. There is a short description at Amit's page here about what I am looking for. My question is about what data-structure should I use to define a road network map. \n"
            ]
        },
        {
            "id": "204-11-12",
            "pair": [
                "I have been reading A* path finding to apply on a road like network structure. Most of the code I saw is about A* on tile based maps. There is a short description at Amit's page here about what I am looking for. My question is about what data-structure should I use to define a road network map. \n",
                "I have infromation like, node, cost,etc. But not sure how to represent both one directional and bidirectional roads. In a graph structure, can I mix directed and undirected edges or use two edges to represent that a segment of road can be traversed in both direction. Any good example available?"
            ]
        }
    ],
    [
        {
            "id": "205-1-2",
            "pair": [
                "So, unfortunately, at work the other day we put a site into production after some editing/changes that still had the development database configuration. As a result, we have several records spanning across 4 tables in the development DB that need to be appended onto the production DB.\n",
                "Currently I have the necessary data extracted into temporary tables on the development DB. \n"
            ]
        },
        {
            "id": "205-2-3",
            "pair": [
                "Currently I have the necessary data extracted into temporary tables on the development DB. \n",
                "t2 - Submission information - contains submission_id\n"
            ]
        },
        {
            "id": "205-3-4",
            "pair": [
                "t2 - Submission information - contains submission_id\n",
                "t3 - Swing table - contains contact_id & submission_id\n"
            ]
        },
        {
            "id": "205-4-5",
            "pair": [
                "t3 - Swing table - contains contact_id & submission_id\n",
                "t4 - Submission attributes - contains submission_id\n"
            ]
        },
        {
            "id": "205-5-6",
            "pair": [
                "t4 - Submission attributes - contains submission_id\n",
                "I have tried just to simply use a query like this for each table:\n"
            ]
        },
        {
            "id": "205-6-7",
            "pair": [
                "I have tried just to simply use a query like this for each table:\n",
                "However, I quickly found that this is tedious & very error prone. Records could be inserted while I am working and I may get id's mixed up, etc..\n"
            ]
        },
        {
            "id": "205-7-8",
            "pair": [
                "However, I quickly found that this is tedious & very error prone. Records could be inserted while I am working and I may get id's mixed up, etc..\n",
                "I have thought about adding a very high number to all of the id's in the development data set such as 50000 (the highest id in production out of all of the tables is around 28000). then Just insert all of the data into production. Would MySQL figure out the auto-inc values for all of the tables if I did this or would there be a clash eventually?\n"
            ]
        },
        {
            "id": "205-8-9",
            "pair": [
                "I have thought about adding a very high number to all of the id's in the development data set such as 50000 (the highest id in production out of all of the tables is around 28000). then Just insert all of the data into production. Would MySQL figure out the auto-inc values for all of the tables if I did this or would there be a clash eventually?\n",
                "How could I get this relational data from the development database to the production database?\n"
            ]
        },
        {
            "id": "205-9-10",
            "pair": [
                "How could I get this relational data from the development database to the production database?\n",
                "Clearly, I am not the most seasoned of DB admins. Would really appreciate some answers! Please, let me know if there is any other information you might find useful.\n"
            ]
        },
        {
            "id": "205-10-11",
            "pair": [
                "Clearly, I am not the most seasoned of DB admins. Would really appreciate some answers! Please, let me know if there is any other information you might find useful.\n",
                "You'll have to remap them to a non-conflicting insertion range, and to create that you'll need to adjust the AUTO_INCREMENT setting on each affected table.\n"
            ]
        },
        {
            "id": "205-11-12",
            "pair": [
                "You'll have to remap them to a non-conflicting insertion range, and to create that you'll need to adjust the AUTO_INCREMENT setting on each affected table.\n",
                "For example, alter the AUTO_INCREMENT values on the affected tables to introduce some headroom. If you need to insert ~100 records into a table with the increment set to 2500, then set it to 2700. That way you have 200 non-conflicting IDs to work with."
            ]
        }
    ],
    [
        {
            "id": "206-1-2",
            "pair": [
                "You want to implement NIC teaming on the server. Depending on the server and OS there are drivers assoicated with the NICs, or if you run windows 2012 that feature is built into the OS. There may be other OSes that support NIC teaming (I know esx does) but I am not familiar with ,NIX.  Or you can try to 'dual home' the server that is give each NIC its own IP address and have the default gateway point out both switches.\n",
                "I have two servers with two Ethernet ports each. I also have two managed Cisco switches. \n"
            ]
        },
        {
            "id": "206-2-3",
            "pair": [
                "I have two servers with two Ethernet ports each. I also have two managed Cisco switches. \n",
                "Is it remotely possible to have each server connect to each switch in a way to make the switches automatically handle one of the switches dying?\n"
            ]
        },
        {
            "id": "206-3-4",
            "pair": [
                "Is it remotely possible to have each server connect to each switch in a way to make the switches automatically handle one of the switches dying?\n",
                "I know very little of networking, but it seems to me like there would be a way to do something like LAGG across both switches and have the switches communicate with each other in a master/slave failover way.\n"
            ]
        },
        {
            "id": "206-4-5",
            "pair": [
                "I know very little of networking, but it seems to me like there would be a way to do something like LAGG across both switches and have the switches communicate with each other in a master/slave failover way.\n",
                "I'm not looking for the best way to handle this necessarily, more just wanting to know if a method exists and a possible light description of it and how it COULD work."
            ]
        }
    ],
    [
        {
            "id": "207-1-2",
            "pair": [
                "I'm trying to configure RRAS (Routing and Remote Access) on Windows 8 (not a server OS).  Is this possible?  I've started the service \"Routing and Remote Access Services\", but the entry still doesn't appear in the Administrative Tools.  Using Win2008 in the past, I've added this via the Server Manager / Add Roles wizard, but that doesn't exist in Windows 8.\n",
                "BTW, I need to do this so I can setup a port redirection (587 to 25).  I sometimes do it with this command in the Startup folder:\n"
            ]
        },
        {
            "id": "207-2-3",
            "pair": [
                "BTW, I need to do this so I can setup a port redirection (587 to 25).  I sometimes do it with this command in the Startup folder:\n",
                "However, that works only when there's a user logging on.  I need it to happen for services too, when there's no user logged on.  That's why I'm trying to use RRAS instead.  If you have any other suggestions, I'm all ears.\n"
            ]
        },
        {
            "id": "207-3-4",
            "pair": [
                "However, that works only when there's a user logging on.  I need it to happen for services too, when there's no user logged on.  That's why I'm trying to use RRAS instead.  If you have any other suggestions, I'm all ears.\n",
                "If necessary, I'll install a server OS on this machine, but I had hoped to avoid this, as it's rather low usage.\n"
            ]
        },
        {
            "id": "207-4-5",
            "pair": [
                "If necessary, I'll install a server OS on this machine, but I had hoped to avoid this, as it's rather low usage.\n",
                "According to microsoft documentation RRAS is supposed to be applied only to server versions of operation systems (http://technet.microsoft.com/en-us/library/dd314183(v=ws.10).aspx). So you'll have to deal with setup of server OS."
            ]
        }
    ],
    [
        {
            "id": "208-1-2",
            "pair": [
                "As Inkbug mentions, you could run each() straight on the query. split() is definitely an elegant means to retrieve minutes and hours. For style reasons, you probably should have 1 var inside the the each() function. You could split up 480 into 60 * 8, this makes it clear you are supposed to work 8 hours a day. Finally, if you put the code inside a self executing function, you will not mess up the global namespace, and you can then store the code as a bookmarklet.\n",
                "I would make it less compact to improve readability.  Leave it up to your build process (minifier) to make things compact.  Your goal should be to make things understandable.\n"
            ]
        },
        {
            "id": "208-2-3",
            "pair": [
                "I would make it less compact to improve readability.  Leave it up to your build process (minifier) to make things compact.  Your goal should be to make things understandable.\n",
                "In cases like this, where you have a strict format, I love using regular expressions:\n"
            ]
        },
        {
            "id": "208-3-4",
            "pair": [
                "In cases like this, where you have a strict format, I love using regular expressions:\n",
                "This makes it clear that you expect a date to consist of two digits a colon, and two more digits.  As a bonus, let everyone know it's a constant with JSDoc and common coding conventions.\n"
            ]
        },
        {
            "id": "208-4-5",
            "pair": [
                "This makes it clear that you expect a date to consist of two digits a colon, and two more digits.  As a bonus, let everyone know it's a constant with JSDoc and common coding conventions.\n",
                "Here, the parseTime parses the time, returning an object, and calculateOvertime calculates the overtime.\n"
            ]
        },
        {
            "id": "208-5-6",
            "pair": [
                "Here, the parseTime parses the time, returning an object, and calculateOvertime calculates the overtime.\n",
                "Putting it together, your actual function does become more compact and more readable because tangental functions are extracted:\n"
            ]
        },
        {
            "id": "208-6-7",
            "pair": [
                "Putting it together, your actual function does become more compact and more readable because tangental functions are extracted:\n",
                "Right now we are exposing a variety of variables and functions on the global object.  This means that anyone could call parseTime or read a similar function.  It also means a minifier can't rename the function to a shorter name.  Wrapping everything in a closure will be the final step for making sure your output code is more compact and your source needn't be:"
            ]
        }
    ],
    [
        {
            "id": "209-1-2",
            "pair": [
                "Ethernet Flow Control is different than Storm Control.\n",
                "Ethernet Flow Control was developed because traffic on a link may be generated faster than the receiver can handle it. The IEEE has several efforts for this in 802.1 and 802.3. Unfortunately, this really doesn't help with STP loops.\n"
            ]
        },
        {
            "id": "209-2-3",
            "pair": [
                "Ethernet Flow Control was developed because traffic on a link may be generated faster than the receiver can handle it. The IEEE has several efforts for this in 802.1 and 802.3. Unfortunately, this really doesn't help with STP loops.\n",
                "Storm Control is something which some switch vendors have implemented, and it is typically used to limit broadcast and multicast traffic to acceptable levels in order to mitigate STP loops. Cisco does have an implementation for unicast traffic, too.\n"
            ]
        },
        {
            "id": "209-3-4",
            "pair": [
                "Storm Control is something which some switch vendors have implemented, and it is typically used to limit broadcast and multicast traffic to acceptable levels in order to mitigate STP loops. Cisco does have an implementation for unicast traffic, too.\n",
                "If I understand this correctly,  \"Ethernet Flow control\" is used when one computer on a single Ethernet network segment can not cope with the rate that pockets are being sent to it.\n"
            ]
        },
        {
            "id": "209-4-5",
            "pair": [
                "If I understand this correctly,  \"Ethernet Flow control\" is used when one computer on a single Ethernet network segment can not cope with the rate that pockets are being sent to it.\n",
                "\"Ethernet Flow control\" is blocked by switches (bridges), so does not effect what happens on any other Ethernet network that is connected to the switch.\n"
            ]
        },
        {
            "id": "209-5-6",
            "pair": [
                "\"Ethernet Flow control\" is blocked by switches (bridges), so does not effect what happens on any other Ethernet network that is connected to the switch.\n",
                "These days, it is uncommon to find a \"Ethernet network segment\" that consists of more then one computer and one switch port.   In the \"old days\" there used to be lots of computers on each network segment, sharing the same wire.\n"
            ]
        },
        {
            "id": "209-6-7",
            "pair": [
                "These days, it is uncommon to find a \"Ethernet network segment\" that consists of more then one computer and one switch port.   In the \"old days\" there used to be lots of computers on each network segment, sharing the same wire.\n",
                "In real life, most flow control is done by the TCP protocol and network buffers are large enough that \"low level\" flow control is not really needed to the extent it was when I was a child and Ethernet run on yellow cables the thickness of my finger."
            ]
        }
    ],
    [
        {
            "id": "21-1-2",
            "pair": [
                "Way too many magic numbers: 65, 90, 97, 120.   Instead, use literal constants that the reader can understand without consulting an ASCII chart, like:\n",
                "Store your character in a local variable, instead of calling .charAt() on the same character 3 times in the last loop. \n"
            ]
        },
        {
            "id": "21-2-3",
            "pair": [
                "Store your character in a local variable, instead of calling .charAt() on the same character 3 times in the last loop. \n",
                "Consider using Character::isUpperCase() and Character::isLowerCase() functions. \n"
            ]
        },
        {
            "id": "21-3-4",
            "pair": [
                "Consider using Character::isUpperCase() and Character::isLowerCase() functions. \n",
                "Use StringBuilder instead of StringBuffer for efficiency. \n"
            ]
        },
        {
            "id": "21-4-5",
            "pair": [
                "Use StringBuilder instead of StringBuffer for efficiency. \n",
                "You should avoid statements on the same line as an if.\n"
            ]
        },
        {
            "id": "21-5-6",
            "pair": [
                "You should avoid statements on the same line as an if.\n",
                "Also, I find that the (s.charAt(0) < 65) || (s.charAt(0) > 90) are really hard to understand as they use magic numbers.\n"
            ]
        },
        {
            "id": "21-6-7",
            "pair": [
                "Also, I find that the (s.charAt(0) < 65) || (s.charAt(0) > 90) are really hard to understand as they use magic numbers.\n",
                "But, most importantly, are you sure your code is responding to all the cases ?\n"
            ]
        },
        {
            "id": "21-7-8",
            "pair": [
                "But, most importantly, are you sure your code is responding to all the cases ?\n",
                "Let's create some unit test with JUnit and AssertJ :\n"
            ]
        },
        {
            "id": "21-8-9",
            "pair": [
                "Let's create some unit test with JUnit and AssertJ :\n",
                "Without much surprise, the isValidString_shouldAcceptStringWithDiacritics is failing... but so is the isValidString_shouldAcceptVeryLongWord because the 120 isn't the good number !\n"
            ]
        },
        {
            "id": "21-9-10",
            "pair": [
                "Without much surprise, the isValidString_shouldAcceptStringWithDiacritics is failing... but so is the isValidString_shouldAcceptVeryLongWord because the 120 isn't the good number !\n",
                "Never forget to do some unit tests :) (you should also add some tests for numbers and special characters)\n"
            ]
        },
        {
            "id": "21-10-11",
            "pair": [
                "Never forget to do some unit tests :) (you should also add some tests for numbers and special characters)\n",
                "To conclude, if I were you, I'd get rid of the current test altogether and use the Character methods that are nicer lookings, namely isUpperCase and isLowerCase :\n"
            ]
        },
        {
            "id": "21-11-12",
            "pair": [
                "To conclude, if I were you, I'd get rid of the current test altogether and use the Character methods that are nicer lookings, namely isUpperCase and isLowerCase :\n",
                "Now, if you rerun the tests, they should be all green.\n"
            ]
        },
        {
            "id": "21-12-13",
            "pair": [
                "Now, if you rerun the tests, they should be all green.\n",
                "You can also use the same methods for your convert function ;)"
            ]
        }
    ],
    [
        {
            "id": "210-1-2",
            "pair": [
                "I wrote a server program for my company and now I'd like to keep it running on the ubuntu server.\n",
                "I was thinking of setting a cronjob to restart the entire system once a week (for security purposes, to clear RAM in case of memory leaks which I still haven't found.. etc..) but I'm not sure how should I keep the process up.\n"
            ]
        },
        {
            "id": "210-2-3",
            "pair": [
                "I was thinking of setting a cronjob to restart the entire system once a week (for security purposes, to clear RAM in case of memory leaks which I still haven't found.. etc..) but I'm not sure how should I keep the process up.\n",
                "and detaching it. Is this a good way to run a web-exposed program on a server?\n"
            ]
        },
        {
            "id": "210-3-4",
            "pair": [
                "and detaching it. Is this a good way to run a web-exposed program on a server?\n",
                "Do I need something like daemontools or runit for whatever reason?\n"
            ]
        },
        {
            "id": "210-4-5",
            "pair": [
                "Do I need something like daemontools or runit for whatever reason?\n",
                "Your question is actually a combination of two questions:\n"
            ]
        },
        {
            "id": "210-5-6",
            "pair": [
                "Your question is actually a combination of two questions:\n",
                "Use Upstart or systemd (a script in /etc/init.d), depending on your version.\n"
            ]
        },
        {
            "id": "210-6-7",
            "pair": [
                "Use Upstart or systemd (a script in /etc/init.d), depending on your version.\n",
                "The most simple approach would be to add a cron job that checks if the process name is in the running state, and restarts it if it isn't. I would supplement this with having it send you an e-mail when it finds this to be the case so that if there is something happening to the system that is repeatedly killing it, you will know. I would also add timestamped logging to the program if it doesn't already have it, so you can have a record of when these events occur.\n"
            ]
        },
        {
            "id": "210-7-8",
            "pair": [
                "The most simple approach would be to add a cron job that checks if the process name is in the running state, and restarts it if it isn't. I would supplement this with having it send you an e-mail when it finds this to be the case so that if there is something happening to the system that is repeatedly killing it, you will know. I would also add timestamped logging to the program if it doesn't already have it, so you can have a record of when these events occur.\n",
                "If possible, the signal or condition that caused it to die should also be logged so you can prevent it. You may even consider adding the program logs to your automated e-mails."
            ]
        }
    ],
    [
        {
            "id": "211-1-2",
            "pair": [
                "I have a Windows 7 laptop (Dell Inspiron) and I'm trying to simply listen to music through headphones. This process used to work flawlessly.\n",
                "Something changed and now iTunes (as well as WMP and any other audio playing device) will not play through the headphones, which are plugged in. Instead, the music plays through the crappy laptop speakers.\n"
            ]
        },
        {
            "id": "211-2-3",
            "pair": [
                "Something changed and now iTunes (as well as WMP and any other audio playing device) will not play through the headphones, which are plugged in. Instead, the music plays through the crappy laptop speakers.\n",
                "The only thing I can see is that the music seems to be playing through the Speakers/Headphones playback device (seems reasonably named) but should be playing through the Independent R.T.C. Headphones, instead. This info comes from the Sound config area of the Windows 7 Control Panel.\n"
            ]
        },
        {
            "id": "211-3-4",
            "pair": [
                "The only thing I can see is that the music seems to be playing through the Speakers/Headphones playback device (seems reasonably named) but should be playing through the Independent R.T.C. Headphones, instead. This info comes from the Sound config area of the Windows 7 Control Panel.\n",
                "The funny thing is that Skype uses my headphones just fine. I just don't know where to start looking to make sure the media players are using the correct audio when headphones are plugged in.\n"
            ]
        },
        {
            "id": "211-4-5",
            "pair": [
                "The funny thing is that Skype uses my headphones just fine. I just don't know where to start looking to make sure the media players are using the correct audio when headphones are plugged in.\n",
                "When you go to Itunes --> Edit --> preferences --> play --> play audio with:\n"
            ]
        },
        {
            "id": "211-5-6",
            "pair": [
                "When you go to Itunes --> Edit --> preferences --> play --> play audio with:\n",
                "you will see two options 'Direct sound' & 'windows audio session' \n"
            ]
        },
        {
            "id": "211-6-7",
            "pair": [
                "you will see two options 'Direct sound' & 'windows audio session' \n",
                "direct sound is the option to play sound from ear/head phones or external speakers(in case of a dock connection)."
            ]
        }
    ],
    [
        {
            "id": "212-1-2",
            "pair": [
                "The most fail-safe method is to take down the master database, and copy the binary datbase files from the master to the slave, bring them both up and start the replication. With 350Gb of tables, I can understand this could be a problematic solution. You can gain some time by making the copy the fastest you can, which usually means copying to an entirely other set of disks on the master server. Then you can bring the master back up and you can take your time moving the copy over a slower network link.\n",
                "Copying MyISAM files out from underneath MySQL will work, or at least I know it did in the days of MySQL v3 and v4. (Make sure you have the slave shutdown when you run myisamchk.) However, this doesn't work for InnoDB files.\n"
            ]
        },
        {
            "id": "212-2-3",
            "pair": [
                "Copying MyISAM files out from underneath MySQL will work, or at least I know it did in the days of MySQL v3 and v4. (Make sure you have the slave shutdown when you run myisamchk.) However, this doesn't work for InnoDB files.\n",
                "There is a tool called \"MySqL Hot Backup\" or some such will is also capable of doing all this in a cleverer way. The catch is that you may have to pay for it, at least for InnoDB support.\n"
            ]
        },
        {
            "id": "212-3-4",
            "pair": [
                "There is a tool called \"MySqL Hot Backup\" or some such will is also capable of doing all this in a cleverer way. The catch is that you may have to pay for it, at least for InnoDB support.\n",
                "Have you LVM enabled? If yes, you could use it to create snapshot of partition without shutting mysql and then you could mount snapshot and copy files from it. Here is article describing the method.\n"
            ]
        },
        {
            "id": "212-4-5",
            "pair": [
                "Have you LVM enabled? If yes, you could use it to create snapshot of partition without shutting mysql and then you could mount snapshot and copy files from it. Here is article describing the method.\n",
                "If you could shutdown database, you could copy raw data files. Just remember to correct record your master information."
            ]
        }
    ],
    [
        {
            "id": "213-1-2",
            "pair": [
                "I'm having trouble setting up a reverse proxy with apache 2.4.6 on RHEL7.5.\n",
                "I have the following virtualhost which sends requests to 3 backends:\n"
            ]
        },
        {
            "id": "213-2-3",
            "pair": [
                "I have the following virtualhost which sends requests to 3 backends:\n",
                "which means that the ProxyPass directive is not matching my request. I have also tried wrapping the ProxyPass* directives within a <Location> directive but it did not work either.\n"
            ]
        },
        {
            "id": "213-3-4",
            "pair": [
                "which means that the ProxyPass directive is not matching my request. I have also tried wrapping the ProxyPass* directives within a <Location> directive but it did not work either.\n",
                "Trying an equivalent rule (v1) with RewriteRule works as expected, balancing requests between the 3 servers:\n"
            ]
        },
        {
            "id": "213-4-5",
            "pair": [
                "Trying an equivalent rule (v1) with RewriteRule works as expected, balancing requests between the 3 servers:\n",
                "I can see that the requests are matching the proper VirtualHost, since I'm seeing the redirects, but I can't fathom why the ProxyPass is seemingly being ignored.\n"
            ]
        },
        {
            "id": "213-5-6",
            "pair": [
                "I can see that the requests are matching the proper VirtualHost, since I'm seeing the redirects, but I can't fathom why the ProxyPass is seemingly being ignored.\n",
                "I've checked and apache is loading the modules (confirmed also by the server-info handler):\n"
            ]
        },
        {
            "id": "213-6-7",
            "pair": [
                "I've checked and apache is loading the modules (confirmed also by the server-info handler):\n",
                "I have other servers with similar setups running apache v2.4.6 on RHEL7.4 and apache 2.2.15 on RHEL6.5 which work. I can't find any difference in loaded modules between the RHEL7 servers, but still one works and the other doesn't.\n"
            ]
        },
        {
            "id": "213-7-8",
            "pair": [
                "I have other servers with similar setups running apache v2.4.6 on RHEL7.4 and apache 2.2.15 on RHEL6.5 which work. I can't find any difference in loaded modules between the RHEL7 servers, but still one works and the other doesn't.\n",
                "There's probably some difference in configuration which I'm missing. The configuration is mostly default, with changes only to files in /etc/httpd/conf.d (the above virtualhost is contained in its own file).\n"
            ]
        },
        {
            "id": "213-8-9",
            "pair": [
                "There's probably some difference in configuration which I'm missing. The configuration is mostly default, with changes only to files in /etc/httpd/conf.d (the above virtualhost is contained in its own file).\n",
                "What is failing here? Why doesn't the ProxyPass directive work?\n"
            ]
        },
        {
            "id": "213-9-10",
            "pair": [
                "What is failing here? Why doesn't the ProxyPass directive work?\n",
                "RewriteRule gets processed at runtime before ProxyPass. The catchall RewriteRule will match everything. Remove that rule and use ErrorPage directives instead."
            ]
        }
    ],
    [
        {
            "id": "214-1-2",
            "pair": [
                "So you have directly correlated WiFi signal with wind direction. Yet you also correctly pointed out that wind should affect EM waves. While there may be a correlation, if it does exist it is likely indirect at best.\n",
                "Let me start by mentioning you don't provide how you are determining \"nice strong Wi Fi signal\". Often wireless clients represent a signal using some sort of iconography. Five bars is good and one is bad, right? But just what does that little icon represent? Is it actual signal strength or signal quality or something else? For example, on my cell phone, I know that one bar of \"LTE\" is better than three bars of \"4G\". So three bars isn't always better than one bar. But back to the question.\n"
            ]
        },
        {
            "id": "214-2-3",
            "pair": [
                "Let me start by mentioning you don't provide how you are determining \"nice strong Wi Fi signal\". Often wireless clients represent a signal using some sort of iconography. Five bars is good and one is bad, right? But just what does that little icon represent? Is it actual signal strength or signal quality or something else? For example, on my cell phone, I know that one bar of \"LTE\" is better than three bars of \"4G\". So three bars isn't always better than one bar. But back to the question.\n",
                "As Ron already mentioned in his post, the frequencies used by WiFi are subject to higher absorption by water. So his suggestion that plants shifting could be part of the answer. Wind also tends to move in certain directions based on weather conditions, which may include humidity.\n"
            ]
        },
        {
            "id": "214-3-4",
            "pair": [
                "As Ron already mentioned in his post, the frequencies used by WiFi are subject to higher absorption by water. So his suggestion that plants shifting could be part of the answer. Wind also tends to move in certain directions based on weather conditions, which may include humidity.\n",
                "But you point out that your \"signal improves when it rains or is cloudy\" so this can't be the case. This statement of course runs contrary to physics, but taking it at face value may be a clue to what is taking place on a larger scale. But back to that in a minute.\n"
            ]
        },
        {
            "id": "214-4-5",
            "pair": [
                "But you point out that your \"signal improves when it rains or is cloudy\" so this can't be the case. This statement of course runs contrary to physics, but taking it at face value may be a clue to what is taking place on a larger scale. But back to that in a minute.\n",
                "If wind direction is truly a factor, then water (plant, humidity, etc) is certainly one thing that could be different. Another thing could be is the geometry at play. Buildings and structures can/will flex or shift with different prevailing winds. These changes may be negligible when both the client and the AP are in the same building, however you mention that you are \"next to a university campus\" rather than in the same building as the AP. \n"
            ]
        },
        {
            "id": "214-5-6",
            "pair": [
                "If wind direction is truly a factor, then water (plant, humidity, etc) is certainly one thing that could be different. Another thing could be is the geometry at play. Buildings and structures can/will flex or shift with different prevailing winds. These changes may be negligible when both the client and the AP are in the same building, however you mention that you are \"next to a university campus\" rather than in the same building as the AP. \n",
                "While these changes are often relatively small, modern 802.11 traffic (802.11n or newer) takes advantage of a phenomenon known as multipath propogation through the use of more than one radio chain and/or spatial stream simultaneously. Without getting into too much detail, ultimately as conditions change, the best \"paths\" between to RF endpoints will change as well. These subtle changes can result in big differences to signal strength/quality and with multipath the coverage pattern is often far from intuitive.\n"
            ]
        },
        {
            "id": "214-6-7",
            "pair": [
                "While these changes are often relatively small, modern 802.11 traffic (802.11n or newer) takes advantage of a phenomenon known as multipath propogation through the use of more than one radio chain and/or spatial stream simultaneously. Without getting into too much detail, ultimately as conditions change, the best \"paths\" between to RF endpoints will change as well. These subtle changes can result in big differences to signal strength/quality and with multipath the coverage pattern is often far from intuitive.\n",
                "So, end result is that the path the signal gets from the AP to your client could change based on things like wind direction.\n"
            ]
        },
        {
            "id": "214-7-8",
            "pair": [
                "So, end result is that the path the signal gets from the AP to your client could change based on things like wind direction.\n",
                "Getting back to your comment that \"signal improves when it rains or is cloudy\",  and you wonder if this could have anything to do with cloud cover bouncing signal back. Physics tells us that this is impossible (the signal strength of an 802.11 device is not strong enough to be usable at the level of clouds, much less to the cloud cover and back) and further that rain/water will absorb RF signal in the frequencies in question.\n"
            ]
        },
        {
            "id": "214-8-9",
            "pair": [
                "Getting back to your comment that \"signal improves when it rains or is cloudy\",  and you wonder if this could have anything to do with cloud cover bouncing signal back. Physics tells us that this is impossible (the signal strength of an 802.11 device is not strong enough to be usable at the level of clouds, much less to the cloud cover and back) and further that rain/water will absorb RF signal in the frequencies in question.\n",
                "So why does your signal improve? My best guess would be that you are measuring signal quality rather than signal strength. While it may seem counter-intuitive, you may actually have better signal quality when conditions are less than ideal.\n"
            ]
        },
        {
            "id": "214-9-10",
            "pair": [
                "So why does your signal improve? My best guess would be that you are measuring signal quality rather than signal strength. While it may seem counter-intuitive, you may actually have better signal quality when conditions are less than ideal.\n",
                "Under \"better\" conditions, your wireless client may be seeing more interference on the channel (i.e. other devices) or other sources of RF noise. This may reduce the signal quality. Less ideal conditions (such as raining, changes in the RF path) may be absorbing or masking a portion of that other interference or noise, allowing your signal to be better.\n"
            ]
        },
        {
            "id": "214-10-11",
            "pair": [
                "Under \"better\" conditions, your wireless client may be seeing more interference on the channel (i.e. other devices) or other sources of RF noise. This may reduce the signal quality. Less ideal conditions (such as raining, changes in the RF path) may be absorbing or masking a portion of that other interference or noise, allowing your signal to be better.\n",
                "Of course, this is all just speculation on my part. Without knowing a lot more about your particular situation, no one could say for certain why things are working as you report. There may be other factors of which we aren't aware; for instance, university buildings near you may be more heavily occupied (more people/devices) under certain circumstances and less occupied under others.\n"
            ]
        },
        {
            "id": "214-11-12",
            "pair": [
                "Of course, this is all just speculation on my part. Without knowing a lot more about your particular situation, no one could say for certain why things are working as you report. There may be other factors of which we aren't aware; for instance, university buildings near you may be more heavily occupied (more people/devices) under certain circumstances and less occupied under others.\n",
                "Water is a Wi-Fi killer. Plants (trees, etc.) are full of water. The wind moving the plants a certain direction, or bringing in moisture can greatly affect Wi-Fi signals.\n"
            ]
        },
        {
            "id": "214-12-13",
            "pair": [
                "Water is a Wi-Fi killer. Plants (trees, etc.) are full of water. The wind moving the plants a certain direction, or bringing in moisture can greatly affect Wi-Fi signals.\n",
                "One of the worst plants for Wi-Fi is alfalfa, but I assume you do not live next to alfalfa fields."
            ]
        }
    ],
    [
        {
            "id": "215-1-2",
            "pair": [
                "Looks like GeoTrust has done some re-jiggering of their signing structure.\n",
                "The third certificate that you're presenting (per Raj's answer) is this one:\n"
            ]
        },
        {
            "id": "215-2-3",
            "pair": [
                "The third certificate that you're presenting (per Raj's answer) is this one:\n",
                "But when I open your certificate with a Windows client, the chain resolves to this one:\n"
            ]
        },
        {
            "id": "215-3-4",
            "pair": [
                "But when I open your certificate with a Windows client, the chain resolves to this one:\n",
                "These two certificates share common cryptography (so they'll both verify for the subordinate's signature), but the one you're presenting is signed by that Equifax certificate, while the one that Windows decides the trust chain leads to is signed by Equifax.\n"
            ]
        },
        {
            "id": "215-4-5",
            "pair": [
                "These two certificates share common cryptography (so they'll both verify for the subordinate's signature), but the one you're presenting is signed by that Equifax certificate, while the one that Windows decides the trust chain leads to is signed by Equifax.\n",
                "Since you're presenting the certificate that is signed by Equifax instead of the self-signed one, you're not presenting the full chain.  Add the Equifax certificate to the chain that your web server is presenting:\n"
            ]
        },
        {
            "id": "215-5-6",
            "pair": [
                "Since you're presenting the certificate that is signed by Equifax instead of the self-signed one, you're not presenting the full chain.  Add the Equifax certificate to the chain that your web server is presenting:\n",
                "Now, that may or may not resolve the trust issues with your openssl command line client - you'll need to make sure that it's properly finding your system's store of trusted roots; you can use -CApath to make sure that it's pointing to the correct location.\n"
            ]
        },
        {
            "id": "215-6-7",
            "pair": [
                "Now, that may or may not resolve the trust issues with your openssl command line client - you'll need to make sure that it's properly finding your system's store of trusted roots; you can use -CApath to make sure that it's pointing to the correct location.\n",
                "Posting in answer instead of comments as this is too big for comment. Don't have ubuntu to test with right at this moment, but chain looks complete from my redhat server(rhel 5 update 3):\n"
            ]
        },
        {
            "id": "215-7-8",
            "pair": [
                "Posting in answer instead of comments as this is too big for comment. Don't have ubuntu to test with right at this moment, but chain looks complete from my redhat server(rhel 5 update 3):\n",
                "openssl s_client -showcerts -connect dev.carlipa-online.com:443"
            ]
        }
    ],
    [
        {
            "id": "216-1-2",
            "pair": [
                "This should be a simple thing to find, but I've spent over an hour on it--time to ask for some assistance.  I have a very basic CentOS 5.5 box running two virtual machines.  The first machine has been up for several weeks without any issues and accepts connections over 80 and 443 with a signed SSL cert.\n",
                "Today I went in to add a second Virtual Host entry for a site that will only need to listen on 80.  I set up the VM as I normally do, created a SIMPLE index.php script that contains only the word 'test'.\n"
            ]
        },
        {
            "id": "216-2-3",
            "pair": [
                "Today I went in to add a second Virtual Host entry for a site that will only need to listen on 80.  I set up the VM as I normally do, created a SIMPLE index.php script that contains only the word 'test'.\n",
                "If I mv that file to index.html the page loads just fine, but when it is index.php I get a 500 error:\n"
            ]
        },
        {
            "id": "216-3-4",
            "pair": [
                "If I mv that file to index.html the page loads just fine, but when it is index.php I get a 500 error:\n",
                "In my logs I get no errors but the access log shows:\n"
            ]
        },
        {
            "id": "216-4-5",
            "pair": [
                "In my logs I get no errors but the access log shows:\n",
                "The original site is a PHP/MySQL site and is working just fine!  At this point I am just out of ideas for where to look so any suggestions would be appreciated.\n"
            ]
        },
        {
            "id": "216-5-6",
            "pair": [
                "The original site is a PHP/MySQL site and is working just fine!  At this point I am just out of ideas for where to look so any suggestions would be appreciated.\n",
                "Are any other PHP files loading correctly on the 2nd VM? \n"
            ]
        },
        {
            "id": "216-6-7",
            "pair": [
                "Are any other PHP files loading correctly on the 2nd VM? \n",
                "save as test.php under your 2nd VM web directory and browse to it.. This will give you an idea if its related to the PHP install, your apache, etc.\n"
            ]
        },
        {
            "id": "216-7-8",
            "pair": [
                "save as test.php under your 2nd VM web directory and browse to it.. This will give you an idea if its related to the PHP install, your apache, etc.\n",
                "I would also check the errors.log if you haven't already.. Better yet, tail -f errors.log while you attempt to access a PHP page."
            ]
        }
    ],
    [
        {
            "id": "217-1-2",
            "pair": [
                "Millions of cron jobs all over the world are being run as root every day (or whatever period they're set to run).\n",
                "The important thing is that proper permissions are set. If you're running something that's writable by everybody, then a malicious user or process could change what it's doing.\n"
            ]
        },
        {
            "id": "217-2-3",
            "pair": [
                "The important thing is that proper permissions are set. If you're running something that's writable by everybody, then a malicious user or process could change what it's doing.\n",
                "Cron jobs are run by the owner of the crontab, generally speaking. A user crontab might be in /var/spool/cron/crontabs/username for example. Cronjobs that are in /etc/crontab, /etc/cron.d/ or /etc/cron.hourly (daily, weekly, monthly) will be run by root. It's important that the ownership and permissions are correct for these crontab files, too.\n"
            ]
        },
        {
            "id": "217-3-4",
            "pair": [
                "Cron jobs are run by the owner of the crontab, generally speaking. A user crontab might be in /var/spool/cron/crontabs/username for example. Cronjobs that are in /etc/crontab, /etc/cron.d/ or /etc/cron.hourly (daily, weekly, monthly) will be run by root. It's important that the ownership and permissions are correct for these crontab files, too.\n",
                "If you have secured access to the script sufficiently and made sensible precautions, running something from roots crontab is not usually a security risk.\n"
            ]
        },
        {
            "id": "217-4-5",
            "pair": [
                "If you have secured access to the script sufficiently and made sensible precautions, running something from roots crontab is not usually a security risk.\n",
                "But don't run a script as root that a non root user can edit or overwrite. This applies to jobs run from cron as well as interactively.\n"
            ]
        },
        {
            "id": "217-5-6",
            "pair": [
                "But don't run a script as root that a non root user can edit or overwrite. This applies to jobs run from cron as well as interactively.\n",
                "If that script includes other files same applies to them too.  \n"
            ]
        },
        {
            "id": "217-6-7",
            "pair": [
                "If that script includes other files same applies to them too.  \n",
                "If in doubt always use the principle of least privilege. If you are still unsure you can always ask specific questions on forums and in IRC.\n"
            ]
        },
        {
            "id": "217-7-8",
            "pair": [
                "If in doubt always use the principle of least privilege. If you are still unsure you can always ask specific questions on forums and in IRC.\n",
                "There is (nearly) always a way to run something as a non root user. If all else fails using sudo to limit a user to specific commands also limits the potential to do harm.\n"
            ]
        },
        {
            "id": "217-8-9",
            "pair": [
                "There is (nearly) always a way to run something as a non root user. If all else fails using sudo to limit a user to specific commands also limits the potential to do harm.\n",
                "So with the example you gave of backing up /etc/apache2/sites-available, that file is by default readable by anyone, so that implies it is access to the destination that is writeable by root only. "
            ]
        }
    ],
    [
        {
            "id": "218-1-2",
            "pair": [
                "I'm trying to run an old piece of software in a VirtualBox VM running Windows XP. My host system is Debian.\n",
                "I have the CD-ROM image in MDF + MDS format (which I believe is an uncommon format created by Alcohol 120%). I converted it to ISO, but unfortunately that breaks the copy-protection on the image, which the MDF format presumably retains, so I'm prevented from running the software after installation.\n"
            ]
        },
        {
            "id": "218-2-3",
            "pair": [
                "I have the CD-ROM image in MDF + MDS format (which I believe is an uncommon format created by Alcohol 120%). I converted it to ISO, but unfortunately that breaks the copy-protection on the image, which the MDF format presumably retains, so I'm prevented from running the software after installation.\n",
                "The ideal solution here would be to somehow mount the image in its original format and map the drive so VirtualBox can mount it (it doesn't support MDF + MDS natively, clearly).\n"
            ]
        },
        {
            "id": "218-3-4",
            "pair": [
                "The ideal solution here would be to somehow mount the image in its original format and map the drive so VirtualBox can mount it (it doesn't support MDF + MDS natively, clearly).\n",
                "I may have to convert the image first. The problem is retaining the copy-protection, held in the MDS file. Does anyone have experience with this?\n"
            ]
        },
        {
            "id": "218-4-5",
            "pair": [
                "I may have to convert the image first. The problem is retaining the copy-protection, held in the MDS file. Does anyone have experience with this?\n",
                "If the image was correctly done you can install Alchool 120% in the guest system (in the virtualized Windows XP) and mount your image from there.\n"
            ]
        },
        {
            "id": "218-5-6",
            "pair": [
                "If the image was correctly done you can install Alchool 120% in the guest system (in the virtualized Windows XP) and mount your image from there.\n",
                " If it doesn't work (and it is a working image), then we have to deduce that the hardware emulation layer introduced by the virtualization is not accurate enough to deal with Alchool 120% and the \"special sectors\"...(with a pun: Alcohol 120% run less then 100%).\n"
            ]
        },
        {
            "id": "218-6-7",
            "pair": [
                " If it doesn't work (and it is a working image), then we have to deduce that the hardware emulation layer introduced by the virtualization is not accurate enough to deal with Alchool 120% and the \"special sectors\"...(with a pun: Alcohol 120% run less then 100%).\n",
                "If your virtualized environment doesn't allow to mount that format natively, \n"
            ]
        },
        {
            "id": "218-7-8",
            "pair": [
                "If your virtualized environment doesn't allow to mount that format natively, \n",
                "you may try more way to convert the image in a more common format, e.g. an ISO. (The OP tried with no success).\n"
            ]
        },
        {
            "id": "218-8-9",
            "pair": [
                "you may try more way to convert the image in a more common format, e.g. an ISO. (The OP tried with no success).\n",
                "You need to put particularly attention doing a byte for byte copy and not a simple copy of the data.\n"
            ]
        },
        {
            "id": "218-9-10",
            "pair": [
                "You need to put particularly attention doing a byte for byte copy and not a simple copy of the data.\n",
                "It may be needed to copy even the read only sectors and to check if there were sectors of different (special) sizes... \n"
            ]
        },
        {
            "id": "218-10-11",
            "pair": [
                "It may be needed to copy even the read only sectors and to check if there were sectors of different (special) sizes... \n",
                "If with one of the previous solutions the image works, it means that the image was correctly done and you can start to search for other program/options in order to convert it in a format that VirtualBox can mount."
            ]
        }
    ],
    [
        {
            "id": "219-1-2",
            "pair": [
                "Full disclosure: I'm predominantly a Chrome user on Windows, but I finally let Firefox download and install the latest version, 4.0. Considering this version has been in development forever, I frankly expected better.\n",
                "It appears that they've \"improved\" text rendering by applying font smoothing or anti-aliasing across the entire user interface\u2014not just the text displayed on the pages themselves, but also the text in menus, toolbars, and dialog boxes. Even the context menus use a re-hinted version of Segoe UI.\n"
            ]
        },
        {
            "id": "219-2-3",
            "pair": [
                "It appears that they've \"improved\" text rendering by applying font smoothing or anti-aliasing across the entire user interface\u2014not just the text displayed on the pages themselves, but also the text in menus, toolbars, and dialog boxes. Even the context menus use a re-hinted version of Segoe UI.\n",
                "I think it's blurry and very difficult to read. How do I turn it off? I can't find anything in the Options dialog that looks useful. Perhaps it's something in about:config, but I don't know enough about Firefox to know where to start looking. I can't imagine they would make such a dramatic change without providing the facility to revert to the classic rendering mode if the user so desired. Open source software is notorious for gratuitous customizability; here's a case where I would actually appreciate it.\n"
            ]
        },
        {
            "id": "219-3-4",
            "pair": [
                "I think it's blurry and very difficult to read. How do I turn it off? I can't find anything in the Options dialog that looks useful. Perhaps it's something in about:config, but I don't know enough about Firefox to know where to start looking. I can't imagine they would make such a dramatic change without providing the facility to revert to the classic rendering mode if the user so desired. Open source software is notorious for gratuitous customizability; here's a case where I would actually appreciate it.\n",
                "Additionally, I've discovered that I frequently out-run its ability to process input. In every text entry field that I've come across, I've out-typed Firefox's ability to display each character. More than once, I've even managed to crash the browser trying to type, with only one tab loaded. I suspect this is related to the font rendering changes, as anti-aliasing requires more computing \"umph\". I'm hoping that if I find the button to turn that off, it'll fix the severe typing lag. But if anyone just so happens to have any other suggestions on how I might fix that, I'm all ears.\n"
            ]
        },
        {
            "id": "219-4-5",
            "pair": [
                "Additionally, I've discovered that I frequently out-run its ability to process input. In every text entry field that I've come across, I've out-typed Firefox's ability to display each character. More than once, I've even managed to crash the browser trying to type, with only one tab loaded. I suspect this is related to the font rendering changes, as anti-aliasing requires more computing \"umph\". I'm hoping that if I find the button to turn that off, it'll fix the severe typing lag. But if anyone just so happens to have any other suggestions on how I might fix that, I'm all ears.\n",
                "The machine in question is a Dual Xeon with 5 GB of RAM running Windows Server 2008 R2, 64-bit.\n"
            ]
        },
        {
            "id": "219-5-6",
            "pair": [
                "The machine in question is a Dual Xeon with 5 GB of RAM running Windows Server 2008 R2, 64-bit.\n",
                "Since Firefox 60, the setting to disable anti-aliasing is now  gfx.text.disable-aa."
            ]
        }
    ],
    [
        {
            "id": "22-1-2",
            "pair": [
                "In my experience (mostly mobile titles), the audio quality should be as low as as possible without overt negative side effects so that you have more in memory for other things.\n",
                "Keep in mind that just because there's compression involved doesn't necessarily mean that your quality is too low.  Your audience isn't going to be doing side by side comparisons with the base sounds like you are, so if they're missing something through the lossy compression, they're generally not going to know about it.  \n"
            ]
        },
        {
            "id": "22-2-3",
            "pair": [
                "Keep in mind that just because there's compression involved doesn't necessarily mean that your quality is too low.  Your audience isn't going to be doing side by side comparisons with the base sounds like you are, so if they're missing something through the lossy compression, they're generally not going to know about it.  \n",
                "For example, you have to ask yourself if there's any reason to use stereo sounds or music.  Is the effect it gives you worth nearly twice the overhead?  For music, depending on how it's authored, the answer might be yes.  Some engines don't even support stereo sources for sound effects.  That kind of thing.\n"
            ]
        },
        {
            "id": "22-3-4",
            "pair": [
                "For example, you have to ask yourself if there's any reason to use stereo sounds or music.  Is the effect it gives you worth nearly twice the overhead?  For music, depending on how it's authored, the answer might be yes.  Some engines don't even support stereo sources for sound effects.  That kind of thing.\n",
                "Where that line is is very subjective, of course.  There are other answers that go into more specific rules of thumb.  \n"
            ]
        },
        {
            "id": "22-4-5",
            "pair": [
                "Where that line is is very subjective, of course.  There are other answers that go into more specific rules of thumb.  \n",
                "The important thing is to have standards, set them early, and try to stick to it.  If you're authoring music for a game and have to half its bitrate to fit into memory, then you're going to be much worse off than if you set a low quality level in the first place.\n"
            ]
        },
        {
            "id": "22-5-6",
            "pair": [
                "The important thing is to have standards, set them early, and try to stick to it.  If you're authoring music for a game and have to half its bitrate to fit into memory, then you're going to be much worse off than if you set a low quality level in the first place.\n",
                "Sound and it's quality is something like religion. Never ending story.\n"
            ]
        },
        {
            "id": "22-6-7",
            "pair": [
                "Sound and it's quality is something like religion. Never ending story.\n",
                "People will say, that MP3 320kbps is lossless, while experts will say that any MP3 is crap, but in the end, no one will hear a difference in the result... unless they're in the music industry for 20 years+.\n"
            ]
        },
        {
            "id": "22-7-8",
            "pair": [
                "People will say, that MP3 320kbps is lossless, while experts will say that any MP3 is crap, but in the end, no one will hear a difference in the result... unless they're in the music industry for 20 years+.\n",
                "What would be the best format for video game, and what properties should be enough (Hertz, bitrate etc.), assuming that 320kbps MP3 is \"heavy\"?\n"
            ]
        },
        {
            "id": "22-8-9",
            "pair": [
                "What would be the best format for video game, and what properties should be enough (Hertz, bitrate etc.), assuming that 320kbps MP3 is \"heavy\"?\n",
                "Maybe an example or two, how some AAA titles work with their sounds."
            ]
        }
    ],
    [
        {
            "id": "220-1-2",
            "pair": [
                "If you have no access to switches to get ARP tables, your last resort can be physical access to the cameras: all cams I've seen had a sticker or other label with serial number and MAC-address, unless someone removed these labels.\n",
                "Also, if these cameras sending their signal somewhere, there must be a kind of list with their IPs and so on.\n"
            ]
        },
        {
            "id": "220-2-3",
            "pair": [
                "Also, if these cameras sending their signal somewhere, there must be a kind of list with their IPs and so on.\n",
                "Another way: recently I installed onto my android tablet the software \"IP cam viewer lite\" by Robert Chou. It scans the network for different models of IP-cams, and does that reliable enough.\n"
            ]
        },
        {
            "id": "220-3-4",
            "pair": [
                "Another way: recently I installed onto my android tablet the software \"IP cam viewer lite\" by Robert Chou. It scans the network for different models of IP-cams, and does that reliable enough.\n",
                "Knowing the IP-address boundaries of the network in question, you may run nmap tool for ping-scan of this range. And, yes, nmap is available for Windows (ask guys from Spiceworks team)\n"
            ]
        },
        {
            "id": "220-4-5",
            "pair": [
                "Knowing the IP-address boundaries of the network in question, you may run nmap tool for ping-scan of this range. And, yes, nmap is available for Windows (ask guys from Spiceworks team)\n",
                "I work in the security devices industry and most of my work relates to IP cameras. Each camera manufacturer has their own software that scans the network for their cameras and gives details about those cameras even if they have no assigned ip address. The problem is that sometimes we go to sites that have cameras from different manufacturers and the people there don't always know what makes they are or even where they are (large sites obviously) so for us trying to reconfigure those cameras gets a bit tricky since you always have to have all the software from different manufacturers with you all the time.\n"
            ]
        },
        {
            "id": "220-5-6",
            "pair": [
                "I work in the security devices industry and most of my work relates to IP cameras. Each camera manufacturer has their own software that scans the network for their cameras and gives details about those cameras even if they have no assigned ip address. The problem is that sometimes we go to sites that have cameras from different manufacturers and the people there don't always know what makes they are or even where they are (large sites obviously) so for us trying to reconfigure those cameras gets a bit tricky since you always have to have all the software from different manufacturers with you all the time.\n",
                "Is there a way that I can scan the network and find the mac and ip (or self assigned ip) addresses even if no ip address is assigned? \n"
            ]
        },
        {
            "id": "220-6-7",
            "pair": [
                "Is there a way that I can scan the network and find the mac and ip (or self assigned ip) addresses even if no ip address is assigned? \n",
                "Ideally I'd like to know how to do this with Windows, but linux might be ok too."
            ]
        }
    ],
    [
        {
            "id": "221-1-2",
            "pair": [
                "I routinely backup (copy) all my Android phone's files to a portable hard drive using my PC.\n",
                "My old phone died. They sent me a new one. I restored all my old files to the new phone, but my old contacts did not restore. \n"
            ]
        },
        {
            "id": "221-2-3",
            "pair": [
                "My old phone died. They sent me a new one. I restored all my old files to the new phone, but my old contacts did not restore. \n",
                "What have I done wrong? Is there some trick to restoring files?\n"
            ]
        },
        {
            "id": "221-3-4",
            "pair": [
                "What have I done wrong? Is there some trick to restoring files?\n",
                "Think of Contacts1.1 as a Windows program that reads it's data from a location that user cannot change... Say, C:\\Users\\sam\\contacts\\... You backup your C:\\Users\\sam\\ directory, and restore it on another Windows machine that has Contacts2.0 installed. Now, your contacts are in the new system as binary data, but if Contacts2.0 is reading them from C:\\Program Files\\Contats only, you wont be able to see them.\n"
            ]
        },
        {
            "id": "221-4-5",
            "pair": [
                "Think of Contacts1.1 as a Windows program that reads it's data from a location that user cannot change... Say, C:\\Users\\sam\\contacts\\... You backup your C:\\Users\\sam\\ directory, and restore it on another Windows machine that has Contacts2.0 installed. Now, your contacts are in the new system as binary data, but if Contacts2.0 is reading them from C:\\Program Files\\Contats only, you wont be able to see them.\n",
                "You could manually move them, but what if they are also in different format?\n"
            ]
        },
        {
            "id": "221-5-6",
            "pair": [
                "You could manually move them, but what if they are also in different format?\n",
                "In a real scenario, database fields might be different.\n"
            ]
        },
        {
            "id": "221-6-7",
            "pair": [
                "In a real scenario, database fields might be different.\n",
                "That's why you manually export/import data to/from a format that is supported on every version of that program. Program then stores the data as it likes.\n"
            ]
        },
        {
            "id": "221-7-8",
            "pair": [
                "That's why you manually export/import data to/from a format that is supported on every version of that program. Program then stores the data as it likes.\n",
                "Windows Mail client for example uses .eml, Outlook uses .pst, and Contacts uses .vcf."
            ]
        }
    ],
    [
        {
            "id": "222-1-2",
            "pair": [
                "Two tables that are in 1:1 relationship should, in almost all cases, be combined into a single table.  Here are some exceptions of where it may be beneficial to have multiple, parallel, vertically partitioned, tables.  Note: you probably do not have any of these cases.\n",
                "Each way has its pros and cons of course. However, here is how I would precede:\n"
            ]
        },
        {
            "id": "222-2-3",
            "pair": [
                "Each way has its pros and cons of course. However, here is how I would precede:\n",
                "I would split the columns into two tables. One for the columns that are frequently used, and the other for the columns that are used less frequently. \n"
            ]
        },
        {
            "id": "222-3-4",
            "pair": [
                "I would split the columns into two tables. One for the columns that are frequently used, and the other for the columns that are used less frequently. \n",
                "For example, if you use this table to authentications users, you probably would need these fields very often: email/username, password, first and last name (for greetings), and probably few other fields, like score or last login time. \n"
            ]
        },
        {
            "id": "222-4-5",
            "pair": [
                "For example, if you use this table to authentications users, you probably would need these fields very often: email/username, password, first and last name (for greetings), and probably few other fields, like score or last login time. \n",
                "On the other hand, you will rarely be in need of the user's address, or the primary school they attended. So, there is no need to load this data each time you use the table. \n"
            ]
        },
        {
            "id": "222-5-6",
            "pair": [
                "On the other hand, you will rarely be in need of the user's address, or the primary school they attended. So, there is no need to load this data each time you use the table. \n",
                "So, grouping columns into [frequently used, less frequently used] has an advantage of utilizing resources. However, it is slightly more work at table management level. \n"
            ]
        },
        {
            "id": "222-6-7",
            "pair": [
                "So, grouping columns into [frequently used, less frequently used] has an advantage of utilizing resources. However, it is slightly more work at table management level. \n",
                "If you go with the option of having all columns in one table, consider covering indexes for the fields that are usually queries together. "
            ]
        }
    ],
    [
        {
            "id": "223-1-2",
            "pair": [
                "Noob poster. Running windows 7, recently updated with all critical updates. Had BSOD with a non paged memory error on startup a few days ago. Otherwise, the system runs well, although I have noticed some problems with Chrome browser crashing, which I just attributed to the usual nonsense.\n",
                "Tried to run windows update today and found the service was not running. Went to Control Panel > Adminsitrative Tools > Services and found Windows Update not running and all the other services hung at \"started\". \n"
            ]
        },
        {
            "id": "223-2-3",
            "pair": [
                "Tried to run windows update today and found the service was not running. Went to Control Panel > Adminsitrative Tools > Services and found Windows Update not running and all the other services hung at \"started\". \n",
                "Started windows update. It hangs at \"started\" too. Tried to run it, says not runnning, try restart... which has no effect, naturally.\n"
            ]
        },
        {
            "id": "223-3-4",
            "pair": [
                "Started windows update. It hangs at \"started\" too. Tried to run it, says not runnning, try restart... which has no effect, naturally.\n",
                "Any ideas? Is this a OS problem or a hardware problem? Virus?\n"
            ]
        },
        {
            "id": "223-4-5",
            "pair": [
                "Any ideas? Is this a OS problem or a hardware problem? Virus?\n",
                "Yes, very likely a virus. There's a few things you might be able to jimmy it with.  \n"
            ]
        },
        {
            "id": "223-5-6",
            "pair": [
                "Yes, very likely a virus. There's a few things you might be able to jimmy it with.  \n",
                "1) Microsoft update fix it. Download it off their website. it'll run a diagnostic set, maybe fix you up.\n"
            ]
        },
        {
            "id": "223-6-7",
            "pair": [
                "1) Microsoft update fix it. Download it off their website. it'll run a diagnostic set, maybe fix you up.\n",
                "2) open up a search, look for .cmd. Right click on program cmd in pulldown, choose \"run as admin\". When you have the dos shell open, type\n"
            ]
        },
        {
            "id": "223-7-8",
            "pair": [
                "2) open up a search, look for .cmd. Right click on program cmd in pulldown, choose \"run as admin\". When you have the dos shell open, type\n",
                "3) Download combofix off of bleepingcomputer.com(there are lookalikes out there that ARE viruses, which is why i recommend getting it there). Run as admin"
            ]
        }
    ],
    [
        {
            "id": "224-1-2",
            "pair": [
                "Similar situation with me. I use a bash script to convert a list of domain names, e.g.:\n",
                "Into a list of IP addresses. Then, I fed the list of IP addresses into ipset:\n"
            ]
        },
        {
            "id": "224-2-3",
            "pair": [
                "Into a list of IP addresses. Then, I fed the list of IP addresses into ipset:\n",
                "The benefit: Every morning a cron job re-resolves the domain list into an IP list and updates the Blacklist set, and I don't have to touch the iptables rule at all. The update script uses -F instead of -N for its first line.\n"
            ]
        },
        {
            "id": "224-3-4",
            "pair": [
                "The benefit: Every morning a cron job re-resolves the domain list into an IP list and updates the Blacklist set, and I don't have to touch the iptables rule at all. The update script uses -F instead of -N for its first line.\n",
                "The trick is to use Squid with authenticated users. SSL traffic can't be proxied if you are running a transparent proxy. Squid can run both ways at the same time (on different ports):\n"
            ]
        },
        {
            "id": "224-4-5",
            "pair": [
                "The trick is to use Squid with authenticated users. SSL traffic can't be proxied if you are running a transparent proxy. Squid can run both ways at the same time (on different ports):\n",
                "You would obviously have to add some rules to allow and deny authenticated users to navigate where it is allowed or forbiden. Still, users who are accessing the web transparently, will be forbiden access to HTTPS if it is blocked on your firewall.\n"
            ]
        },
        {
            "id": "224-5-6",
            "pair": [
                "You would obviously have to add some rules to allow and deny authenticated users to navigate where it is allowed or forbiden. Still, users who are accessing the web transparently, will be forbiden access to HTTPS if it is blocked on your firewall.\n",
                "The other way (more dirty one), would be to get the sites allowed from a file, get their DNS records, and update/remove rules, something like:\n"
            ]
        },
        {
            "id": "224-6-7",
            "pair": [
                "The other way (more dirty one), would be to get the sites allowed from a file, get their DNS records, and update/remove rules, something like:\n",
                "This would create a new chain SSL_FORWARD, and send packets coming from your net destined to port 443 to be evaluated on this new chain. If the packet doesn't match any rule inside this chain, then it will be dropped."
            ]
        }
    ],
    [
        {
            "id": "225-1-2",
            "pair": [
                "Docker uses seccomp, which is a syscall filter \u2013 it can whitelist or blacklist specific kernel calls, e.g. block module loading or time adjustment.\n",
                "Docker can use AppArmor, which is a security module designed to enforce per-app policies beyond ordinary UID-based privileges. (Some distributions use AppArmor even for standard apps.)\n"
            ]
        },
        {
            "id": "225-2-3",
            "pair": [
                "Docker can use AppArmor, which is a security module designed to enforce per-app policies beyond ordinary UID-based privileges. (Some distributions use AppArmor even for standard apps.)\n",
                "Docker can also use user namespaces, which allow each container to have distinct UIDs from the host system. Your container processes think they are uid 0, but only have root-like privileges within their own domain (user namespace), while the host system sees a normal non-root uid. Each container's uid 0 is also separate from every other container's uid 0.\n"
            ]
        },
        {
            "id": "225-3-4",
            "pair": [
                "Docker can also use user namespaces, which allow each container to have distinct UIDs from the host system. Your container processes think they are uid 0, but only have root-like privileges within their own domain (user namespace), while the host system sees a normal non-root uid. Each container's uid 0 is also separate from every other container's uid 0.\n",
                "I am aware that directly setting the time from inside a container is not permitted by default, i.e. when I run:\n"
            ]
        },
        {
            "id": "225-4-5",
            "pair": [
                "I am aware that directly setting the time from inside a container is not permitted by default, i.e. when I run:\n",
                "I get an error stating: date: cannot set date: Operation not permitted\n"
            ]
        },
        {
            "id": "225-5-6",
            "pair": [
                "I get an error stating: date: cannot set date: Operation not permitted\n",
                "So Actually I have sufficient permissions and should be able to run it. \n"
            ]
        },
        {
            "id": "225-6-7",
            "pair": [
                "So Actually I have sufficient permissions and should be able to run it. \n",
                "I undertstand that there is another mechanism preventing me from setting the data, and my question is how exactly is that achieved?"
            ]
        }
    ],
    [
        {
            "id": "226-1-2",
            "pair": [
                "What you refer to as \"semi-fixed\" and \"fully-fixed\" timestep are, in my mind, both fixed timestep -- the dt that you are passing to your update call doesn't change between frames.\n",
                "So, your question is actually \"do I want to implement interpolation for the purposes of rendering?\" The answer is: probably not.\n"
            ]
        },
        {
            "id": "226-2-3",
            "pair": [
                "So, your question is actually \"do I want to implement interpolation for the purposes of rendering?\" The answer is: probably not.\n",
                "Interpolation is something you would only want to do if your fixed update timestep is markedly different to your target render timestep. It's not uncommon for games that have a strenuous update phase to only call update at, say, 10Hz, but render at full framerate.\n"
            ]
        },
        {
            "id": "226-3-4",
            "pair": [
                "Interpolation is something you would only want to do if your fixed update timestep is markedly different to your target render timestep. It's not uncommon for games that have a strenuous update phase to only call update at, say, 10Hz, but render at full framerate.\n",
                "Since you're writing an iPhone shmup, neither your update nor your render need be particularly CPU intensive; you can easily lock your framerate to 30Hz or 60Hz, not bother with implementation, and still have a smooth-looking game.\n"
            ]
        },
        {
            "id": "226-4-5",
            "pair": [
                "Since you're writing an iPhone shmup, neither your update nor your render need be particularly CPU intensive; you can easily lock your framerate to 30Hz or 60Hz, not bother with implementation, and still have a smooth-looking game.\n",
                "I am making an iphone shmup and am trying to decide what type of game loop to use. I want to use either semi-fixed timestep or fully-fixed timestep.\n"
            ]
        },
        {
            "id": "226-5-6",
            "pair": [
                "I am making an iphone shmup and am trying to decide what type of game loop to use. I want to use either semi-fixed timestep or fully-fixed timestep.\n",
                "With semi-fixed timestep I will make zero or more update(FIXED_INTERVAL) calls followed by one update(dt) call where dt <= FIXED_INTERVAL per game loop. As I understand it the drawbacks with this method are that my physics update(dt) logic is going to be more difficult to program because I basically have to assume a variable dt for every update. And then I've also heard that each run of my game will be slightly different due to floating point values not being the same every time.\n"
            ]
        },
        {
            "id": "226-6-7",
            "pair": [
                "With semi-fixed timestep I will make zero or more update(FIXED_INTERVAL) calls followed by one update(dt) call where dt <= FIXED_INTERVAL per game loop. As I understand it the drawbacks with this method are that my physics update(dt) logic is going to be more difficult to program because I basically have to assume a variable dt for every update. And then I've also heard that each run of my game will be slightly different due to floating point values not being the same every time.\n",
                "Then with fully-fixed timestep I am making zero or more update(FIXED_INTERVAL) calls followed by one interpolation(dt/FIXED_INTERVAL) call where dt < FIXED_INTERVAL per game loop.\n"
            ]
        },
        {
            "id": "226-7-8",
            "pair": [
                "Then with fully-fixed timestep I am making zero or more update(FIXED_INTERVAL) calls followed by one interpolation(dt/FIXED_INTERVAL) call where dt < FIXED_INTERVAL per game loop.\n",
                "So it seems like the big decision I really have to make is: do I want to tackle the challenge of implementing an update(dt) with a variable dt or do I want to tackle the challenge of implementing interpolation?\n"
            ]
        },
        {
            "id": "226-8-9",
            "pair": [
                "So it seems like the big decision I really have to make is: do I want to tackle the challenge of implementing an update(dt) with a variable dt or do I want to tackle the challenge of implementing interpolation?\n",
                "Now from what I've read the majority of people are saying to use fully-fixed and do the interpolation. But when I think about implementing interpolation it seems like I'd be a lot more complex than an update(dt) with variable dt. This is because if I use interpolation I have to remember both the previous state and the current state. So if I want to use interpolation I have to come up with an additional layer of indirection that abstracts out entire individual game states. Whereas with semi-fixed timestep where I don't have to use interpolation I don't have to come up with a game state abstraction because there's always only one game state and it's simply the \"global arrays\" that represent my enemies, and enemy bullets etc.\n"
            ]
        },
        {
            "id": "226-9-10",
            "pair": [
                "Now from what I've read the majority of people are saying to use fully-fixed and do the interpolation. But when I think about implementing interpolation it seems like I'd be a lot more complex than an update(dt) with variable dt. This is because if I use interpolation I have to remember both the previous state and the current state. So if I want to use interpolation I have to come up with an additional layer of indirection that abstracts out entire individual game states. Whereas with semi-fixed timestep where I don't have to use interpolation I don't have to come up with a game state abstraction because there's always only one game state and it's simply the \"global arrays\" that represent my enemies, and enemy bullets etc.\n",
                "So what's the more practical choice: do I implement it semi-fixed knowing that my physics updates could get complicated with the variable dt. Or do I use fully-fixed and try to come up with a game state abstraction so that I can keep track of previous state and current state in order to perform interpolation?"
            ]
        }
    ],
    [
        {
            "id": "227-1-2",
            "pair": [
                "I prefer to use the wizard to do this sort of thing.\n",
                "It will prompt for your data-source (your SQL Server database) and your data destination (choose either MS Access or ODBC for Access.)\n"
            ]
        },
        {
            "id": "227-2-3",
            "pair": [
                "It will prompt for your data-source (your SQL Server database) and your data destination (choose either MS Access or ODBC for Access.)\n",
                "I believe you can also use a data transformation task to do this. Drag on a source connection (SQL Server) and a destination connection (Access/ODBC) then join them with a transformation task.  You won't actually do any transformations, but it still needs to know which field names match which in the source/destination table.\n"
            ]
        },
        {
            "id": "227-3-4",
            "pair": [
                "I believe you can also use a data transformation task to do this. Drag on a source connection (SQL Server) and a destination connection (Access/ODBC) then join them with a transformation task.  You won't actually do any transformations, but it still needs to know which field names match which in the source/destination table.\n",
                "Note: ODBC functionality is not available in 64-bit Windows 2003 servers, as Microsoft helpfully removed the 64-bit ODBC DLL.  It is coming back in 2008 due to overwhelming demand, apparently.\n"
            ]
        },
        {
            "id": "227-4-5",
            "pair": [
                "Note: ODBC functionality is not available in 64-bit Windows 2003 servers, as Microsoft helpfully removed the 64-bit ODBC DLL.  It is coming back in 2008 due to overwhelming demand, apparently.\n",
                "There is an a destination for Access database, as long as the Access database already exist. You would add your SQL Server 2005 and the Access database to the connection manager. Then chose the OLE DB source and destination within your data flow. It will allow you to chose the Access connection as your destination.\n"
            ]
        },
        {
            "id": "227-5-6",
            "pair": [
                "There is an a destination for Access database, as long as the Access database already exist. You would add your SQL Server 2005 and the Access database to the connection manager. Then chose the OLE DB source and destination within your data flow. It will allow you to chose the Access connection as your destination.\n",
                "Now if you are wanting to create the Access database on the fly, that is something different and would require a bit of scripting to accomplish.\n"
            ]
        },
        {
            "id": "227-6-7",
            "pair": [
                "Now if you are wanting to create the Access database on the fly, that is something different and would require a bit of scripting to accomplish.\n",
                "Your Connection Manager properties and OLE DB destination editor for your access database would look similar to this:"
            ]
        }
    ],
    [
        {
            "id": "228-1-2",
            "pair": [
                "The function handleInput(e) on line 92 does not need 4 if statements to determine if you are going the opposite direction. You can concatonate that in your switch and make a comment about it:\n",
                "You have an update function on line 123, with a forEach method. That method takes a function as parameter, and provides one argument. You have this on line 126:\n"
            ]
        },
        {
            "id": "228-2-3",
            "pair": [
                "You have an update function on line 123, with a forEach method. That method takes a function as parameter, and provides one argument. You have this on line 126:\n",
                "You pass that variable in a function and do nothing else then calling another function with the same parameter. You could replace that like this:\n"
            ]
        },
        {
            "id": "228-3-4",
            "pair": [
                "You pass that variable in a function and do nothing else then calling another function with the same parameter. You could replace that like this:\n",
                "This does not work for line 327 (functor push will throw error)\n"
            ]
        },
        {
            "id": "228-4-5",
            "pair": [
                "This does not work for line 327 (functor push will throw error)\n",
                "I like what you do with defineAppleProperties, using the while loop to determine the apple location, but it could cause issues;\n"
            ]
        },
        {
            "id": "228-5-6",
            "pair": [
                "I like what you do with defineAppleProperties, using the while loop to determine the apple location, but it could cause issues;\n",
                "If the player has a perfect game, the snake will fill the entire map if he ate the last apple. If that happens, you have an endless while loop, causing a tab/browser crash. I would check the snake length vs amount of positions on the game board to fix that.\n"
            ]
        },
        {
            "id": "228-6-7",
            "pair": [
                "If the player has a perfect game, the snake will fill the entire map if he ate the last apple. If that happens, you have an endless while loop, causing a tab/browser crash. I would check the snake length vs amount of positions on the game board to fix that.\n",
                "  let canvas = document.getElementById('gameboard');\n"
            ]
        },
        {
            "id": "228-7-8",
            "pair": [
                "  let canvas = document.getElementById('gameboard');\n",
                "  let scoreBoard = document.querySelector('.scoreboard');\n"
            ]
        },
        {
            "id": "228-8-9",
            "pair": [
                "  let scoreBoard = document.querySelector('.scoreboard');\n",
                "  let highscoreLabel = document.querySelector('.highscore');\n"
            ]
        },
        {
            "id": "228-9-10",
            "pair": [
                "  let highscoreLabel = document.querySelector('.highscore');\n",
                "      x: Math.floor( Math.random() * gameBoardWidth ),\n"
            ]
        },
        {
            "id": "228-10-11",
            "pair": [
                "      x: Math.floor( Math.random() * gameBoardWidth ),\n",
                "      y: Math.floor( Math.random() * gameBoardHeight ),\n"
            ]
        },
        {
            "id": "228-11-12",
            "pair": [
                "      y: Math.floor( Math.random() * gameBoardHeight ),\n",
                "          if( ( bodySegment.x === this.x ) && ( bodySegment.y === this.y ) ) {\n"
            ]
        },
        {
            "id": "228-12-13",
            "pair": [
                "          if( ( bodySegment.x === this.x ) && ( bodySegment.y === this.y ) ) {\n",
                "          if( (( this.x % snake.blockSize === 0 ) && ( this.y % snake.blockSize === 0 )) && this.isOutsideSnakeBody() ) {\n"
            ]
        },
        {
            "id": "228-13-14",
            "pair": [
                "          if( (( this.x % snake.blockSize === 0 ) && ( this.y % snake.blockSize === 0 )) && this.isOutsideSnakeBody() ) {\n",
                "            this.x =  Math.floor( Math.random() * gameBoardWidth );\n"
            ]
        },
        {
            "id": "228-14-15",
            "pair": [
                "            this.x =  Math.floor( Math.random() * gameBoardWidth );\n",
                "            this.y = Math.floor( Math.random() * gameBoardHeight );\n"
            ]
        },
        {
            "id": "228-15-16",
            "pair": [
                "            this.y = Math.floor( Math.random() * gameBoardHeight );\n",
                "    if(snake.direction === 'right' && event.keyCode === 37) {\n"
            ]
        },
        {
            "id": "228-16-17",
            "pair": [
                "    if(snake.direction === 'right' && event.keyCode === 37) {\n",
                "    else if(snake.direction === 'left' && event.keyCode === 39) {\n"
            ]
        },
        {
            "id": "228-17-18",
            "pair": [
                "    else if(snake.direction === 'left' && event.keyCode === 39) {\n",
                "    else if(snake.direction === 'up' && event.keyCode === 40) {\n"
            ]
        },
        {
            "id": "228-18-19",
            "pair": [
                "    else if(snake.direction === 'up' && event.keyCode === 40) {\n",
                "    else if(snake.direction === 'down' && event.keyCode === 38) {\n"
            ]
        },
        {
            "id": "228-19-20",
            "pair": [
                "    else if(snake.direction === 'down' && event.keyCode === 38) {\n",
                "    ctx.clearRect(0, 0, canvas.width, canvas.height);\n"
            ]
        },
        {
            "id": "228-20-21",
            "pair": [
                "    ctx.clearRect(0, 0, canvas.width, canvas.height);\n",
                "      ctx.fillRect( o.x, o.y, snake.blockSize, snake.blockSize );\n"
            ]
        },
        {
            "id": "228-21-22",
            "pair": [
                "      ctx.fillRect( o.x, o.y, snake.blockSize, snake.blockSize );\n",
                "  for( let y = snake.blockSize; y < 600; y += snake.blockSize ) {\n"
            ]
        },
        {
            "id": "228-22-23",
            "pair": [
                "  for( let y = snake.blockSize; y < 600; y += snake.blockSize ) {\n",
                "    for( let x = snake.blockSize; x < 800; x += snake.blockSize ) {\n"
            ]
        },
        {
            "id": "228-23-24",
            "pair": [
                "    for( let x = snake.blockSize; x < 800; x += snake.blockSize ) {\n",
                "    scoreBoard.innerHTML = 'Points: ' + player.points;\n"
            ]
        },
        {
            "id": "228-24-25",
            "pair": [
                "    scoreBoard.innerHTML = 'Points: ' + player.points;\n",
                "      let storageHighscoresItems = JSON.parse(localStorage.getItem('highscores'));\n"
            ]
        },
        {
            "id": "228-25-26",
            "pair": [
                "      let storageHighscoresItems = JSON.parse(localStorage.getItem('highscores'));\n",
                "      storageHighscoresItems.forEach( ( item ) => {\n"
            ]
        },
        {
            "id": "228-26-27",
            "pair": [
                "      storageHighscoresItems.forEach( ( item ) => {\n",
                "        storageHighscoresItemsArr.push( item.score );\n"
            ]
        },
        {
            "id": "228-27-28",
            "pair": [
                "        storageHighscoresItemsArr.push( item.score );\n",
                "      let highscore = Math.max( ...storageHighscoresItemsArr );\n"
            ]
        },
        {
            "id": "228-28-29",
            "pair": [
                "      let highscore = Math.max( ...storageHighscoresItemsArr );\n",
                "      highscoreLabel.innerHTML = 'Highscore: ' + highscore;\n"
            ]
        },
        {
            "id": "228-29-30",
            "pair": [
                "      highscoreLabel.innerHTML = 'Highscore: ' + highscore;\n",
                "      snake.body[i].x = snake.body[i - 1].lastPosX;\n"
            ]
        },
        {
            "id": "228-30-31",
            "pair": [
                "      snake.body[i].x = snake.body[i - 1].lastPosX;\n",
                "      snake.body[i].y = snake.body[i - 1].lastPosY;\n"
            ]
        },
        {
            "id": "228-31-32",
            "pair": [
                "      snake.body[i].y = snake.body[i - 1].lastPosY;\n",
                "      snake.body[i - 1].lastPosX = snake.body[i - 1].x;\n"
            ]
        },
        {
            "id": "228-32-33",
            "pair": [
                "      snake.body[i - 1].lastPosX = snake.body[i - 1].x;\n",
                "      snake.body[i - 1].lastPosY = snake.body[i - 1].y;\n"
            ]
        },
        {
            "id": "228-33-34",
            "pair": [
                "      snake.body[i - 1].lastPosY = snake.body[i - 1].y;\n",
                "    if(( snakeHead.x === apple.x ) && ( snakeHead.y === apple.y )) {\n"
            ]
        },
        {
            "id": "228-34-35",
            "pair": [
                "    if(( snakeHead.x === apple.x ) && ( snakeHead.y === apple.y )) {\n",
                "    if((snakeHead.x === snake.body[i].x) && (snakeHead.y === snake.body[i].y)) {\n"
            ]
        },
        {
            "id": "228-35-36",
            "pair": [
                "    if((snakeHead.x === snake.body[i].x) && (snakeHead.y === snake.body[i].y)) {\n",
                "    addEventListener( 'keydown', e => handleInput(e), true );\n"
            ]
        },
        {
            "id": "228-36-37",
            "pair": [
                "    addEventListener( 'keydown', e => handleInput(e), true );\n",
                "    removeEventListener( 'keydown', handleInput, true );\n"
            ]
        },
        {
            "id": "228-37-38",
            "pair": [
                "    removeEventListener( 'keydown', handleInput, true );\n",
                "  let restartBtn = document.getElementById('restart'),\n"
            ]
        },
        {
            "id": "228-38-39",
            "pair": [
                "  let restartBtn = document.getElementById('restart'),\n",
                "    gameOverScreen = document.querySelector('.gameover-screen'),\n"
            ]
        },
        {
            "id": "228-39-40",
            "pair": [
                "    gameOverScreen = document.querySelector('.gameover-screen'),\n",
                "    finalScoreLabel = document.querySelector('.final-score'),\n"
            ]
        },
        {
            "id": "228-40-41",
            "pair": [
                "    finalScoreLabel = document.querySelector('.final-score'),\n",
                "    playerNameInput = document.querySelector('.player-name'),\n"
            ]
        },
        {
            "id": "228-41-42",
            "pair": [
                "    playerNameInput = document.querySelector('.player-name'),\n",
                "    highscoreBoardBtn = document.getElementById('highscores-btn');\n"
            ]
        },
        {
            "id": "228-42-43",
            "pair": [
                "    highscoreBoardBtn = document.getElementById('highscores-btn');\n",
                "    finalScoreLabel.innerHTML = 'Your score: ' + GAME_STATE.getPlayer().points;\n"
            ]
        },
        {
            "id": "228-43-44",
            "pair": [
                "    finalScoreLabel.innerHTML = 'Your score: ' + GAME_STATE.getPlayer().points;\n",
                "    if(GAME_STATE.getPlayer().isSaved === false) {\n"
            ]
        },
        {
            "id": "228-44-45",
            "pair": [
                "    if(GAME_STATE.getPlayer().isSaved === false) {\n",
                "      GAME_STATE.getPlayer().name = playerNameInput.value;\n"
            ]
        },
        {
            "id": "228-45-46",
            "pair": [
                "      GAME_STATE.getPlayer().name = playerNameInput.value;\n",
                "        highscores = highscores.concat(JSON.parse(localStorage.getItem('highscores')));\n"
            ]
        },
        {
            "id": "228-46-47",
            "pair": [
                "        highscores = highscores.concat(JSON.parse(localStorage.getItem('highscores')));\n",
                "      highscores.push({name: GAME_STATE.getPlayer().name, score: GAME_STATE.getPlayer().points});\n"
            ]
        },
        {
            "id": "228-47-48",
            "pair": [
                "      highscores.push({name: GAME_STATE.getPlayer().name, score: GAME_STATE.getPlayer().points});\n",
                "      localStorage.setItem('highscores', JSON.stringify(highscores));\n"
            ]
        },
        {
            "id": "228-48-49",
            "pair": [
                "      localStorage.setItem('highscores', JSON.stringify(highscores));\n",
                "    restartBtn.addEventListener( 'click', restartGame, false );\n"
            ]
        },
        {
            "id": "228-49-50",
            "pair": [
                "    restartBtn.addEventListener( 'click', restartGame, false );\n",
                "    highscoreBoardBtn.addEventListener('click', showHighscores, false );\n"
            ]
        },
        {
            "id": "228-50-51",
            "pair": [
                "    highscoreBoardBtn.addEventListener('click', showHighscores, false );\n",
                "    restartBtn.removeEventListener( 'click', restartGame, false );\n"
            ]
        },
        {
            "id": "228-51-52",
            "pair": [
                "    restartBtn.removeEventListener( 'click', restartGame, false );\n",
                "    highscoreBoardBtn.removeEventListener('click', showHighscores, false );\n"
            ]
        },
        {
            "id": "228-52-53",
            "pair": [
                "    highscoreBoardBtn.removeEventListener('click', showHighscores, false );\n",
                "  let highscoresBoard = document.querySelector('.highscores-board'),\n"
            ]
        },
        {
            "id": "228-53-54",
            "pair": [
                "  let highscoresBoard = document.querySelector('.highscores-board'),\n",
                "    highscoresList = document.querySelector('.highscores-list'),\n"
            ]
        },
        {
            "id": "228-54-55",
            "pair": [
                "    highscoresList = document.querySelector('.highscores-list'),\n",
                "    highscoresReturnBtn = document.getElementById('return-from-highscores');\n"
            ]
        },
        {
            "id": "228-55-56",
            "pair": [
                "    highscoresReturnBtn = document.getElementById('return-from-highscores');\n",
                "    let storageHighscoresItems = JSON.parse(localStorage.getItem('highscores'));\n"
            ]
        },
        {
            "id": "228-56-57",
            "pair": [
                "    let storageHighscoresItems = JSON.parse(localStorage.getItem('highscores'));\n",
                "    storageHighscoresItemsArr.sort(function (a, b) {\n"
            ]
        },
        {
            "id": "228-57-58",
            "pair": [
                "    storageHighscoresItemsArr.sort(function (a, b) {\n",
                "            let li = document.createElement('li');\n"
            ]
        },
        {
            "id": "228-58-59",
            "pair": [
                "            let li = document.createElement('li');\n",
                "            li.innerHTML = (storageHighscoresItemsArr[i].name || 'Noname') +\n"
            ]
        },
        {
            "id": "228-59-60",
            "pair": [
                "            li.innerHTML = (storageHighscoresItemsArr[i].name || 'Noname') +\n",
                "             ' - ' + storageHighscoresItemsArr[i].score;\n"
            ]
        },
        {
            "id": "228-60-61",
            "pair": [
                "             ' - ' + storageHighscoresItemsArr[i].score;\n",
                "    highscoresReturnBtn.addEventListener( 'click', goBack, false );\n"
            ]
        },
        {
            "id": "228-61-62",
            "pair": [
                "    highscoresReturnBtn.addEventListener( 'click', goBack, false );\n",
                "    highscoresReturnBtn.removeEventListener( 'click', goBack, false );\n"
            ]
        },
        {
            "id": "228-62-63",
            "pair": [
                "    highscoresReturnBtn.removeEventListener( 'click', goBack, false );\n",
                "        <li><p class=\"final-score\">Your score: </p></li>\n"
            ]
        },
        {
            "id": "228-63-64",
            "pair": [
                "        <li><p class=\"final-score\">Your score: </p></li>\n",
                "        <input class=\"player-name\" type=\"text\" maxlength=\"6\"></li>\n"
            ]
        },
        {
            "id": "228-64-65",
            "pair": [
                "        <input class=\"player-name\" type=\"text\" maxlength=\"6\"></li>\n",
                "        <li><button class=\"btn reset-btn\" id=\"restart\">Restart</button></li>\n"
            ]
        },
        {
            "id": "228-65-66",
            "pair": [
                "        <li><button class=\"btn reset-btn\" id=\"restart\">Restart</button></li>\n",
                "        <li><button class=\"btn highscores-btn\" id=\"highscores-btn\">Highscores</button></li>\n"
            ]
        },
        {
            "id": "228-66-67",
            "pair": [
                "        <li><button class=\"btn highscores-btn\" id=\"highscores-btn\">Highscores</button></li>\n",
                "      <button class=\"btn return-btn\" id=\"return-from-highscores\">Back</button>\n"
            ]
        },
        {
            "id": "228-67-68",
            "pair": [
                "      <button class=\"btn return-btn\" id=\"return-from-highscores\">Back</button>\n",
                "    <canvas id=\"gameboard\" class=\"board\"></canvas>\n"
            ]
        },
        {
            "id": "228-68-69",
            "pair": [
                "    <canvas id=\"gameboard\" class=\"board\"></canvas>\n",
                "I've decided to take your advice and improve the style of my code.I would like to know what do you think of it now,and maybe what's more needs to be changed.Though I'm not sure if I refactored it correctly because now my JS script has 100 more lines of code.\n"
            ]
        },
        {
            "id": "228-69-70",
            "pair": [
                "I've decided to take your advice and improve the style of my code.I would like to know what do you think of it now,and maybe what's more needs to be changed.Though I'm not sure if I refactored it correctly because now my JS script has 100 more lines of code.\n",
                "1.GAME_STATE - responible for everything while player is not dead.Updates data,draws on the canvas\n"
            ]
        },
        {
            "id": "228-70-71",
            "pair": [
                "1.GAME_STATE - responible for everything while player is not dead.Updates data,draws on the canvas\n",
                "2.GAMEOVER_STATE - updates localStorage and handles user input on the Game Over screen\n"
            ]
        },
        {
            "id": "228-71-72",
            "pair": [
                "2.GAMEOVER_STATE - updates localStorage and handles user input on the Game Over screen\n",
                "4.GAME - initialazes and updates a modules,depending on it's 'state' varaible"
            ]
        }
    ],
    [
        {
            "id": "229-1-2",
            "pair": [
                "Only secondary IP addresses can be moved around without terminating the instance.\n",
                "The Network Interfaces option in the console will also greet you with a greyed-out \"detach\" button for primary interfaces.\n"
            ]
        },
        {
            "id": "229-2-3",
            "pair": [
                "The Network Interfaces option in the console will also greet you with a greyed-out \"detach\" button for primary interfaces.\n",
                "I have a Windows instance (m1.medium) running in a VPC in AWS with one Network Interface (eth0). \n"
            ]
        },
        {
            "id": "229-3-4",
            "pair": [
                "I have a Windows instance (m1.medium) running in a VPC in AWS with one Network Interface (eth0). \n",
                "I am trying to see if there is a way (or a workaround?) to migrate the private IP (IE: 10.80.78.0/24 subnet)from this instance to another instance? \n"
            ]
        },
        {
            "id": "229-4-5",
            "pair": [
                "I am trying to see if there is a way (or a workaround?) to migrate the private IP (IE: 10.80.78.0/24 subnet)from this instance to another instance? \n",
                "I know with Elastic Network Interfaces (eth1) you can hotswap the ENI as needed between different nodes. \n"
            ]
        },
        {
            "id": "229-5-6",
            "pair": [
                "I know with Elastic Network Interfaces (eth1) you can hotswap the ENI as needed between different nodes. \n",
                "I have a bad feeling that this instance will need to be terminated, to allow us to migrate the IP to another node. \n"
            ]
        },
        {
            "id": "229-6-7",
            "pair": [
                "I have a bad feeling that this instance will need to be terminated, to allow us to migrate the IP to another node. \n",
                "Before doing so though, I was wondering if anyone has ever dealt with this before-and maybe has a solution? Right now all services for a particular application are pointing to the above Private IP, so launching a new instance in the same subnet and updating our code to reflect the new IP (in Production) would be to hasty (at this juncture).."
            ]
        }
    ],
    [
        {
            "id": "23-1-2",
            "pair": [
                "This is a very common problem with TMG, not be design, but by the implementation you decided to use, you need to make sure that you understand the HTTPS inspection before you actually enable it. \n",
                "I found out that SecureNAT clients does not work with HTTPS inspection, you will either need to enable the WebProxy clients or install the TMG Client on your client computers to make it work, still some sites will break and stop working after doing so since the inspection will replace the original certificate for the sites your clients are visiting with another one from the TMG server it self. \n"
            ]
        },
        {
            "id": "23-2-3",
            "pair": [
                "I found out that SecureNAT clients does not work with HTTPS inspection, you will either need to enable the WebProxy clients or install the TMG Client on your client computers to make it work, still some sites will break and stop working after doing so since the inspection will replace the original certificate for the sites your clients are visiting with another one from the TMG server it self. \n",
                "For more information, take a read here: http://blogs.technet.com/b/isablog/archive/2009/10/19/common-problems-while-implementing-https-inspection-on-forefront-tmg-2010-rc.aspx \n"
            ]
        },
        {
            "id": "23-3-4",
            "pair": [
                "For more information, take a read here: http://blogs.technet.com/b/isablog/archive/2009/10/19/common-problems-while-implementing-https-inspection-on-forefront-tmg-2010-rc.aspx \n",
                "Forefront TMG is Stop Internet after few min after enabling HTTPS Inspection,\n"
            ]
        },
        {
            "id": "23-4-5",
            "pair": [
                "Forefront TMG is Stop Internet after few min after enabling HTTPS Inspection,\n",
                "specialty I am enable to block other https websites but gmail is not blocking\n"
            ]
        },
        {
            "id": "23-5-6",
            "pair": [
                "specialty I am enable to block other https websites but gmail is not blocking\n",
                "till i enable https inspection & if i enable https inspection internet will stop\n"
            ]
        },
        {
            "id": "23-6-7",
            "pair": [
                "till i enable https inspection & if i enable https inspection internet will stop\n",
                "working after few min, FFTMG 2010 server not hung, its still response but there is no internet\n"
            ]
        },
        {
            "id": "23-7-8",
            "pair": [
                "working after few min, FFTMG 2010 server not hung, its still response but there is no internet\n",
                "if i disable https inspection then internet will work fine & services running till couple of days without any issue but gmail blocking in deny rule but still opening."
            ]
        }
    ],
    [
        {
            "id": "230-1-2",
            "pair": [
                "I have a laptop with Windows 7. Sometime ago, I tried to install Ubuntu using the cd and clicking the option \"Install with Windows\", however it never worked, never finished the installation. Now I am trying to install it again. I know how to make partitions and format them using Linux Command Line, but only with \"empty\" drives. Now, I have lots of information in my HD. When I insert the CD, Ubuntu displays the message \"Multiple Operating Systems detected\" which leads to two options: \"Erase everything and install Ubuntu\" or \"Use the partition manager\", when I see the manager I only see two partitions, one is very small (sda1 like 512 Mb) and the big one (sd2 = 318 Gb). As far as I know, I would need to erase Ubuntu's bootloader. My question is, How can I install Ubuntu using \"Install with Windows\" options instead of manually making partitions?\n",
                "Best: Use Ubuntu in virtual box. What is your computer specifications? In the current scenario, you might risk loosing data so before you do anything, please make a backup of your drives. I'd suggest you provide us with screen shots so we can look at the problem in more detail. Thanks"
            ]
        }
    ],
    [
        {
            "id": "231-1-2",
            "pair": [
                "This will also fix a bug you may not have noticed.  A derived table returns an unordered set of rows.  That is, the optimizer is permitted to ignore the ORDER BY in the formulation you have.  This gives you not the first 10 rows, but some arbitrary 10 rows.\n",
                "FOUND_ROWS is passe.  Have you noticed that search engines first moved to \"about 1,000,000 hits\", then got rid of the count?  Do you really need the count?\n"
            ]
        },
        {
            "id": "231-2-3",
            "pair": [
                "FOUND_ROWS is passe.  Have you noticed that search engines first moved to \"about 1,000,000 hits\", then got rid of the count?  Do you really need the count?\n",
                "What indexes do you have on chitentrygroup?  In particular, this one may be beneficial:\n"
            ]
        },
        {
            "id": "231-3-4",
            "pair": [
                "What indexes do you have on chitentrygroup?  In particular, this one may be beneficial:\n",
                "Are you using InnoDB?  What is the value of innodb_buffer_pool_size.  For a tiny 2GB machine (or VM), it should be no more than 400M.\n"
            ]
        },
        {
            "id": "231-4-5",
            "pair": [
                "Are you using InnoDB?  What is the value of innodb_buffer_pool_size.  For a tiny 2GB machine (or VM), it should be no more than 400M.\n",
                "What version of MySQL are your running?  Old versions defaulted that to 8M or 128M.  These are too small.\n"
            ]
        },
        {
            "id": "231-5-6",
            "pair": [
                "What version of MySQL are your running?  Old versions defaulted that to 8M or 128M.  These are too small.\n",
                "What are the mappings?  Is there one employee per chitentrygroupid?  That is \"1:many\".  What about account and chitentry?  If these are each 1:many, you don't need to GROUP BY -- which is the real villain.\n"
            ]
        },
        {
            "id": "231-6-7",
            "pair": [
                "What are the mappings?  Is there one employee per chitentrygroupid?  That is \"1:many\".  What about account and chitentry?  If these are each 1:many, you don't need to GROUP BY -- which is the real villain.\n",
                "That is, if you can SELECT the chitentrygroupids, get through the ORDER BY and LIMIT without needing the ORDER BY, you avoid the \"inflate-deflate\" that happens with JOIN + GROUP BY.\n"
            ]
        },
        {
            "id": "231-7-8",
            "pair": [
                "That is, if you can SELECT the chitentrygroupids, get through the ORDER BY and LIMIT without needing the ORDER BY, you avoid the \"inflate-deflate\" that happens with JOIN + GROUP BY.\n",
                "This method takes a little longer SQL_CALC_FOUND_ROWS; have you tried using count()\n"
            ]
        },
        {
            "id": "231-8-9",
            "pair": [
                "This method takes a little longer SQL_CALC_FOUND_ROWS; have you tried using count()\n",
                "I find it cool to have the response time using some methods\n"
            ]
        },
        {
            "id": "231-9-10",
            "pair": [
                "I find it cool to have the response time using some methods\n",
                "http://phpdevblog.niknovo.com/2009/06/mysql-pagination-sql-calc-found-rows-vs-count-query.html"
            ]
        }
    ],
    [
        {
            "id": "232-1-2",
            "pair": [
                "I was in the same boat and extensively researched various Bluetooth devices for a TRRS-compatible solution.  Finally I found this unit, the Plantronics Pulsar 260 http://www.amazon.com/Plantronics-UPP260-Pulsar-260-Stereo/dp/B000N8P4R8\n",
                "While it doesn't say TRRS anywhere, if you look at the way it works, it utilizes a 2.5mm earbud/inline mic combo, rather than the usual 3.5mm audio-only output w/ an internal mic in the hardware.\n"
            ]
        },
        {
            "id": "232-2-3",
            "pair": [
                "While it doesn't say TRRS anywhere, if you look at the way it works, it utilizes a 2.5mm earbud/inline mic combo, rather than the usual 3.5mm audio-only output w/ an internal mic in the hardware.\n",
                "I ordered this + a 2.5mm to 3.5mm adapter, and it works great with any pair of 3.5mm earbud/mic hardware.   Just ensure that the adapter you order has a 4-conductor plug - if you end up with a standard 3-conductor plug you'll lose the mic signal.\n"
            ]
        },
        {
            "id": "232-3-4",
            "pair": [
                "I ordered this + a 2.5mm to 3.5mm adapter, and it works great with any pair of 3.5mm earbud/mic hardware.   Just ensure that the adapter you order has a 4-conductor plug - if you end up with a standard 3-conductor plug you'll lose the mic signal.\n",
                "Working great for both music listening and phone calls on an iPhone 5S.  As is traditionally the case with Bluetooth, music listening is not super crystal clear 100% of the time - but it's stereo and works well.\n"
            ]
        },
        {
            "id": "232-4-5",
            "pair": [
                "Working great for both music listening and phone calls on an iPhone 5S.  As is traditionally the case with Bluetooth, music listening is not super crystal clear 100% of the time - but it's stereo and works well.\n",
                "Ladies and Gentlemen, after much frustrating searching just like all of you, I believe I came across a forum angel that gave me a lead where I found the perfect solution.\n"
            ]
        },
        {
            "id": "232-5-6",
            "pair": [
                "Ladies and Gentlemen, after much frustrating searching just like all of you, I believe I came across a forum angel that gave me a lead where I found the perfect solution.\n",
                "https://www.hq98.com/prymeblu-bth-300-wireless-mic-bluetooth/\n"
            ]
        },
        {
            "id": "232-6-7",
            "pair": [
                "https://www.hq98.com/prymeblu-bth-300-wireless-mic-bluetooth/\n",
                "If you go to the Pryme website and open the manual, it states that it will even work with apple-configured TRRS jacks!  I purchased it right away and should be here in a couple days. What's more, it works as an amplifier and does transmit stereo and not mono.  This is the device we've been craving that TRULY turns any headset into a bluetooth one!\n"
            ]
        },
        {
            "id": "232-7-8",
            "pair": [
                "If you go to the Pryme website and open the manual, it states that it will even work with apple-configured TRRS jacks!  I purchased it right away and should be here in a couple days. What's more, it works as an amplifier and does transmit stereo and not mono.  This is the device we've been craving that TRULY turns any headset into a bluetooth one!\n",
                "I will try to come back and report my test results, but I feel like a kid at Christmas right now!"
            ]
        }
    ],
    [
        {
            "id": "233-1-2",
            "pair": [
                "There is a command called \"InsertSectionBreak\" under \"Commands Not in the Ribbon\" category in the \"Customize Keyboard\" window, but it doesn't seem to support evey section break type. However it does seem like it creates a \"Section Break (Next Page).\"\n",
                "This is the only customizable keyboard shortcut I've been able to find so far. Hope this helps.\n"
            ]
        },
        {
            "id": "233-2-3",
            "pair": [
                "This is the only customizable keyboard shortcut I've been able to find so far. Hope this helps.\n",
                "PS. Microsoft reaaaly needs to tidy up things in Office keyboard shortcuts customization window.\n"
            ]
        },
        {
            "id": "233-3-4",
            "pair": [
                "PS. Microsoft reaaaly needs to tidy up things in Office keyboard shortcuts customization window.\n",
                "I use the Continuous Section Break regularly because it allows me to break inserted excel tables into smaller pieces for editing. Deleting the section break joins them back up and the columns don't have to match.\n"
            ]
        },
        {
            "id": "233-4-5",
            "pair": [
                "I use the Continuous Section Break regularly because it allows me to break inserted excel tables into smaller pieces for editing. Deleting the section break joins them back up and the columns don't have to match.\n",
                "The easiest way to quickly access one of the Section Breaks not listed under 'All Commands' is to record a macro and assign a keyboard shortcut.\n"
            ]
        },
        {
            "id": "233-5-6",
            "pair": [
                "The easiest way to quickly access one of the Section Breaks not listed under 'All Commands' is to record a macro and assign a keyboard shortcut.\n",
                "Under the Developer tab choose Record Macro.  You'll have to name the macro and\n"
            ]
        },
        {
            "id": "233-6-7",
            "pair": [
                "Under the Developer tab choose Record Macro.  You'll have to name the macro and\n",
                "Go to layout tab, click the Break drop-down and choose your favorite;\n"
            ]
        },
        {
            "id": "233-7-8",
            "pair": [
                "Go to layout tab, click the Break drop-down and choose your favorite;\n",
                "That's it.  Alt B (or whatever you assigned) is now your preferred break shortcut."
            ]
        }
    ],
    [
        {
            "id": "234-1-2",
            "pair": [
                "You don't need an X-server for this. You need the X-server up and running on the machine where you tunnel to. You even don't need to tunnel anything unless you are behind NAT or some tricky firewall:\n",
                "export DISPLAY=<IP of machine where you are sitting at:0.0\n"
            ]
        },
        {
            "id": "234-2-3",
            "pair": [
                "export DISPLAY=<IP of machine where you are sitting at:0.0\n",
                "I'm having quite a bit of difficulty installing X server on a VM running Debian Wheezy.  I basically am just looking to be able to SSH to the box using X11 forwarding.\n"
            ]
        },
        {
            "id": "234-3-4",
            "pair": [
                "I'm having quite a bit of difficulty installing X server on a VM running Debian Wheezy.  I basically am just looking to be able to SSH to the box using X11 forwarding.\n",
                "After installing xserver through the apt packaging system, (apt-get install xorg xserver-xorg) I'm unable to actually run the server:\n"
            ]
        },
        {
            "id": "234-4-5",
            "pair": [
                "After installing xserver through the apt packaging system, (apt-get install xorg xserver-xorg) I'm unable to actually run the server:\n",
                "Googling around a bit, there seems to be a consensus that this may have something to do with the video drivers.  \n"
            ]
        },
        {
            "id": "234-5-6",
            "pair": [
                "Googling around a bit, there seems to be a consensus that this may have something to do with the video drivers.  \n",
                "This seems to be validated somewhat by checking the log, as suggested by the above error message.  The relevant portion of the log reads: \n"
            ]
        },
        {
            "id": "234-6-7",
            "pair": [
                "This seems to be validated somewhat by checking the log, as suggested by the above error message.  The relevant portion of the log reads: \n",
                "So it seems that while looking for a driver, startx attempts to open the device /dev/fb0 (not sure what that is - a framebuffer?), fails, and aborts.\n"
            ]
        },
        {
            "id": "234-7-8",
            "pair": [
                "So it seems that while looking for a driver, startx attempts to open the device /dev/fb0 (not sure what that is - a framebuffer?), fails, and aborts.\n",
                "So apparently I may need to edit the video driver settings in /etc/X11/xorg.conf.  Except, my system doesn't have /etc/X11/xorg.conf, or an xorg.conf file anywhere for that matter.  So I've basically hit a dead-end here."
            ]
        }
    ],
    [
        {
            "id": "235-1-2",
            "pair": [
                "If there are no options in the BIOS relating to PS/2, the only thing I can advise is to check that the PS/2 socket on the pc is fully working.\n",
                "If the touch-screen interface is USB or removable, you can try unplugging it and testing the mouse again (may be it gets disabled if something else is detected - but unlikely).\n"
            ]
        },
        {
            "id": "235-2-3",
            "pair": [
                "If the touch-screen interface is USB or removable, you can try unplugging it and testing the mouse again (may be it gets disabled if something else is detected - but unlikely).\n",
                "The mouse itself is likely 64-bit compatible. It's generally not a matter of OS bitness. Windows 7, x86 and x64, has PS/2 compatibility.\n"
            ]
        },
        {
            "id": "235-3-4",
            "pair": [
                "The mouse itself is likely 64-bit compatible. It's generally not a matter of OS bitness. Windows 7, x86 and x64, has PS/2 compatibility.\n",
                "If it's a complicated mouse, it could be driver-related, but for a simple 2/3-button mouse, you shouldn't have a problem. Does the mouse come with drivers?\n"
            ]
        },
        {
            "id": "235-4-5",
            "pair": [
                "If it's a complicated mouse, it could be driver-related, but for a simple 2/3-button mouse, you shouldn't have a problem. Does the mouse come with drivers?\n",
                "It's far more likely the motherboard, BIOS, or PS/2 mouse plug is the problem.\n"
            ]
        },
        {
            "id": "235-5-6",
            "pair": [
                "It's far more likely the motherboard, BIOS, or PS/2 mouse plug is the problem.\n",
                "The motherboard may not be designed to handle PS/2 mice. If it has a PS/2 plug, you can generally assume it should, but... ya never know. ;)\n"
            ]
        },
        {
            "id": "235-6-7",
            "pair": [
                "The motherboard may not be designed to handle PS/2 mice. If it has a PS/2 plug, you can generally assume it should, but... ya never know. ;)\n",
                "I haven't seen a BIOS setting to enable/disable PS/2 devices, they tend to be something that's just on if it's there and are a very low-level interface (hence the support in older systems). You may look for settings that specify where the BIOS enables/loads input methods, there may be something there (mine has a PS/2 or USB-preferred setting, I think).\n"
            ]
        },
        {
            "id": "235-7-8",
            "pair": [
                "I haven't seen a BIOS setting to enable/disable PS/2 devices, they tend to be something that's just on if it's there and are a very low-level interface (hence the support in older systems). You may look for settings that specify where the BIOS enables/loads input methods, there may be something there (mine has a PS/2 or USB-preferred setting, I think).\n",
                "It's also possible the PS/2 plug is shot. It's rarer on modern mobos, but older ones were finicky about hot-swapping PS/2 devices.\n"
            ]
        },
        {
            "id": "235-8-9",
            "pair": [
                "It's also possible the PS/2 plug is shot. It's rarer on modern mobos, but older ones were finicky about hot-swapping PS/2 devices.\n",
                "Test the plug with a different PS/2 mouse, test the mouse on a different system, and check the manufacturer's websites (for the motherboard/laptop and mouse) for any incompatibility reports or (better yet) patches."
            ]
        }
    ],
    [
        {
            "id": "236-1-2",
            "pair": [
                "Break NTFS inheritance on the toplevel folder. I'll call it root in this answer. Then grant Authenticated Users - Read to This Folder Only in the advanced view. You might also want to add Administrators - Full Control to This Folder, subfolders, and files so that administrators of the server can still traverse the entire directory tree.\n",
                "Then, make your two subfolders: folder1 and folder2 in this example. These folders should inherit the Administrators - Full Control ACE, but not the Authenticated Users - Read one. Now, you can just add the groups and grant permissions as needed on the subfolders.\n"
            ]
        },
        {
            "id": "236-2-3",
            "pair": [
                "Then, make your two subfolders: folder1 and folder2 in this example. These folders should inherit the Administrators - Full Control ACE, but not the Authenticated Users - Read one. Now, you can just add the groups and grant permissions as needed on the subfolders.\n",
                "Just as a side note, you almost never want to give Full Control to users, you want to give Modify. This will let them delete, create, rename, etc but giving them Full Control instead allows them to change permissions as well.\n"
            ]
        },
        {
            "id": "236-3-4",
            "pair": [
                "Just as a side note, you almost never want to give Full Control to users, you want to give Modify. This will let them delete, create, rename, etc but giving them Full Control instead allows them to change permissions as well.\n",
                "I am an AD newbie and have been tasked with this situation:\n"
            ]
        },
        {
            "id": "236-4-5",
            "pair": [
                "I am an AD newbie and have been tasked with this situation:\n",
                "Win 2008 r2 environment, create security groups in AD where root folder has only read access for all; sub folder 1 has read for 1 group public and full for another group; sub folder 2 has no public access and full access for another - no group can change root folders names."
            ]
        }
    ],
    [
        {
            "id": "237-1-2",
            "pair": [
                "Even with innodb_file_per_table setting, the ibdata* still stores critical (meta)data about your tables, you should not have destroyed it. I'm affraid your data are lost, except if there is a way to extract them from the idb files and restore them after the ibdata* file has been rebuilt.\n",
                "I was having innodb with one ibdata file. i have changed it in my.cnf to have ibd file for every table (innodb_file_per_table). then i run the following query on all of my innodb tables to have its own ibd file\n"
            ]
        },
        {
            "id": "237-2-3",
            "pair": [
                "I was having innodb with one ibdata file. i have changed it in my.cnf to have ibd file for every table (innodb_file_per_table). then i run the following query on all of my innodb tables to have its own ibd file\n",
                "now after converting all of my tables, ibdata still having the same size, so i have deleted it, and restarted mysql. mysql has created it again with 10MB size (as defined in my.cnf)\n"
            ]
        },
        {
            "id": "237-3-4",
            "pair": [
                "now after converting all of my tables, ibdata still having the same size, so i have deleted it, and restarted mysql. mysql has created it again with 10MB size (as defined in my.cnf)\n",
                "but the problem now is that i can see all of my tables normally when  show tables;\n"
            ]
        },
        {
            "id": "237-4-5",
            "pair": [
                "but the problem now is that i can see all of my tables normally when  show tables;\n",
                "but whenever i want to desc tablename or select * from tablename i am getting this error message\n"
            ]
        },
        {
            "id": "237-5-6",
            "pair": [
                "but whenever i want to desc tablename or select * from tablename i am getting this error message\n",
                "and in show engines i can't see innodb in the list!!\n"
            ]
        },
        {
            "id": "237-6-7",
            "pair": [
                "and in show engines i can't see innodb in the list!!\n",
                "also i tried to delete the ib_logfile0 and ib_logfile1, i got another message\n"
            ]
        },
        {
            "id": "237-7-8",
            "pair": [
                "also i tried to delete the ib_logfile0 and ib_logfile1, i got another message\n",
                "I know that i was supposed to mysqldump and restore, but this is what i have done :( , anybody has an idea of how to delete the ibdata1 file and keep innodb engine enabled?"
            ]
        }
    ],
    [
        {
            "id": "238-1-2",
            "pair": [
                "There have been many changes in file sharing (SMB) from XP to Vista/7, one of them being on the authentication method.\n",
                "You could try making the Win7 machines go back to using NTLMv1 (Google \"windows 7 NTLMv1\") but this is a bad idea.\n"
            ]
        },
        {
            "id": "238-2-3",
            "pair": [
                "You could try making the Win7 machines go back to using NTLMv1 (Google \"windows 7 NTLMv1\") but this is a bad idea.\n",
                "You have an environment that is outdated beyond imagination. I would not even think about fixing ANYTHING in it, sincerely.\n"
            ]
        },
        {
            "id": "238-3-4",
            "pair": [
                "You have an environment that is outdated beyond imagination. I would not even think about fixing ANYTHING in it, sincerely.\n",
                "To get you started on the Right Path (TM), start studying the implementation of CentOS 7 to replace both Fedora/RedHat, preferibly on a new machine. Transferring the old files and setting up file sharing will be a piece of cake (relatively speaking).\n"
            ]
        },
        {
            "id": "238-4-5",
            "pair": [
                "To get you started on the Right Path (TM), start studying the implementation of CentOS 7 to replace both Fedora/RedHat, preferibly on a new machine. Transferring the old files and setting up file sharing will be a piece of cake (relatively speaking).\n",
                "That being said, I'm not familiar with jBase and it looks like legacy software. Contact the vendor to check if they support a newer Linux distribution. If not, I would consider migrating from it (and hiring a dedicated sysadmin or contracting with a service provider to maintain your servers).\n"
            ]
        },
        {
            "id": "238-5-6",
            "pair": [
                "That being said, I'm not familiar with jBase and it looks like legacy software. Contact the vendor to check if they support a newer Linux distribution. If not, I would consider migrating from it (and hiring a dedicated sysadmin or contracting with a service provider to maintain your servers).\n",
                "Go to: Control Panel -> Administrative Tools -> Local Security Policy\n"
            ]
        },
        {
            "id": "238-6-7",
            "pair": [
                "Go to: Control Panel -> Administrative Tools -> Local Security Policy\n",
                "\"Network security: LAN Manager authentication level\" -> Send LM & NTLM responses\n"
            ]
        },
        {
            "id": "238-7-8",
            "pair": [
                "\"Network security: LAN Manager authentication level\" -> Send LM & NTLM responses\n",
                "\"Minimum session security for NTLM SSP\" -> uncheck: Require 128-bit encryption  \n"
            ]
        },
        {
            "id": "238-8-9",
            "pair": [
                "\"Minimum session security for NTLM SSP\" -> uncheck: Require 128-bit encryption  \n",
                "source: http://www.enterprisenetworkingplanet.com/windows/article.php/3849061/Use-Samba-With-Windows-7-Clients.htm\n"
            ]
        },
        {
            "id": "238-9-10",
            "pair": [
                "source: http://www.enterprisenetworkingplanet.com/windows/article.php/3849061/Use-Samba-With-Windows-7-Clients.htm\n",
                "which I found from this post that may also be helpful for you, https://www.kubuntuforums.net/archive/index.php/t-43698.html"
            ]
        }
    ],
    [
        {
            "id": "239-1-2",
            "pair": [
                "Recently, I've connected two devices using a 20 meter long Ethernet cable. This solved a huge problem with my Wi-Fi (as I've reduced the distance from 20+ to 4 meters and also avoided some obstacles to the signal), but another problem emerged. My connection to the Internet looks like\n",
                "My connection to device1 works always well (<1% packets lost), but my connection to device2 makes strange things: sometimes it works well too, sometimes 50% packets get lost, sometimes nearly all of them. The behavior changes very rapidly, two measurements in row may give totally different results, as if there were some strong jamming. The result from running\n"
            ]
        },
        {
            "id": "239-2-3",
            "pair": [
                "My connection to device1 works always well (<1% packets lost), but my connection to device2 makes strange things: sometimes it works well too, sometimes 50% packets get lost, sometimes nearly all of them. The behavior changes very rapidly, two measurements in row may give totally different results, as if there were some strong jamming. The result from running\n",
                "Resetting of the devices doesn't really help (it does in case it malfunctioned completely, which happens from time to time). AFAIK, the maximum Ethernet cable length is 100 meter, so this should be no problem. Moreover, I'd expect some signal deterioration leading to consistently lowered transmission rate or maybe a complete failure, but not this.\n"
            ]
        },
        {
            "id": "239-3-4",
            "pair": [
                "Resetting of the devices doesn't really help (it does in case it malfunctioned completely, which happens from time to time). AFAIK, the maximum Ethernet cable length is 100 meter, so this should be no problem. Moreover, I'd expect some signal deterioration leading to consistently lowered transmission rate or maybe a complete failure, but not this.\n",
                "There was water coming through the cables, which would have been easy to find out if I had had physical access to it. The water destroyed the connector, end of the story. I'm sorry for the noise.\n"
            ]
        },
        {
            "id": "239-4-5",
            "pair": [
                "There was water coming through the cables, which would have been easy to find out if I had had physical access to it. The water destroyed the connector, end of the story. I'm sorry for the noise.\n",
                "Something is wrong with your set up.  I have something similar set up at home and have zero packet loss.  Have you checked that your hardware is at the latest firmware and drivers?"
            ]
        }
    ],
    [
        {
            "id": "24-1-2",
            "pair": [
                "I don't know FC11 all that well, but you may not be able to do an online shrink of the root partition.  If the partition on the logical volume isn't root, boot into single user, and shrink it from there.  If it is /, you need to boot from a rescue disk.  Lots of good ones around, but get one with GParted!  Ubuntu Live CD fits the bill, as does SystemRescueDisk: http://www.sysresccd.org/Main_Page.\n",
                "Once you've done that, you need to first shrink the space in use by LVM, and then re-write the parition table to expand /boot. \n"
            ]
        },
        {
            "id": "24-2-3",
            "pair": [
                "Once you've done that, you need to first shrink the space in use by LVM, and then re-write the parition table to expand /boot. \n",
                "Once you've got things ready to shrink the LV, first shrink the filesystem on the partition:\n"
            ]
        },
        {
            "id": "24-3-4",
            "pair": [
                "Once you've got things ready to shrink the LV, first shrink the filesystem on the partition:\n",
                "where SLICE is the partition, and new_size is the total new size you want once shrunk.  You can use 'M' or 'G' to specify the size.  \n"
            ]
        },
        {
            "id": "24-4-5",
            "pair": [
                "where SLICE is the partition, and new_size is the total new size you want once shrunk.  You can use 'M' or 'G' to specify the size.  \n",
                "For example, if I wanted to shrink my root parition down to 100G, I would do this:\n"
            ]
        },
        {
            "id": "24-5-6",
            "pair": [
                "For example, if I wanted to shrink my root parition down to 100G, I would do this:\n",
                "Now shrink the logical volume that holds the partition:\n"
            ]
        },
        {
            "id": "24-6-7",
            "pair": [
                "Now shrink the logical volume that holds the partition:\n",
                "Now shrink the physical volume (actually, the amount of the pv availble to LVM)\n"
            ]
        },
        {
            "id": "24-7-8",
            "pair": [
                "Now shrink the physical volume (actually, the amount of the pv availble to LVM)\n",
                "If you're not sure of the physical volume name, use pvs.\n"
            ]
        },
        {
            "id": "24-8-9",
            "pair": [
                "If you're not sure of the physical volume name, use pvs.\n",
                "Now you need to actually change the partition table on the drive, shinking /dev/sda2 to that same 100G, and put the rest into whatever partition /boot is on.  Use GParted for this -- it's a million times easier than fdisk.  It's a GUI program, so can't show you an example, but it's pretty easy to use.\n"
            ]
        },
        {
            "id": "24-9-10",
            "pair": [
                "Now you need to actually change the partition table on the drive, shinking /dev/sda2 to that same 100G, and put the rest into whatever partition /boot is on.  Use GParted for this -- it's a million times easier than fdisk.  It's a GUI program, so can't show you an example, but it's pretty easy to use.\n",
                "Shrink the filesystem (resize2fs), the logical volume (lvreduce), the physical volume (pvresize), then the partition."
            ]
        }
    ],
    [
        {
            "id": "240-1-2",
            "pair": [
                "When I add a user to Outlook delegation, using this method\n",
                "However when I set the user as a delegate, I get the following error:\n"
            ]
        },
        {
            "id": "240-2-3",
            "pair": [
                "However when I set the user as a delegate, I get the following error:\n",
                "The powershell cmdlet you're using there (Set-CalendarProcessing) is only for 'Resource' mailboxes. This refers to objects like rooms, company cars, projectors, and other things that your organization use by booking time on a calendar. \n"
            ]
        },
        {
            "id": "240-3-4",
            "pair": [
                "The powershell cmdlet you're using there (Set-CalendarProcessing) is only for 'Resource' mailboxes. This refers to objects like rooms, company cars, projectors, and other things that your organization use by booking time on a calendar. \n",
                "This particular Exchange object type has a special calendar assistant available that can either automatically accept or reject calendar requests that are sent to it, or you can set a delegate. The naming here is misleading, since delegate is used elsewhere in the Exchange world to mean something else. \n"
            ]
        },
        {
            "id": "240-4-5",
            "pair": [
                "This particular Exchange object type has a special calendar assistant available that can either automatically accept or reject calendar requests that are sent to it, or you can set a delegate. The naming here is misleading, since delegate is used elsewhere in the Exchange world to mean something else. \n",
                "The error you recieved is indicating that the mailbox you're trying to run this cmdlet against is not a resource type, so it doesn't have the special calendar attendant, and therefore cannot have a delegate. \n"
            ]
        },
        {
            "id": "240-5-6",
            "pair": [
                "The error you recieved is indicating that the mailbox you're trying to run this cmdlet against is not a resource type, so it doesn't have the special calendar attendant, and therefore cannot have a delegate. \n",
                "As far as I know, the only way to set the type of delegate you're trying to set is by doing it manually through the Outlook desktop client (like your screenshot shows)."
            ]
        }
    ],
    [
        {
            "id": "241-1-2",
            "pair": [
                "Holding Ctrl+Shift while clicking on a shortcut (or in the Start Menu\u2019s search box) no longer works to run as administrator. Specifically, nothing happens; no UAC prompt and the program does not run at all.\n",
                "I can still run things as administrator by using the run as admin context-menu item on the EXE, but not on shortcuts to it. I can also run as admin if the run as admin property of a shortcut is checked and the shortcut is opened normally. Also, EXEs that have the admin flag (those with the shield icon overlay) work either directly or through the shortcut. It is non-admin programs that cannot be manually run as admin.\n"
            ]
        },
        {
            "id": "241-2-3",
            "pair": [
                "I can still run things as administrator by using the run as admin context-menu item on the EXE, but not on shortcuts to it. I can also run as admin if the run as admin property of a shortcut is checked and the shortcut is opened normally. Also, EXEs that have the admin flag (those with the shield icon overlay) work either directly or through the shortcut. It is non-admin programs that cannot be manually run as admin.\n",
                "The only notable change since this behavior that I can think of is that I disabled the Win-key hotkeys (NoWinKeys=1), but that should not be related.\n"
            ]
        },
        {
            "id": "241-3-4",
            "pair": [
                "The only notable change since this behavior that I can think of is that I disabled the Win-key hotkeys (NoWinKeys=1), but that should not be related.\n",
                "Does anyone know what could be causing this? I thought that maybe something in the shortcut handler (HKCR\\lnkfile) was corrupted, but it looks okay.\n"
            ]
        },
        {
            "id": "241-4-5",
            "pair": [
                "Does anyone know what could be causing this? I thought that maybe something in the shortcut handler (HKCR\\lnkfile) was corrupted, but it looks okay.\n",
                "I have the impression that CTRL+SHIFT+ENTER only works when using the Instant Search feature in the Windows Start menu. Open the Services applet and check if Windows Search Service is enabled. If disabled, see if enabling it changes anything (let it have a few moments to do its indexing).\n"
            ]
        },
        {
            "id": "241-5-6",
            "pair": [
                "I have the impression that CTRL+SHIFT+ENTER only works when using the Instant Search feature in the Windows Start menu. Open the Services applet and check if Windows Search Service is enabled. If disabled, see if enabling it changes anything (let it have a few moments to do its indexing).\n",
                "A question : What happens if you turn off UAC temporarily ?"
            ]
        }
    ],
    [
        {
            "id": "242-1-2",
            "pair": [
                "You can make a rules file in /etc/udev/rules.d/ folder and make it run a bash script when your USB device is connected. For this you need to find the ID_Vendor and ID_Product code of device. Run lsusb command. There you will see something like Bus 003 Device 005: ID ffff:0005. In this case idVendor = ffff and idProduct = 0005. Yours will be different. \n",
                "Than create for example 50-myUSB.rules file in the above directory with content something like (change ID_vendor and ID_product):\n"
            ]
        },
        {
            "id": "242-2-3",
            "pair": [
                "Than create for example 50-myUSB.rules file in the above directory with content something like (change ID_vendor and ID_product):\n",
                "ATTRS{idVendor}==\"ffff\", ATTRS{idProduct}==\"0005\", RUN+=\"/path/to/script.sh\"\n"
            ]
        },
        {
            "id": "242-3-4",
            "pair": [
                "ATTRS{idVendor}==\"ffff\", ATTRS{idProduct}==\"0005\", RUN+=\"/path/to/script.sh\"\n",
                "This will run script.sh every time you will plug in the device. Now you have to create the script.sh with content something like:\n"
            ]
        },
        {
            "id": "242-4-5",
            "pair": [
                "This will run script.sh every time you will plug in the device. Now you have to create the script.sh with content something like:\n",
                "I am not sure if you need to restart the OS or not for the changes to take effect. \n"
            ]
        },
        {
            "id": "242-5-6",
            "pair": [
                "I am not sure if you need to restart the OS or not for the changes to take effect. \n",
                "Click here if you want to know more about udev rules.\n"
            ]
        },
        {
            "id": "242-6-7",
            "pair": [
                "Click here if you want to know more about udev rules.\n",
                "Im using 4G airtel dongle to connect  internet on Raspberry pi , Im able to auto connect successfully to the internet when the Pi is booted.\n"
            ]
        },
        {
            "id": "242-7-8",
            "pair": [
                "Im using 4G airtel dongle to connect  internet on Raspberry pi , Im able to auto connect successfully to the internet when the Pi is booted.\n",
                "I have removed the dongle from the USB of Pi and when I plug it back again it doesn't connect to internet,but the Pi recognizes as modem (wwan0). Im using wvdial to connect  internet.\n"
            ]
        },
        {
            "id": "242-8-9",
            "pair": [
                "I have removed the dongle from the USB of Pi and when I plug it back again it doesn't connect to internet,but the Pi recognizes as modem (wwan0). Im using wvdial to connect  internet.\n",
                "Can anyone please help with the solution why the pi is unable to connect to internet when it is replugged ?"
            ]
        }
    ],
    [
        {
            "id": "243-1-2",
            "pair": [
                "This was migrated here from stackoverflow, and I'm not sure that this is the best place for it. The TCS Stack Exchange is really for theoretical questions, so details about how many bytes are in the key are often not seen here.\n",
                "That being said, I will try to answer a question similar to yours that is framed in a somewhat more theoretical way:\n"
            ]
        },
        {
            "id": "243-2-3",
            "pair": [
                "That being said, I will try to answer a question similar to yours that is framed in a somewhat more theoretical way:\n",
                "On a disk, you're more concerned about I/O than computation.\n"
            ]
        },
        {
            "id": "243-3-4",
            "pair": [
                "On a disk, you're more concerned about I/O than computation.\n",
                "I know of a few ways to reduce the number of I/Os needed on insert.\n"
            ]
        },
        {
            "id": "243-4-5",
            "pair": [
                "I know of a few ways to reduce the number of I/Os needed on insert.\n",
                "There are a few more that you might be interested in, including buffer trees and Brodal et. al's tunable update/query tradeoff trees.\n"
            ]
        },
        {
            "id": "243-5-6",
            "pair": [
                "There are a few more that you might be interested in, including buffer trees and Brodal et. al's tunable update/query tradeoff trees.\n",
                "The mutable state is the set of active IDs and the number of hits for each.  I bet you can fit those in RAM.\n"
            ]
        },
        {
            "id": "243-6-7",
            "pair": [
                "The mutable state is the set of active IDs and the number of hits for each.  I bet you can fit those in RAM.\n",
                "To insert an item, query the hashtable.  If the key is found, increment the hit-counter.  Otherwise, insert the key with count 0 into the hashtable, and append a record to the end of the journal file.  After all the items are inserted, walk through the file and update each hit-counter to reflect what's in the hashtable.\n"
            ]
        },
        {
            "id": "243-7-8",
            "pair": [
                "To insert an item, query the hashtable.  If the key is found, increment the hit-counter.  Otherwise, insert the key with count 0 into the hashtable, and append a record to the end of the journal file.  After all the items are inserted, walk through the file and update each hit-counter to reflect what's in the hashtable.\n",
                "The increment case involves only a hashtable lookup and increment operation, and never touches disk, so it should be fast.  The append case involves only a hashtable lookup and a single disk write.  You'll build the journal in sequential order which will make good use of cache and avoid seeks."
            ]
        }
    ],
    [
        {
            "id": "244-1-2",
            "pair": [
                "I am trying to install libtirpc on RHEL 6.5 but it fails and asks for dependencies.\n",
                "Why here it is showing three GLIBC versions 2.14, 2.15 and 2.16. ??\n"
            ]
        },
        {
            "id": "244-2-3",
            "pair": [
                "Why here it is showing three GLIBC versions 2.14, 2.15 and 2.16. ??\n",
                "Does it mean that any of these three versions can be installed ??\n"
            ]
        },
        {
            "id": "244-3-4",
            "pair": [
                "Does it mean that any of these three versions can be installed ??\n",
                "There are many other private RPMs which depend on glibc-2.12-1.132.el6.x86_64, so I dont want to touch glibc-2.12, is that possible to install same RPMs of different version. ??\n"
            ]
        },
        {
            "id": "244-4-5",
            "pair": [
                "There are many other private RPMs which depend on glibc-2.12-1.132.el6.x86_64, so I dont want to touch glibc-2.12, is that possible to install same RPMs of different version. ??\n",
                "The version of glibc you have installed does not satisfy the dependencies of the RPM you are trying to install.\n"
            ]
        },
        {
            "id": "244-5-6",
            "pair": [
                "The version of glibc you have installed does not satisfy the dependencies of the RPM you are trying to install.\n",
                "You mentioned that you are using RHEL 6.5 -- the RPM you are trying to install has \"el7\" in the filename (libtirpc-0.2.4-0.3.el7.x86_64.rpm) which means that it was compiled and built against RHEL 7.\n"
            ]
        },
        {
            "id": "244-6-7",
            "pair": [
                "You mentioned that you are using RHEL 6.5 -- the RPM you are trying to install has \"el7\" in the filename (libtirpc-0.2.4-0.3.el7.x86_64.rpm) which means that it was compiled and built against RHEL 7.\n",
                "You can see that your system has GLIBC 2.12, but your RPM requires at least GLIBC 2.14 (but anything up to GLIBC 2.16 will work, too).\n"
            ]
        },
        {
            "id": "244-7-8",
            "pair": [
                "You can see that your system has GLIBC 2.12, but your RPM requires at least GLIBC 2.14 (but anything up to GLIBC 2.16 will work, too).\n",
                "So, you need to find a version of libtirpc for \"el6\" which will work on RHEL 6.x. I'm not sure where you found your RPM, but check the provider of libtirpc for an el6 version.\n"
            ]
        },
        {
            "id": "244-8-9",
            "pair": [
                "So, you need to find a version of libtirpc for \"el6\" which will work on RHEL 6.x. I'm not sure where you found your RPM, but check the provider of libtirpc for an el6 version.\n",
                "If you can't find one, you could locate the source rpm for the RPM that you have and try rebuilding it yourself on your RHEL 6.5 machine by following this guide. That will produce an RPM that works with the GLIBC you have installed."
            ]
        }
    ],
    [
        {
            "id": "245-1-2",
            "pair": [
                "C++ is really a family of languages that are all in one compiler, so it's difficult to be truly a master of everything. There's a 'core' language which is a 'better C', there's an objected oriented langauge (i.e. classes, inheritance, virtuals, etc), the Standard Library (including but not limited to the STL), and a meta-programming template language. Add in the complexities of evolution from C++98 to C++11 to C++14 and it can be a bit daunting.\n",
                "Personally, I'd suggest once you have the basics of C++ down, read the three Scott Meyer's books: Effective C++, More Effective C++, and Effective STL. Then read his blog for the notes on what he thinks is most important about C++11. I think his list is pretty good, but I wish he'd have mentioned std::unique_ptr which I've found to be extremely useful. You may also find the content in my article Dual-use Coding Techniques for Games useful even if you are not targeting Microsoft platforms in particular.\n"
            ]
        },
        {
            "id": "245-2-3",
            "pair": [
                "Personally, I'd suggest once you have the basics of C++ down, read the three Scott Meyer's books: Effective C++, More Effective C++, and Effective STL. Then read his blog for the notes on what he thinks is most important about C++11. I think his list is pretty good, but I wish he'd have mentioned std::unique_ptr which I've found to be extremely useful. You may also find the content in my article Dual-use Coding Techniques for Games useful even if you are not targeting Microsoft platforms in particular.\n",
                "Being a really great coder in any language requires experience mostly learned 'on the job'. Looking at examples of code can also be a good way to learn it. You may want to dig into some existing code that has been well code reviewed by peers and generally judged to be 'good code' in terms of style. I've had lots of good feedback on DirectX Tool Kit, DirectXTex, and DirectXMesh which all make use of C++11 standards and have been though many peer reviews.\n"
            ]
        },
        {
            "id": "245-3-4",
            "pair": [
                "Being a really great coder in any language requires experience mostly learned 'on the job'. Looking at examples of code can also be a good way to learn it. You may want to dig into some existing code that has been well code reviewed by peers and generally judged to be 'good code' in terms of style. I've had lots of good feedback on DirectX Tool Kit, DirectXTex, and DirectXMesh which all make use of C++11 standards and have been though many peer reviews.\n",
                "Of course, being an \"expert in C++\" doesn't mean much if you are not also proficient in whatever specific area of computer science or technology you are supposed to be addressing with your \"expert C++\" code. There's many well-known classic books in each field, so figure out which one is most relevant to your job search.\n"
            ]
        },
        {
            "id": "245-4-5",
            "pair": [
                "Of course, being an \"expert in C++\" doesn't mean much if you are not also proficient in whatever specific area of computer science or technology you are supposed to be addressing with your \"expert C++\" code. There's many well-known classic books in each field, so figure out which one is most relevant to your job search.\n",
                "so the other day I was looking for jobs in the gaming industry (software engineer/game programmer/gameplay programmer, etc) just to get a look at what are some of the requirements. Most of these jobs require 5+ years of experience with C++, and to be an \"expert\" in this language. My question is, what does exactly an expert in C++ need to know, apart from pointers, STL, memory allocation, inheritance, polymorphism? Do you need to know the programming language from A to Z? Every bit of the language? Or what are some of the most important topics that you need to know in order to qualify as a \"C++ expert\"?"
            ]
        }
    ],
    [
        {
            "id": "246-1-2",
            "pair": [
                "I am trying to log slow serial data (about 40 bytes per minute at 9600 baud) on the Pi.\n",
                "I have a rough program using wiringSerial which only provides unbuffered serialGetchar\n"
            ]
        },
        {
            "id": "246-2-3",
            "pair": [
                "I have a rough program using wiringSerial which only provides unbuffered serialGetchar\n",
                "I could write a serialGets but wonder if/why it hasn't been done before.\n"
            ]
        },
        {
            "id": "246-3-4",
            "pair": [
                "I could write a serialGets but wonder if/why it hasn't been done before.\n",
                "PS The last time I wrote something like this was in the 1980s on CP/M\n"
            ]
        },
        {
            "id": "246-4-5",
            "pair": [
                "PS The last time I wrote something like this was in the 1980s on CP/M\n",
                "I want the equivalent of fgets(), and was going to write this using serialGetchar but the timeout caused problems, requiring post processing.\n"
            ]
        },
        {
            "id": "246-5-6",
            "pair": [
                "I want the equivalent of fgets(), and was going to write this using serialGetchar but the timeout caused problems, requiring post processing.\n",
                "I realised the required functionality was already in the kernel, it just required different initialisation.\n"
            ]
        },
        {
            "id": "246-6-7",
            "pair": [
                "I realised the required functionality was already in the kernel, it just required different initialisation.\n",
                "Do you need to write a program for this or can you just use basic Unix commands? Could you provide us more details on what you want to achieve...\n"
            ]
        },
        {
            "id": "246-7-8",
            "pair": [
                "Do you need to write a program for this or can you just use basic Unix commands? Could you provide us more details on what you want to achieve...\n",
                "Assuming you serial port is connected through your USB port. You can easily log incoming bytes from serial port by redirecting (or simply listening) the right interface file, such as:\n"
            ]
        },
        {
            "id": "246-8-9",
            "pair": [
                "Assuming you serial port is connected through your USB port. You can easily log incoming bytes from serial port by redirecting (or simply listening) the right interface file, such as:\n",
                "In the same way, you can write on the serial port:\n"
            ]
        },
        {
            "id": "246-9-10",
            "pair": [
                "In the same way, you can write on the serial port:\n",
                "To find out on which interface file your device is available, check /var/log/dmesg or issue a lsusb."
            ]
        }
    ],
    [
        {
            "id": "247-1-2",
            "pair": [
                "I'd really made GET and POST routes into separate functions. Makes your project clearer as it grows.\n",
                "Instead of error +=, I'd made errors a list, and appended to that. It may be not an obvious win in your case, but imagine you will want some other way to separate errors in the future, eg, with HTML.\n"
            ]
        },
        {
            "id": "247-2-3",
            "pair": [
                "Instead of error +=, I'd made errors a list, and appended to that. It may be not an obvious win in your case, but imagine you will want some other way to separate errors in the future, eg, with HTML.\n",
                "Also it would be much simpler, if postable could serve as a validation and returned reasons why it's not postable. Then there will be no need to check it twice.\n"
            ]
        },
        {
            "id": "247-3-4",
            "pair": [
                "Also it would be much simpler, if postable could serve as a validation and returned reasons why it's not postable. Then there will be no need to check it twice.\n",
                "JSON data with spaces on key side is not JavaScript-friendly, so it's better be something like:\n"
            ]
        },
        {
            "id": "247-4-5",
            "pair": [
                "JSON data with spaces on key side is not JavaScript-friendly, so it's better be something like:\n",
                "Note also using ISO format: With front-end side library like Moment.js, it will be easy to turn it into \"N days ago\", to other timezone or to other format later.\n"
            ]
        },
        {
            "id": "247-5-6",
            "pair": [
                "Note also using ISO format: With front-end side library like Moment.js, it will be easy to turn it into \"N days ago\", to other timezone or to other format later.\n",
                "open(f\"{POSTS}{file}\", \"r\") is not good practice from security point of view. Ensure the resulting path concatenation still lies within the directory it is intended to be. (See some answers here: https://stackoverflow.com/questions/6803505/does-my-code-prevent-directory-traversal )\n"
            ]
        },
        {
            "id": "247-6-7",
            "pair": [
                "open(f\"{POSTS}{file}\", \"r\") is not good practice from security point of view. Ensure the resulting path concatenation still lies within the directory it is intended to be. (See some answers here: https://stackoverflow.com/questions/6803505/does-my-code-prevent-directory-traversal )\n",
                "Note the use of a set instead of a list for membership tests.\n"
            ]
        },
        {
            "id": "247-7-8",
            "pair": [
                "Note the use of a set instead of a list for membership tests.\n",
                "You should really consider removing your styles from the index head and putting them into a separate .css file. Among other things, it'll improve caching behaviour."
            ]
        }
    ],
    [
        {
            "id": "248-1-2",
            "pair": [
                "We are using regular expressions to recognise the valid message but then manually extracting the values.\n",
                "If we change the regex pattern to ^s:([^;]+);r:([^;]+);m--\\\"([\\\\w\\\\s]+)\\\"$ we can use the groups with the regex (the parentheses indicate the groups)\n"
            ]
        },
        {
            "id": "248-2-3",
            "pair": [
                "If we change the regex pattern to ^s:([^;]+);r:([^;]+);m--\\\"([\\\\w\\\\s]+)\\\"$ we can use the groups with the regex (the parentheses indicate the groups)\n",
                "We can be a bit fancier if desired and use named groups ^s:(?<sender>[^;]+);r:(?<receiver>[^;]+);m--\\\"(?<message>[\\\\w\\\\s]+)\\\"$\n"
            ]
        },
        {
            "id": "248-3-4",
            "pair": [
                "We can be a bit fancier if desired and use named groups ^s:(?<sender>[^;]+);r:(?<receiver>[^;]+);m--\\\"(?<message>[\\\\w\\\\s]+)\\\"$\n",
                "We can simplify things an amount by placing all the active code in the validation/parsing piece and using the MessageInfo  so hold the results.\n"
            ]
        },
        {
            "id": "248-4-5",
            "pair": [
                "We can simplify things an amount by placing all the active code in the validation/parsing piece and using the MessageInfo  so hold the results.\n",
                "If we ever need to make changes, we now have them all localized to ProcessLine not split between CoreAction/lineSplitter and the MessageInfo class\n"
            ]
        },
        {
            "id": "248-5-6",
            "pair": [
                "If we ever need to make changes, we now have them all localized to ProcessLine not split between CoreAction/lineSplitter and the MessageInfo class\n",
                "I can tell you're concerned about performance (and not just because of the tag on the question). This is a good impulse, because it helps you avoid selecting terribly inefficient algorithms. But it can also hurt, when it leads you to micromanage details that would be better left to (for example) a regular expression engine.\n"
            ]
        },
        {
            "id": "248-6-7",
            "pair": [
                "I can tell you're concerned about performance (and not just because of the tag on the question). This is a good impulse, because it helps you avoid selecting terribly inefficient algorithms. But it can also hurt, when it leads you to micromanage details that would be better left to (for example) a regular expression engine.\n",
                "You have some good habits. I love to see foreach loops. I'm very glad that you used a StringBuilder rather than just the + operator. Putting sanitization logic in the public setter for a property so that you can maintain a \"clean\" private backing field is a great idea.\n"
            ]
        },
        {
            "id": "248-7-8",
            "pair": [
                "You have some good habits. I love to see foreach loops. I'm very glad that you used a StringBuilder rather than just the + operator. Putting sanitization logic in the public setter for a property so that you can maintain a \"clean\" private backing field is a great idea.\n",
                "Passing ref parameters generally makes code more difficult to maintain. In fact, I'll go farther than that: Anything that increases the scope of a variable usually makes code more difficult to maintain. Imagine I'm reading your lineSplitter function, I've never seen it before, and I'm in a big hurry because there's a bug in the company's legacy chat log parsing program that's costing us thousands of dollars for every minute it's down. I can't tell what I am and am not allowed to change with how the indexer variable is treated, without reading other functions. I can't tell what it will be when it's passed in, I can't tell how it will change when I pass it to stringFill, and I don't know if I'll break anything in CoreAction if I change its value within lineSplitter.\n"
            ]
        },
        {
            "id": "248-8-9",
            "pair": [
                "Passing ref parameters generally makes code more difficult to maintain. In fact, I'll go farther than that: Anything that increases the scope of a variable usually makes code more difficult to maintain. Imagine I'm reading your lineSplitter function, I've never seen it before, and I'm in a big hurry because there's a bug in the company's legacy chat log parsing program that's costing us thousands of dollars for every minute it's down. I can't tell what I am and am not allowed to change with how the indexer variable is treated, without reading other functions. I can't tell what it will be when it's passed in, I can't tell how it will change when I pass it to stringFill, and I don't know if I'll break anything in CoreAction if I change its value within lineSplitter.\n",
                "Now, the fact that you've made all three of those functions private is a big help - I can be sure that there are no other places in the codebase that I might be breaking with edits to lineSplitter. But it would still be vastly preferable to find an approach that doesn't require ref variables at all.\n"
            ]
        },
        {
            "id": "248-9-10",
            "pair": [
                "Now, the fact that you've made all three of those functions private is a big help - I can be sure that there are no other places in the codebase that I might be breaking with edits to lineSplitter. But it would still be vastly preferable to find an approach that doesn't require ref variables at all.\n",
                "An object should always be valid. Take this contrived example:\n"
            ]
        },
        {
            "id": "248-10-11",
            "pair": [
                "An object should always be valid. Take this contrived example:\n",
                "I've heard this called \"temporal coupling\". It's unfriendly because there's no warning that I can't call Length() on an invalid object - just the error when I try. My favorite approach to provide that warning is by required constructor arguments; simply don't allow invalid objects to exist.\n"
            ]
        },
        {
            "id": "248-11-12",
            "pair": [
                "I've heard this called \"temporal coupling\". It's unfriendly because there's no warning that I can't call Length() on an invalid object - just the error when I try. My favorite approach to provide that warning is by required constructor arguments; simply don't allow invalid objects to exist.\n",
                "    public string Value { get; } // no \"set\" -> must be assigned at construction\n"
            ]
        },
        {
            "id": "248-12-13",
            "pair": [
                "    public string Value { get; } // no \"set\" -> must be assigned at construction\n",
                "    /// The value used for <see cref=\"Length()\"/>. Cannot be null.\n"
            ]
        },
        {
            "id": "248-13-14",
            "pair": [
                "    /// The value used for <see cref=\"Length()\"/>. Cannot be null.\n",
                "        Value = value ?? throw new ArgumentNullException(\"Foo requires a non-null value\");\n"
            ]
        },
        {
            "id": "248-14-15",
            "pair": [
                "        Value = value ?? throw new ArgumentNullException(\"Foo requires a non-null value\");\n",
                "Why am I ranting about this? It applies to your strategy of creating a list of MessageInfo objects, only some of which are valid. That works, so long as you always remember to check. I would much rather see the invalid input lines completely filtered out.\n"
            ]
        },
        {
            "id": "248-15-16",
            "pair": [
                "Why am I ranting about this? It applies to your strategy of creating a list of MessageInfo objects, only some of which are valid. That works, so long as you always remember to check. I would much rather see the invalid input lines completely filtered out.\n",
                "String.Join is often nicer than StringBuilder, in the same way Linq is often nicer than a for- or foreach-loop. A StringBuilder generally requires four lines of code:\n"
            ]
        },
        {
            "id": "248-16-17",
            "pair": [
                "String.Join is often nicer than StringBuilder, in the same way Linq is often nicer than a for- or foreach-loop. A StringBuilder generally requires four lines of code:\n",
                "Whereas string.Join (which has similar performance characteristics), often requires only one (or perhaps two for readability):\n"
            ]
        },
        {
            "id": "248-17-18",
            "pair": [
                "Whereas string.Join (which has similar performance characteristics), often requires only one (or perhaps two for readability):\n",
                "Regex Capture Groups are your friends, as Alan T pointed out. Here's a demo you can play around with and see live results (That site uses the Ruby regex engine rather than the .NET regex engine, but the differences are minor enough that you can ignore them here).\n"
            ]
        },
        {
            "id": "248-18-19",
            "pair": [
                "Regex Capture Groups are your friends, as Alan T pointed out. Here's a demo you can play around with and see live results (That site uses the Ruby regex engine rather than the .NET regex engine, but the differences are minor enough that you can ignore them here).\n",
                "Linq is a beautiful library in C# for transforming sequences, with extension functions on IEnumerable like Select(mapper), Where(filter), and ToList(). To me, this task is begging for the application of Linq. My solution uses foreach in 1 place, Linq in 5 places, and 0 fors or whiles. I'll show you more below.\n"
            ]
        },
        {
            "id": "248-19-20",
            "pair": [
                "Linq is a beautiful library in C# for transforming sequences, with extension functions on IEnumerable like Select(mapper), Where(filter), and ToList(). To me, this task is begging for the application of Linq. My solution uses foreach in 1 place, Linq in 5 places, and 0 fors or whiles. I'll show you more below.\n",
                "Separate classes to hold separate logic. You've already done some of this; I would take it even further. Here's how I sliced it all up:\n"
            ]
        },
        {
            "id": "248-20-21",
            "pair": [
                "Separate classes to hold separate logic. You've already done some of this; I would take it even further. Here's how I sliced it all up:\n",
                "An outline of my solution - brace yourself, because I may have gone a little too far (but I'll let you be the judge):\n"
            ]
        },
        {
            "id": "248-21-22",
            "pair": [
                "An outline of my solution - brace yourself, because I may have gone a little too far (but I'll let you be the judge):\n",
                "// Program - \"build these objects, then ask them questions\"\n"
            ]
        },
        {
            "id": "248-22-23",
            "pair": [
                "// Program - \"build these objects, then ask them questions\"\n",
                "            Console.WriteLine($@\"{message.Sender} says \"\"{message.Message}\"\" to {message.Receiver}\");\n"
            ]
        },
        {
            "id": "248-23-24",
            "pair": [
                "            Console.WriteLine($@\"{message.Sender} says \"\"{message.Message}\"\" to {message.Receiver}\");\n",
                "        Console.WriteLine($\"Total data transferred: {log.TotalPayloadSize}MB\");\n"
            ]
        },
        {
            "id": "248-24-25",
            "pair": [
                "        Console.WriteLine($\"Total data transferred: {log.TotalPayloadSize}MB\");\n",
                "// ChatLog - \"Filter bad lines, build ChatMessages, and summarize\"\n"
            ]
        },
        {
            "id": "248-25-26",
            "pair": [
                "// ChatLog - \"Filter bad lines, build ChatMessages, and summarize\"\n",
                "    public IEnumerable<ChatMessage> Messages { get; }\n"
            ]
        },
        {
            "id": "248-26-27",
            "pair": [
                "    public IEnumerable<ChatMessage> Messages { get; }\n",
                "        var lines = Enumerable       // My Linq approach to avoid a foreach loop:\n"
            ]
        },
        {
            "id": "248-27-28",
            "pair": [
                "        var lines = Enumerable       // My Linq approach to avoid a foreach loop:\n",
                "            .Range(1, numberOfLines) // Generate a sequence of N integers\n"
            ]
        },
        {
            "id": "248-28-29",
            "pair": [
                "            .Range(1, numberOfLines) // Generate a sequence of N integers\n",
                "            .Select(_ => readLine()) // Replace each integer with a string from the input\n"
            ]
        },
        {
            "id": "248-29-30",
            "pair": [
                "            .Select(_ => readLine()) // Replace each integer with a string from the input\n",
                "        TotalPayloadSize = Messages.Sum(message => message.PayloadSize);\n"
            ]
        },
        {
            "id": "248-30-31",
            "pair": [
                "        TotalPayloadSize = Messages.Sum(message => message.PayloadSize);\n",
                "    private List<ChatMessages> Parse(IEnumerable<string> lines)\n"
            ]
        },
        {
            "id": "248-31-32",
            "pair": [
                "    private List<ChatMessages> Parse(IEnumerable<string> lines)\n",
                "        //     transform each line to a LineParser object (Select)\n"
            ]
        },
        {
            "id": "248-32-33",
            "pair": [
                "        //     transform each line to a LineParser object (Select)\n",
                "        //     transform each remaining LineParser to a ChatMessage (Select)\n"
            ]
        },
        {
            "id": "248-33-34",
            "pair": [
                "        //     transform each remaining LineParser to a ChatMessage (Select)\n",
                "        //     return a list, to avoid the multiple enumeration trap (ToList)\n"
            ]
        },
        {
            "id": "248-34-35",
            "pair": [
                "        //     return a list, to avoid the multiple enumeration trap (ToList)\n",
                "// LineParser - \"Filter out junk and calculate size\"\n"
            ]
        },
        {
            "id": "248-35-36",
            "pair": [
                "// LineParser - \"Filter out junk and calculate size\"\n",
                "    public ChatMessage Message => _message ?? throw new InvalidOperationException(\"Line could not be parsed\");\n"
            ]
        },
        {
            "id": "248-36-37",
            "pair": [
                "    public ChatMessage Message => _message ?? throw new InvalidOperationException(\"Line could not be parsed\");\n",
                "    private static Regex _format { get; } = new Regex(\"...\");\n"
            ]
        },
        {
            "id": "248-37-38",
            "pair": [
                "    private static Regex _format { get; } = new Regex(\"...\");\n",
                "            // set the _message variable to a new ChatMessage from on match.Groups\n"
            ]
        },
        {
            "id": "248-38-39",
            "pair": [
                "            // set the _message variable to a new ChatMessage from on match.Groups\n",
                "    public ChatMessage(string rawSender, string rawReceiver, string message)\n"
            ]
        },
        {
            "id": "248-39-40",
            "pair": [
                "    public ChatMessage(string rawSender, string rawReceiver, string message)\n",
                "        // Construct two NameParsers, and set field values\n"
            ]
        },
        {
            "id": "248-40-41",
            "pair": [
                "        // Construct two NameParsers, and set field values\n",
                "// NameParser - \"Filter out junk and calculate size\"\n"
            ]
        },
        {
            "id": "248-41-42",
            "pair": [
                "// NameParser - \"Filter out junk and calculate size\"\n",
                "    private int SumOfDigitCharacters(string rawName)\n"
            ]
        },
        {
            "id": "248-42-43",
            "pair": [
                "    private int SumOfDigitCharacters(string rawName)\n",
                "        //     transform rawName to an array of characters\n"
            ]
        },
        {
            "id": "248-43-44",
            "pair": [
                "        //     transform rawName to an array of characters\n",
                "        //     transform each digit character to an int\n"
            ]
        },
        {
            "id": "248-44-45",
            "pair": [
                "        //     transform each digit character to an int\n",
                "    private static Regex _letterOrSpace { get; } = new Regex(\"...\");\n"
            ]
        },
        {
            "id": "248-45-46",
            "pair": [
                "    private static Regex _letterOrSpace { get; } = new Regex(\"...\");\n",
                "    private string StripNonAlphaCharacters(string rawName)\n"
            ]
        },
        {
            "id": "248-46-47",
            "pair": [
                "    private string StripNonAlphaCharacters(string rawName)\n",
                "        var lettersAndSpaces = // Here I used Linq to\n"
            ]
        },
        {
            "id": "248-47-48",
            "pair": [
                "        var lettersAndSpaces = // Here I used Linq to\n",
                "        //     transform rawName to an array of chars\n"
            ]
        },
        {
            "id": "248-48-49",
            "pair": [
                "        //     transform rawName to an array of chars\n",
                "Why so many classes? It might look awkward, but it's hugely valuable in terms of testability. The same goes for the Func<string> parameter to ChatLog - it feels strange but it allows ChatLog to be completely independent of Console, which is a big win. This means that I can create a suite of unit tests automatically verifying the behavior of the program without manually inspecting what was printed to the console after each run of the program. For example:\n"
            ]
        },
        {
            "id": "248-49-50",
            "pair": [
                "Why so many classes? It might look awkward, but it's hugely valuable in terms of testability. The same goes for the Func<string> parameter to ChatLog - it feels strange but it allows ChatLog to be completely independent of Console, which is a big win. This means that I can create a suite of unit tests automatically verifying the behavior of the program without manually inspecting what was printed to the console after each run of the program. For example:\n",
                "    public void NameParser_CalculatesSizeCorrectly()\n"
            ]
        },
        {
            "id": "248-50-51",
            "pair": [
                "    public void NameParser_CalculatesSizeCorrectly()\n",
                "        var testCases = new List<(string rawName, int expectedSize)>\n"
            ]
        },
        {
            "id": "248-51-52",
            "pair": [
                "        var testCases = new List<(string rawName, int expectedSize)>\n",
                "        foreach (var (rawName, expectedSize) in testCases)\n"
            ]
        },
        {
            "id": "248-52-53",
            "pair": [
                "        foreach (var (rawName, expectedSize) in testCases)\n",
                "            Assert.AreEqual(expectedSize, parser.PayloadSize, $\"'{rawName}' should have a payload of size {expectedSize}, not {parser.PayloadSize}\");\n"
            ]
        },
        {
            "id": "248-53-54",
            "pair": [
                "            Assert.AreEqual(expectedSize, parser.PayloadSize, $\"'{rawName}' should have a payload of size {expectedSize}, not {parser.PayloadSize}\");\n",
                "Now that I've verified the behavior of NameParser directly, I don't need to worry about it in any of my other tests. If I make any changes, I can run all of my tests again in a matter of seconds. If there are any problems, the tests immediately show what went wrong.\n"
            ]
        },
        {
            "id": "248-54-55",
            "pair": [
                "Now that I've verified the behavior of NameParser directly, I don't need to worry about it in any of my other tests. If I make any changes, I can run all of my tests again in a matter of seconds. If there are any problems, the tests immediately show what went wrong.\n",
                "A final note about performance. The three situations where you should be concerned about code performance, in my opinion, are these:\n"
            ]
        },
        {
            "id": "248-55-56",
            "pair": [
                "A final note about performance. The three situations where you should be concerned about code performance, in my opinion, are these:\n",
                "Short of those situations, the maintainability of your code is far more important than the speed. I don't know whether my solution is faster than yours, but\n"
            ]
        },
        {
            "id": "248-56-57",
            "pair": [
                "Short of those situations, the maintainability of your code is far more important than the speed. I don't know whether my solution is faster than yours, but\n",
                "I've gone on long enough, so I'll spare you any more preaching about Separation of Concerns and Unit Testing. Let me just include some snippets from my other tests:\n"
            ]
        },
        {
            "id": "248-57-58",
            "pair": [
                "I've gone on long enough, so I'll spare you any more preaching about Separation of Concerns and Unit Testing. Let me just include some snippets from my other tests:\n",
                "private Func<string> Enumerate(params string[] lines)\n"
            ]
        },
        {
            "id": "248-58-59",
            "pair": [
                "private Func<string> Enumerate(params string[] lines)\n",
                "        : throw new InvalidOperationException($\"There were only {lines.Length} lines of input!\");\n"
            ]
        },
        {
            "id": "248-59-60",
            "pair": [
                "        : throw new InvalidOperationException($\"There were only {lines.Length} lines of input!\");\n",
                "    \"s:G3er%6g43i;r:Kak\u20ac$in2% re3p5ab3lic%an;m--\\\"I can sing\\\"\","
            ]
        }
    ],
    [
        {
            "id": "249-1-2",
            "pair": [
                "Memory optimized tables store LOB types in internal tables. \n",
                "I have a table with no explicit LOB types but when I run a query against sys.memory_optimized_tables_internal_attributes, I see varchar(255) columns listed with a type_desc value of INTERNAL OFF-ROW DATA TABLE.\n"
            ]
        },
        {
            "id": "249-2-3",
            "pair": [
                "I have a table with no explicit LOB types but when I run a query against sys.memory_optimized_tables_internal_attributes, I see varchar(255) columns listed with a type_desc value of INTERNAL OFF-ROW DATA TABLE.\n",
                "Running the following query yields 10 columns (COL_58, COL_64, COL_65, COL_67, COL_70, COL_72, COL_73, COL_75, COL_76, COL_79) listed as off-row data table\n"
            ]
        },
        {
            "id": "249-3-4",
            "pair": [
                "Running the following query yields 10 columns (COL_58, COL_64, COL_65, COL_67, COL_70, COL_72, COL_73, COL_75, COL_76, COL_79) listed as off-row data table\n",
                "I assume this storage decision is based on this table being super wide (nearly 10,400 bytes wide), yes?\n"
            ]
        },
        {
            "id": "249-4-5",
            "pair": [
                "I assume this storage decision is based on this table being super wide (nearly 10,400 bytes wide), yes?\n",
                "Bill you are absolutely correct and this is a new feature added in SQL 2016.\n"
            ]
        },
        {
            "id": "249-5-6",
            "pair": [
                "Bill you are absolutely correct and this is a new feature added in SQL 2016.\n",
                "To prove that I took out 10 of you varchar(255) data type columns and recreated the table.  Now I get zero record for the 2nd query.  Because you row maximum row length becomes 7824 MB. \n"
            ]
        },
        {
            "id": "249-6-7",
            "pair": [
                "To prove that I took out 10 of you varchar(255) data type columns and recreated the table.  Now I get zero record for the 2nd query.  Because you row maximum row length becomes 7824 MB. \n",
                "Now if you add one more column with varchar(255) you will see a single entry for internal off-row data table because maximum row length is over 8060 MB  (8079 MB) \n"
            ]
        },
        {
            "id": "249-7-8",
            "pair": [
                "Now if you add one more column with varchar(255) you will see a single entry for internal off-row data table because maximum row length is over 8060 MB  (8079 MB) \n",
                "You can use this query to check your maximum row length."
            ]
        }
    ],
    [
        {
            "id": "25-1-2",
            "pair": [
                "I use KDE and there is a keybinding for this. From the K menu, run \"System Settings\" (you can search for that phrase in the search field if the icon is not already showing). Then go to \"Shortcuts and Gestures\", then \"Global Keyboard Shortcuts\", choose \"KWin\" from list of KDE components. There is an action called \"Window to Next Screen\". Assign a hotkey (I use Ctrl-Alt-Shift-Right) for mine. It's strange that there isn't a hotkey for Window to Previous Screen. But, since I only have 2 monitors, it acts as a toggle between the left and right monitor. I should add that I'm using Linux Mint 13 64-bit with KDE 4.8.5.\n",
                "Perform following commands in terminal (based on this git instructions) and then assign a keyboard shortcut.\n"
            ]
        },
        {
            "id": "25-2-3",
            "pair": [
                "Perform following commands in terminal (based on this git instructions) and then assign a keyboard shortcut.\n",
                "Executing the shortcut should move the active window to the other monitor.\n"
            ]
        },
        {
            "id": "25-3-4",
            "pair": [
                "Executing the shortcut should move the active window to the other monitor.\n",
                "Assigning a keyboard shortcut (based on this blog)\n"
            ]
        },
        {
            "id": "25-4-5",
            "pair": [
                "Assigning a keyboard shortcut (based on this blog)\n",
                "Open up the Xfce settings \u2192 Keyboard \u2192 Keyboard shortcuts\n"
            ]
        },
        {
            "id": "25-5-6",
            "pair": [
                "Open up the Xfce settings \u2192 Keyboard \u2192 Keyboard shortcuts\n",
                "Add an entry for move-to-next-monitor and assign a keyboard shortcut to it.\n"
            ]
        },
        {
            "id": "25-6-7",
            "pair": [
                "Add an entry for move-to-next-monitor and assign a keyboard shortcut to it.\n",
                "It should be active right away, so try it on the settings window. :)"
            ]
        }
    ],
    [
        {
            "id": "250-1-2",
            "pair": [
                "The Url Rewrite 2-module for IIS7 is very flexible and allows you to define redirect rules based on patterns and data sources. \n",
                "You can specify a pattern and then have a defined \"list\" in an xml-file with mappings between different IDs if that is what you want. That means that you can do the following:\n"
            ]
        },
        {
            "id": "250-2-3",
            "pair": [
                "You can specify a pattern and then have a defined \"list\" in an xml-file with mappings between different IDs if that is what you want. That means that you can do the following:\n",
                "You know that 345 in the old system actually maps to 49287, so it rewrites it with a single rule (but with mapping from a list) to:\n"
            ]
        },
        {
            "id": "250-3-4",
            "pair": [
                "You know that 345 in the old system actually maps to 49287, so it rewrites it with a single rule (but with mapping from a list) to:\n",
                "http://learn.iis.net/page.aspx/469/using-rewrite-maps-in-url-rewrite-module/\n"
            ]
        },
        {
            "id": "250-4-5",
            "pair": [
                "http://learn.iis.net/page.aspx/469/using-rewrite-maps-in-url-rewrite-module/\n",
                "I've done this with a map with over 2000 entries, and didn't find it to be slow at all. However, it wasn't 2000 rules, which might be a difference. In my case it was 3 rules, but with a map of 2000 entries.\n"
            ]
        },
        {
            "id": "250-5-6",
            "pair": [
                "I've done this with a map with over 2000 entries, and didn't find it to be slow at all. However, it wasn't 2000 rules, which might be a difference. In my case it was 3 rules, but with a map of 2000 entries.\n",
                "You could however map the asp filenames in a map and probably use less rules for it.\n"
            ]
        },
        {
            "id": "250-6-7",
            "pair": [
                "You could however map the asp filenames in a map and probably use less rules for it.\n",
                "Ouch! You might consider making this data-driven - that's probably the approach I'd take. Assuming the new site/CMS will be done in ASP.NET, change the .asp file extension to be handled by ASP.NET instead, then either have a catch-all HTTP handler for *.asp, or catch 404 errors in Global.asax. From there, see if there's a redirect defined for the URL (put these in a database table or something, and optionally stuff them in ASP.NET's Cache object on the fly to cut down on repeated lookups) and spit out a 301 redirect if so. Otherwise just 404."
            ]
        }
    ],
    [
        {
            "id": "251-1-2",
            "pair": [
                "Suppose primary has 1.2.3.4 and mgmt has 2.3.4.5 (and their default gateways are 1.2.3.1 and 2.3.4.1 respectively):\n",
                "If there's a linux host with two interfaces on two different networks (the primary network, and the out-of-band management network, call them \"primary\" and \"mgmt\"), how can return traffic be routed back through the same interface from whence it came?\n"
            ]
        },
        {
            "id": "251-2-3",
            "pair": [
                "If there's a linux host with two interfaces on two different networks (the primary network, and the out-of-band management network, call them \"primary\" and \"mgmt\"), how can return traffic be routed back through the same interface from whence it came?\n",
                "In other words: From where I sit, I go through a router to access either the primary or mgmt interfaces of the machine. When I ssh to the primary interface, no problem. When I ssh to the mgmt interface, my packet arrives at the server, but the server sends its response back through the default gateway which is on the primary interface. Hence, my connection is not established. How can I make the server respond to requests through the mgmt network, that were inbound received on the mgmt network?\n"
            ]
        },
        {
            "id": "251-3-4",
            "pair": [
                "In other words: From where I sit, I go through a router to access either the primary or mgmt interfaces of the machine. When I ssh to the primary interface, no problem. When I ssh to the mgmt interface, my packet arrives at the server, but the server sends its response back through the default gateway which is on the primary interface. Hence, my connection is not established. How can I make the server respond to requests through the mgmt network, that were inbound received on the mgmt network?\n",
                "I found most of an answer here: https://unix.stackexchange.com/questions/4420/reply-on-same-interface-as-incoming\n"
            ]
        },
        {
            "id": "251-4-5",
            "pair": [
                "I found most of an answer here: https://unix.stackexchange.com/questions/4420/reply-on-same-interface-as-incoming\n",
                "More info here: http://linux-ip.net/html/routing-tables.html\n"
            ]
        },
        {
            "id": "251-5-6",
            "pair": [
                "More info here: http://linux-ip.net/html/routing-tables.html\n",
                "Also, thanks to Tom Yan, for providing an answer that had most of the needed parts. But I'm posting this answer because none of the others is really complete, totally accurate, or easy to follow.\n"
            ]
        },
        {
            "id": "251-6-7",
            "pair": [
                "Also, thanks to Tom Yan, for providing an answer that had most of the needed parts. But I'm posting this answer because none of the others is really complete, totally accurate, or easy to follow.\n",
                "The following process works for me on Centos 7. I could not get it to work with NetworkManager, because NetworkManager does not read /etc/sysconfig/network-scripts/rule-* and route-*, and I could not find an equivalent nmcli command to make these changes persistent. If anyone has such a command, please share. So the first required step is to disable NetworkManager:\n"
            ]
        },
        {
            "id": "251-7-8",
            "pair": [
                "The following process works for me on Centos 7. I could not get it to work with NetworkManager, because NetworkManager does not read /etc/sysconfig/network-scripts/rule-* and route-*, and I could not find an equivalent nmcli command to make these changes persistent. If anyone has such a command, please share. So the first required step is to disable NetworkManager:\n",
                "Linux kernel 2.2 and 2.4 support multiple routing tables, each one numbered 0 to 255. The two routing tables normally employed are local (routing table 255) and main (routing table 254). These are listed in /etc/iproute2/rt_tables. You can create a new routing table by choosing an unused number (look in /etc/iproute2/rt_tables) and adding it to /etc/iproute2/rt_tables. \n"
            ]
        },
        {
            "id": "251-8-9",
            "pair": [
                "Linux kernel 2.2 and 2.4 support multiple routing tables, each one numbered 0 to 255. The two routing tables normally employed are local (routing table 255) and main (routing table 254). These are listed in /etc/iproute2/rt_tables. You can create a new routing table by choosing an unused number (look in /etc/iproute2/rt_tables) and adding it to /etc/iproute2/rt_tables. \n",
                "In my case, the \"primary\" interface is eth0, 192.168.20.20 with gateway 192.168.20.1, and the \"mgmt\" interface is eth1, 192.168.5.5 with gateway 192.168.5.1.\n"
            ]
        },
        {
            "id": "251-9-10",
            "pair": [
                "In my case, the \"primary\" interface is eth0, 192.168.20.20 with gateway 192.168.20.1, and the \"mgmt\" interface is eth1, 192.168.5.5 with gateway 192.168.5.1.\n",
                "I am choosing the new routing table number 200, and new routing table name mgmt.\n"
            ]
        },
        {
            "id": "251-10-11",
            "pair": [
                "I am choosing the new routing table number 200, and new routing table name mgmt.\n",
                "Next, you need to create a rule to use the new routing table for traffic that was received on the mgmt interface. Other resources on the internet say you only need to do this for one interface, but you really need to do it on both, for the following reason: Notice, if you bring up only one interface, that interface is able to respond to ping, but if you bring up both interfaces, the one you brought up first responds to ping, and the second one doesn't. This means, if you bring up the mgmt interface first and then the primary interface, the primary will not respond. So it's good practice to define the rules on both interfaces.\n"
            ]
        },
        {
            "id": "251-11-12",
            "pair": [
                "Next, you need to create a rule to use the new routing table for traffic that was received on the mgmt interface. Other resources on the internet say you only need to do this for one interface, but you really need to do it on both, for the following reason: Notice, if you bring up only one interface, that interface is able to respond to ping, but if you bring up both interfaces, the one you brought up first responds to ping, and the second one doesn't. This means, if you bring up the mgmt interface first and then the primary interface, the primary will not respond. So it's good practice to define the rules on both interfaces.\n",
                "This can be done one-time (non-persistent) as follows:\n"
            ]
        },
        {
            "id": "251-12-13",
            "pair": [
                "This can be done one-time (non-persistent) as follows:\n",
                "At this point, both interfaces should be responding. Now, to make it persistent, notice, if you read /etc/sysconfig/network-scripts/ifup-routes, it will parse all the rule-* and route-* files and pass each line as an argument to ip rule add or ip route add accordingly. So create four new files, as follows:\n"
            ]
        },
        {
            "id": "251-13-14",
            "pair": [
                "At this point, both interfaces should be responding. Now, to make it persistent, notice, if you read /etc/sysconfig/network-scripts/ifup-routes, it will parse all the rule-* and route-* files and pass each line as an argument to ip rule add or ip route add accordingly. So create four new files, as follows:\n",
                "After reboot, you should find that both interfaces work by default."
            ]
        }
    ],
    [
        {
            "id": "252-1-2",
            "pair": [
                "Well.. if you change your point of view to a security based one the choice becomes pretty obvious (vBulletin).\n",
                "I don't run anything big enough (vBulletin and SMF for a small amount of users) to comment on performance, but I know that a extremely large gaming site (mmo-champion.com) just switched to vBulletin to increase scalability.\n"
            ]
        },
        {
            "id": "252-2-3",
            "pair": [
                "I don't run anything big enough (vBulletin and SMF for a small amount of users) to comment on performance, but I know that a extremely large gaming site (mmo-champion.com) just switched to vBulletin to increase scalability.\n",
                "Also - consider SMF. It's kind of stalled in development right now (getting 2.0 released), but from what I've seen it's by far the best free software out there for forums. I've played a little with it using nginx, php-fpm and apc and the speed is simply blazing - much faster than I ever could get my vbulletin.\n"
            ]
        },
        {
            "id": "252-3-4",
            "pair": [
                "Also - consider SMF. It's kind of stalled in development right now (getting 2.0 released), but from what I've seen it's by far the best free software out there for forums. I've played a little with it using nginx, php-fpm and apc and the speed is simply blazing - much faster than I ever could get my vbulletin.\n",
                "It all depends on how far you need it to scale and in what direction.\n"
            ]
        },
        {
            "id": "252-4-5",
            "pair": [
                "It all depends on how far you need it to scale and in what direction.\n",
                "I'm currently working on a moderately large vBulletin-based site. Some rough numbers:\n"
            ]
        },
        {
            "id": "252-5-6",
            "pair": [
                "I'm currently working on a moderately large vBulletin-based site. Some rough numbers:\n",
                "There are plenty of bigger forum sites out there, but we're big enough to have run into scaling problems with vBulletin.\n"
            ]
        },
        {
            "id": "252-6-7",
            "pair": [
                "There are plenty of bigger forum sites out there, but we're big enough to have run into scaling problems with vBulletin.\n",
                "We're not running stock vBulletin and from anecdotal evidence many sites of our size or greater have modified their installs to account for particular issues they run into. For example, we modified vBulletin to speed up handling of long threads since stock vBulletin isn't really set up to handle threads with 100,000+ posts in them. Other forum sites will have made hacks to account for large numbers of users, or for fast-moving threads or whatever their particular bottleneck is. Anything I've seen from large phpBB sites would indicate that it's fairly common for them to do so too.\n"
            ]
        },
        {
            "id": "252-7-8",
            "pair": [
                "We're not running stock vBulletin and from anecdotal evidence many sites of our size or greater have modified their installs to account for particular issues they run into. For example, we modified vBulletin to speed up handling of long threads since stock vBulletin isn't really set up to handle threads with 100,000+ posts in them. Other forum sites will have made hacks to account for large numbers of users, or for fast-moving threads or whatever their particular bottleneck is. Anything I've seen from large phpBB sites would indicate that it's fairly common for them to do so too.\n",
                "If you think your forum is going to be large, then you probably will run into scaling issues with the software no matter what you pick. Pick the forum software with which you have the most expertise or where it's easy for you to buy in expertise."
            ]
        }
    ],
    [
        {
            "id": "253-1-2",
            "pair": [
                "You cannot avoid casting. You can use generics wildcard to solve the compile error. from the example code, I see that the order of DTOs in the list determines their concrete type. (is that correct?) so this should work:  \n",
                "I Have a class which calls a method, that will convert an excel file, and each sheet will be a DTO, and returns a list containing those DTOs\n"
            ]
        },
        {
            "id": "253-2-3",
            "pair": [
                "I Have a class which calls a method, that will convert an excel file, and each sheet will be a DTO, and returns a list containing those DTOs\n",
                "But I need to get some attributes individually from each DTO.\n"
            ]
        },
        {
            "id": "253-3-4",
            "pair": [
                "But I need to get some attributes individually from each DTO.\n",
                "At first, I could iterate that list doing a instanceof and casting to my specific DTO.\n"
            ]
        },
        {
            "id": "253-4-5",
            "pair": [
                "At first, I could iterate that list doing a instanceof and casting to my specific DTO.\n",
                "But I don't like this approach, and I'm struggling to improve it.\n"
            ]
        },
        {
            "id": "253-5-6",
            "pair": [
                "But I don't like this approach, and I'm struggling to improve it.\n",
                "I tried and created an Abstract class called AbstractDTO, where all DTOs would extend it, and implement a method:\n"
            ]
        },
        {
            "id": "253-6-7",
            "pair": [
                "I tried and created an Abstract class called AbstractDTO, where all DTOs would extend it, and implement a method:\n",
                "But since the list of AbstractDTO type, the return will not be specific:\n"
            ]
        },
        {
            "id": "253-7-8",
            "pair": [
                "But since the list of AbstractDTO type, the return will not be specific:\n",
                "How can I iterate that list of Object/AbstractDTO, getting the specific dto class, without using instanceof, and maybe without casting? Is it possible?\n"
            ]
        },
        {
            "id": "253-8-9",
            "pair": [
                "How can I iterate that list of Object/AbstractDTO, getting the specific dto class, without using instanceof, and maybe without casting? Is it possible?\n",
                "By the way, I can't change the service to make individual calls for each DTO, because DTO numbers also depends on some arguments, and not only sheets number."
            ]
        }
    ],
    [
        {
            "id": "254-1-2",
            "pair": [
                "If the router has any sort of branding - either stickers on the device itself or the ISP's logo on the web interface, it is more than likely configured by the manufacturer for the ISP and can be locked down or anything.\n",
                "Without knowing your ISP, it is impossible to say for sure, but a lot of ISPs do try to limit functionality on devices to give their support staff an easy life.\n"
            ]
        },
        {
            "id": "254-2-3",
            "pair": [
                "Without knowing your ISP, it is impossible to say for sure, but a lot of ISPs do try to limit functionality on devices to give their support staff an easy life.\n",
                "Your best bet will either to buy a new router and use it on the service or reflashing the router with a generic firmware that is unlocked and gives you full functionality.\n"
            ]
        },
        {
            "id": "254-3-4",
            "pair": [
                "Your best bet will either to buy a new router and use it on the service or reflashing the router with a generic firmware that is unlocked and gives you full functionality.\n",
                "If you do reflash, make sure you have the ability to go back to the ISPs firmware as it is possible that they will not support you or you may have to hand the router back when you finish with their service e.t.c. (it is unlikely, but just be careful.) - Also, be sure to backup all settings as a firmware update will probably wipe all connection settings.\n"
            ]
        },
        {
            "id": "254-4-5",
            "pair": [
                "If you do reflash, make sure you have the ability to go back to the ISPs firmware as it is possible that they will not support you or you may have to hand the router back when you finish with their service e.t.c. (it is unlikely, but just be careful.) - Also, be sure to backup all settings as a firmware update will probably wipe all connection settings.\n",
                "First time here ! So here is my little situation :\n"
            ]
        },
        {
            "id": "254-5-6",
            "pair": [
                "First time here ! So here is my little situation :\n",
                "I have purchased an Wireless IP Camera which I have correctly set up to work on my Wireless Router. Here are the IPs : \n"
            ]
        },
        {
            "id": "254-6-7",
            "pair": [
                "I have purchased an Wireless IP Camera which I have correctly set up to work on my Wireless Router. Here are the IPs : \n",
                "So the camera is perfectly visible when going to 192.168.1.150 when at home.\n"
            ]
        },
        {
            "id": "254-7-8",
            "pair": [
                "So the camera is perfectly visible when going to 192.168.1.150 when at home.\n",
                "The thing is that I want to make it accessible on the Internet.  And for this, as I heard, I need to do port forwarding of my IP Camera to make it accessible on the net (let's say my router's Net ip : 111.111.1.111, I would have to forward the camera to be accessible to 111.111.1.111:8888\n"
            ]
        },
        {
            "id": "254-8-9",
            "pair": [
                "The thing is that I want to make it accessible on the Internet.  And for this, as I heard, I need to do port forwarding of my IP Camera to make it accessible on the net (let's say my router's Net ip : 111.111.1.111, I would have to forward the camera to be accessible to 111.111.1.111:8888\n",
                "So the Wireless Router is a ZyXEL P-600 Series, provided by my ISP here in Thailand.  The thing is that, there doesn't seem to be any configurations for the port forwarding in the Router set up web page (at 192.168.1.1).  On the other hand, the manual of the ZyXEL shows the options accessible. Could the ISP have disabled them ?\n"
            ]
        },
        {
            "id": "254-9-10",
            "pair": [
                "So the Wireless Router is a ZyXEL P-600 Series, provided by my ISP here in Thailand.  The thing is that, there doesn't seem to be any configurations for the port forwarding in the Router set up web page (at 192.168.1.1).  On the other hand, the manual of the ZyXEL shows the options accessible. Could the ISP have disabled them ?\n",
                "So how could I make my IP Camera work then ? Is my only option to reinstall the driver of the P-600 and setup the Internet access by myself (thus probably screwing up my Internet since I'm a networking incompetent) ?"
            ]
        }
    ],
    [
        {
            "id": "255-1-2",
            "pair": [
                "The vast majority of hard disks today use 512-byte logical sectors. Some external enclosures and adapters, though, translate groups of eight 512-byte sectors into 4096-byte sectors. Doing so has certain advantages, but it makes it very difficult to safely move a disk from a direct connection to a connection involving an adapter that does such a translation. Changing the sector sizes in this way renders critical data structures, such as the partition table, invalid. If you start writing data with your USB adapter, you're almost certain to do additional harm to your disk.\n",
                "To recover your data, you should first connect the disk in the way it was connected before your problems began, or at least find a way to connect it that will produce the same sector size as it had when it was connected normally. Only after you've connected your disk in this way will you be able to extract useful data for diagnosis and repair.\n"
            ]
        },
        {
            "id": "255-2-3",
            "pair": [
                "To recover your data, you should first connect the disk in the way it was connected before your problems began, or at least find a way to connect it that will produce the same sector size as it had when it was connected normally. Only after you've connected your disk in this way will you be able to extract useful data for diagnosis and repair.\n",
                "did you ever try GParted live? its very effective.\n"
            ]
        },
        {
            "id": "255-3-4",
            "pair": [
                "did you ever try GParted live? its very effective.\n",
                "after converting your gpt hdd system to mbr, do not change or write on your hdd. use ufs data recovery software for recover your data. also ufs have raw explorer. if you want, you can directly copy/paste your datas."
            ]
        }
    ],
    [
        {
            "id": "256-1-2",
            "pair": [
                "I am using Let's Encrypt (certonly) to generate SSL certificates for several websites hosted on an Apache server. The file location of these certificates is determinate before they are created, so I am writing their paths into my virtual host configuration in advance. Once the site is running, I will use certbot to get the certificate files and then reload the Apache configuration.\n",
                "I also have a global SSL certificate defined with valid files, so every SSL virtual host will be certain to have a certificate.\n"
            ]
        },
        {
            "id": "256-2-3",
            "pair": [
                "I also have a global SSL certificate defined with valid files, so every SSL virtual host will be certain to have a certificate.\n",
                "The problem I'm having is that Apache won't run without all the certificate files, despite having a global fallback.  I tried to conditionally configure the Let's Encrypt certificate only when the file exists using IF, but Apache says SSLCertificateFile not allowed here.\n"
            ]
        },
        {
            "id": "256-3-4",
            "pair": [
                "The problem I'm having is that Apache won't run without all the certificate files, despite having a global fallback.  I tried to conditionally configure the Let's Encrypt certificate only when the file exists using IF, but Apache says SSLCertificateFile not allowed here.\n",
                "How might I override the global SSLCertificateFile only when the new certificate files exist? I'm trying to do all of this without having to modify the configuration before and after the certificates have been generated.\n"
            ]
        },
        {
            "id": "256-4-5",
            "pair": [
                "How might I override the global SSLCertificateFile only when the new certificate files exist? I'm trying to do all of this without having to modify the configuration before and after the certificates have been generated.\n",
                "Another idea: if I understood correctly, you have global virtualhost as a catchall and then other virtualhosts, one per website.\n"
            ]
        },
        {
            "id": "256-5-6",
            "pair": [
                "Another idea: if I understood correctly, you have global virtualhost as a catchall and then other virtualhosts, one per website.\n",
                "If the website has not yet the associated SSL certificate, then it should not be configured at all. You can put each website configuration in its own file, and have 2 directories: sites-enabled and sites-waiting with an Apache configuration saying to include everything from sites-enabled. When your website is configured (meaning it has its SSL certificate) you just need to move its configuration from one directory to another, and reload Apache.\n"
            ]
        },
        {
            "id": "256-6-7",
            "pair": [
                "If the website has not yet the associated SSL certificate, then it should not be configured at all. You can put each website configuration in its own file, and have 2 directories: sites-enabled and sites-waiting with an Apache configuration saying to include everything from sites-enabled. When your website is configured (meaning it has its SSL certificate) you just need to move its configuration from one directory to another, and reload Apache.\n",
                "This kind of setup is done by various Linux distributions."
            ]
        }
    ],
    [
        {
            "id": "257-1-2",
            "pair": [
                "I am having difficulty understanding why anyone would purchase the retail version of windows when the OEM version does not seem to be very different. For example, at the time of writing this, I see the difference on amazon to be $130 for the OEM license and $190 for the retail license.\n",
                "After doing quite a bit of research, it looks as though OEM can only be installed one time, on one machine, and that this is the real advantage of the retail version, that it can be reinstalled multiple times.\n"
            ]
        },
        {
            "id": "257-2-3",
            "pair": [
                "After doing quite a bit of research, it looks as though OEM can only be installed one time, on one machine, and that this is the real advantage of the retail version, that it can be reinstalled multiple times.\n",
                "Here's the catch. OEM copies CAN be reinstalled on different hardware. Microsoft even has support links on their website explaining how to do this. The key is to make sure that the license is fully deactivated and that only one copy of the OEM license is active at any given time. Link\n"
            ]
        },
        {
            "id": "257-3-4",
            "pair": [
                "Here's the catch. OEM copies CAN be reinstalled on different hardware. Microsoft even has support links on their website explaining how to do this. The key is to make sure that the license is fully deactivated and that only one copy of the OEM license is active at any given time. Link\n",
                "If I am buying Windows 10 Professional for a small business and they need 8 copies for example, why would I buy an OS that is $60 more expensive per copy when there does not appear to be any obvious difference between them (other than maybe the OEM version may be more of a pain in the rear in the unlikely scenario that I need to swap out a failing motherboard).\n"
            ]
        },
        {
            "id": "257-4-5",
            "pair": [
                "If I am buying Windows 10 Professional for a small business and they need 8 copies for example, why would I buy an OS that is $60 more expensive per copy when there does not appear to be any obvious difference between them (other than maybe the OEM version may be more of a pain in the rear in the unlikely scenario that I need to swap out a failing motherboard).\n",
                "A very important note is vs osm vs retail is the retail version updates any updates that were added to the iso that you can download. I've installed an OEM version of windows and after it was installed, there were 287 updates,OEM version and  (177 on the retail version)then I installed windows 7 retail that was given to me by a close friend that works for Microsoft. I love the retail version, that I can download the latest version of windows 7 with all the updates. I know I can slip stream the updates to the OEM version of Windows (windows 7 professional in my case. ) retail version is just simpler in my option. \n"
            ]
        },
        {
            "id": "257-5-6",
            "pair": [
                "A very important note is vs osm vs retail is the retail version updates any updates that were added to the iso that you can download. I've installed an OEM version of windows and after it was installed, there were 287 updates,OEM version and  (177 on the retail version)then I installed windows 7 retail that was given to me by a close friend that works for Microsoft. I love the retail version, that I can download the latest version of windows 7 with all the updates. I know I can slip stream the updates to the OEM version of Windows (windows 7 professional in my case. ) retail version is just simpler in my option. \n",
                "On a different note. I wanted to upgrade window 10 from\n"
            ]
        },
        {
            "id": "257-6-7",
            "pair": [
                "On a different note. I wanted to upgrade window 10 from\n",
                "Windows 7. And while it seemed to work for a couple of days. It now hangs for a very long time. Figuring I just go back to windows 7, it say my retail key is incorrect. I've got a call back at 10:30 am from Microsoft to reactivate the retail key that that came with the cd."
            ]
        }
    ],
    [
        {
            "id": "258-1-2",
            "pair": [
                "There are plenty of questions on DBA.SE regarding DBCC CHECKDB and how to resolve problems when errors are returned. My specific question is on actually getting notified that DBCC CHECKDB returned errors. Most all DBAs know that you can automate the command and should run it often.\n",
                "I came across this article by Cindy Gross, which has some very good notes. In it she mentions use of SQL Server Agent that if it finds errors from the execution of the CHECKDB command it will fail that step (or job depending on configuration). She points to Paul Randal's blog post on the topic here.\n"
            ]
        },
        {
            "id": "258-2-3",
            "pair": [
                "I came across this article by Cindy Gross, which has some very good notes. In it she mentions use of SQL Server Agent that if it finds errors from the execution of the CHECKDB command it will fail that step (or job depending on configuration). She points to Paul Randal's blog post on the topic here.\n",
                "Now I am curious if anyone knows that the Check Database Integrity Task in a maintenance plan would do the same thing? MSDN does not mention that it will and I have not truthfully been an environment where it has come across a corruption issue; so can't say that it does. This would be versus simply setting up a SQL Agent Job with multiple steps that runs the specific command against each database, as Cindy suggested.\n"
            ]
        },
        {
            "id": "258-3-4",
            "pair": [
                "Now I am curious if anyone knows that the Check Database Integrity Task in a maintenance plan would do the same thing? MSDN does not mention that it will and I have not truthfully been an environment where it has come across a corruption issue; so can't say that it does. This would be versus simply setting up a SQL Agent Job with multiple steps that runs the specific command against each database, as Cindy suggested.\n",
                "Thoughts? Obviously proof is in the pudding so providing more than just a guess would be helpful...\n"
            ]
        },
        {
            "id": "258-4-5",
            "pair": [
                "Thoughts? Obviously proof is in the pudding so providing more than just a guess would be helpful...\n",
                "I setup a SQL Server 2008 R2 instance and have the following databases:\n"
            ]
        },
        {
            "id": "258-5-6",
            "pair": [
                "I setup a SQL Server 2008 R2 instance and have the following databases:\n",
                "I setup a Maintenance Plan with the Check Database Integrity Task in it.\n"
            ]
        },
        {
            "id": "258-6-7",
            "pair": [
                "I setup a Maintenance Plan with the Check Database Integrity Task in it.\n",
                "After reviewing the log it showed that it will continue checking additional databases after hitting one that failed from corruption being found:\n"
            ]
        },
        {
            "id": "258-7-8",
            "pair": [
                "After reviewing the log it showed that it will continue checking additional databases after hitting one that failed from corruption being found:\n",
                "To get notified about DBCC CHECKDB errors I parse the error log with Powershell every night and it emails us when there is an error in the SQL error log and specifically if there are DBCC errors\n"
            ]
        },
        {
            "id": "258-8-9",
            "pair": [
                "To get notified about DBCC CHECKDB errors I parse the error log with Powershell every night and it emails us when there is an error in the SQL error log and specifically if there are DBCC errors\n",
                "It's the easiest way for us to monitor 700+ databases"
            ]
        }
    ],
    [
        {
            "id": "259-1-2",
            "pair": [
                "The relation would be not as much a pure space:cpu issue. Depending on the solution you are aiming at, you might need CPU power for features like deduplication, compression, encryption or hash / checksum calculations. \n",
                "A simple data copy process would not incur much CPU overhead unless you have a really badly designed system. Your requirement of 100 GB per 18 hours would average to 1,58 MB/second - even a netbook would be able to cope with that nowdays.\n"
            ]
        },
        {
            "id": "259-2-3",
            "pair": [
                "A simple data copy process would not incur much CPU overhead unless you have a really badly designed system. Your requirement of 100 GB per 18 hours would average to 1,58 MB/second - even a netbook would be able to cope with that nowdays.\n",
                "You also should concentrate on the I/O backend you are using. While writing 1,58 MB/second does not sound terribly challenging, using 50-100 simultaneous processes to do so will incur a lot of random write load. Hard disks do not cope well with random (write) loads as these will incur a lot of time-intensive head seeks, so you will need to have something that cushions randomness - like a DRAM or SSD write cache.\n"
            ]
        },
        {
            "id": "259-3-4",
            "pair": [
                "You also should concentrate on the I/O backend you are using. While writing 1,58 MB/second does not sound terribly challenging, using 50-100 simultaneous processes to do so will incur a lot of random write load. Hard disks do not cope well with random (write) loads as these will incur a lot of time-intensive head seeks, so you will need to have something that cushions randomness - like a DRAM or SSD write cache.\n",
                "is there a relationship between storage space and number of cpu's required to handle it? if i have 100GB of data a night coming in through a fat pipe remotely from 50 different sites so that the total data footprint over a 18hr period is 100GB, and i wanted to have a 20TB NAS or storage system receiving it, does the NAS server need to have, for example, 1 xeon or should it be a dual cpu with multi core etc\n"
            ]
        },
        {
            "id": "259-4-5",
            "pair": [
                "is there a relationship between storage space and number of cpu's required to handle it? if i have 100GB of data a night coming in through a fat pipe remotely from 50 different sites so that the total data footprint over a 18hr period is 100GB, and i wanted to have a 20TB NAS or storage system receiving it, does the NAS server need to have, for example, 1 xeon or should it be a dual cpu with multi core etc\n",
                "the purpose of the NAS is redundancy for data in the field. Assume that there is no bandwidth issue and we will be using fat pipes to move the data, my concern is the simultaneous piping of seperate geographic instances of 2GB of data coming in from 50 - 100 seperate sites. i don't want to have read/write issues or missing/crashing of a sync.\n"
            ]
        },
        {
            "id": "259-5-6",
            "pair": [
                "the purpose of the NAS is redundancy for data in the field. Assume that there is no bandwidth issue and we will be using fat pipes to move the data, my concern is the simultaneous piping of seperate geographic instances of 2GB of data coming in from 50 - 100 seperate sites. i don't want to have read/write issues or missing/crashing of a sync.\n",
                "i need a direction to start testing with this idea, so if i have to move that much data onto a NAS server nightly.\n"
            ]
        },
        {
            "id": "259-6-7",
            "pair": [
                "i need a direction to start testing with this idea, so if i have to move that much data onto a NAS server nightly.\n",
                "if it is too vague a question i can elaborate it more, thanks."
            ]
        }
    ],
    [
        {
            "id": "26-1-2",
            "pair": [
                "This really isn't a full answer, but apparently comments are frowned upon here. \n",
                "For the most part, the indentation, use of double quotes and whitespace around operators seem consistent. There is one place (see below) where you continue an expression on the next line. Someone glancing over the code can assume that randExtreme() is it's own call instead of part of the previous expression. I would recommend adding extra indentation when continueing an expression on the next line.\n"
            ]
        },
        {
            "id": "26-2-3",
            "pair": [
                "For the most part, the indentation, use of double quotes and whitespace around operators seem consistent. There is one place (see below) where you continue an expression on the next line. Someone glancing over the code can assume that randExtreme() is it's own call instead of part of the previous expression. I would recommend adding extra indentation when continueing an expression on the next line.\n",
                "Never ever use eval. Eval allows for arbitrary code execution. Even though you create the elements yourself, you cannot protect yourself from elements that are created by something else elsewhere on the page. Always use an alternative. In your case you can precompute the result and store it in a data-attribute.\n"
            ]
        },
        {
            "id": "26-3-4",
            "pair": [
                "Never ever use eval. Eval allows for arbitrary code execution. Even though you create the elements yourself, you cannot protect yourself from elements that are created by something else elsewhere on the page. Always use an alternative. In your case you can precompute the result and store it in a data-attribute.\n",
                "You are using classes for elements you expect to only have at most 1 of. Use id's instead for elements that should only ever appear once.\n"
            ]
        },
        {
            "id": "26-4-5",
            "pair": [
                "You are using classes for elements you expect to only have at most 1 of. Use id's instead for elements that should only ever appear once.\n",
                "In incrementLevel(..) you abuse the switch-statement. The whole point of the switch statement is to use an appropriate case for the variable in the head of that construction. You should use an if-elseif-else construction instead. In that case you can remove several conditions too, considering that score < 10 is guaranteed to be false if score < 20 is evaluated. Why are you not using Math.floor(..) and division to calculate the level number?\n"
            ]
        },
        {
            "id": "26-5-6",
            "pair": [
                "In incrementLevel(..) you abuse the switch-statement. The whole point of the switch statement is to use an appropriate case for the variable in the head of that construction. You should use an if-elseif-else construction instead. In that case you can remove several conditions too, considering that score < 10 is guaranteed to be false if score < 20 is evaluated. Why are you not using Math.floor(..) and division to calculate the level number?\n",
                "Styling is the task of CSS. U can use HTMLElement.style to change the style of some things via javascript, but generally you should not do this. Consider using an Array of possible classes you can use for the blocks, and randomly select one to add. Then put the styling in your CSS. Put common CSS in it's own class and apply that regardless.\n"
            ]
        },
        {
            "id": "26-6-7",
            "pair": [
                "Styling is the task of CSS. U can use HTMLElement.style to change the style of some things via javascript, but generally you should not do this. Consider using an Array of possible classes you can use for the blocks, and randomly select one to add. Then put the styling in your CSS. Put common CSS in it's own class and apply that regardless.\n",
                "See @BarryCarter's answer for more information on that.\n"
            ]
        },
        {
            "id": "26-7-8",
            "pair": [
                "See @BarryCarter's answer for more information on that.\n",
                "I notice that you are using jQuery in your project, but only in very specific situations. Why use $(\".exp1\").text(), but use scoreIndicator.innerHTML instead of $(scoreIndicator).html( .. )? Consider using more jQuery if you want to use jQuery, or transform the remaining bits to plain javascript.\n"
            ]
        },
        {
            "id": "26-8-9",
            "pair": [
                "I notice that you are using jQuery in your project, but only in very specific situations. Why use $(\".exp1\").text(), but use scoreIndicator.innerHTML instead of $(scoreIndicator).html( .. )? Consider using more jQuery if you want to use jQuery, or transform the remaining bits to plain javascript.\n",
                "You are loading hundreds of numbers into arrays for a series of functions that generate an integer between a continuous interval between a lower bound and an upper bound. You can replace this with a single function that takes a lower bound and an upper bound. To make it inclusive, you just need to increment max with 1.\n"
            ]
        },
        {
            "id": "26-9-10",
            "pair": [
                "You are loading hundreds of numbers into arrays for a series of functions that generate an integer between a continuous interval between a lower bound and an upper bound. You can replace this with a single function that takes a lower bound and an upper bound. To make it inclusive, you just need to increment max with 1.\n",
                "From a usability aspect, white text on a yellow background is impossible to read. Similarly, it is harder to read moving text than it is to read static text. To signify how much time is left, consider using something like a progress bar instead."
            ]
        }
    ],
    [
        {
            "id": "260-1-2",
            "pair": [
                "No.  SQL Developer is just a client that interacts with an Oracle database.  It is not a database in and of itself so it has no way to store and manipulate data other than via an Oracle database.\n",
                "You could download and install an Oracle database on your machine, connect SQL Developer to that, and use the SQL Developer wizard to load the Excel file into a table you create in this new Oracle database. But that is a decent amount of effort to go through just to deal with a single spreadsheet.\n"
            ]
        },
        {
            "id": "260-2-3",
            "pair": [
                "You could download and install an Oracle database on your machine, connect SQL Developer to that, and use the SQL Developer wizard to load the Excel file into a table you create in this new Oracle database. But that is a decent amount of effort to go through just to deal with a single spreadsheet.\n",
                "I have an xlsx file with content in the form of a database table - first row is column names, and columns contain varchars2, integers etc.\n"
            ]
        },
        {
            "id": "260-3-4",
            "pair": [
                "I have an xlsx file with content in the form of a database table - first row is column names, and columns contain varchars2, integers etc.\n",
                "Since VBA and Excel worksheet functions are too slow for it, I want to view and manipulate it in Oracle SQL Developer, Version 19.2.0.206 developer. Everything occurs locally, there is no existing database connection, and none available.\n"
            ]
        },
        {
            "id": "260-4-5",
            "pair": [
                "Since VBA and Excel worksheet functions are too slow for it, I want to view and manipulate it in Oracle SQL Developer, Version 19.2.0.206 developer. Everything occurs locally, there is no existing database connection, and none available.\n",
                "I attempt to do this by clicking \"new connection\" (below screenshot; in German), but it prompts me to enter information for a database connection that I cannot have, because there is none yet. \n"
            ]
        },
        {
            "id": "260-5-6",
            "pair": [
                "I attempt to do this by clicking \"new connection\" (below screenshot; in German), but it prompts me to enter information for a database connection that I cannot have, because there is none yet. \n",
                "When I try to make up a username, password, and use the below seen default hostname, port etc., and click on test, it shows: I/O error: The network adapter could not establish the connection.\n"
            ]
        },
        {
            "id": "260-6-7",
            "pair": [
                "When I try to make up a username, password, and use the below seen default hostname, port etc., and click on test, it shows: I/O error: The network adapter could not establish the connection.\n",
                "Is it possible, and how do I achieve, that there is a \"dummy\" \"connection\" that is not actually connected to anything existing, but allows me to turn a suitable xlsx file into a table for data manipulation?\n"
            ]
        },
        {
            "id": "260-7-8",
            "pair": [
                "Is it possible, and how do I achieve, that there is a \"dummy\" \"connection\" that is not actually connected to anything existing, but allows me to turn a suitable xlsx file into a table for data manipulation?\n",
                "I have access to MS Access, but I hate the interface and have more experience with Oracle SQL, so I would appreciate a solution with Oracle SQL"
            ]
        }
    ],
    [
        {
            "id": "261-1-2",
            "pair": [
                "All you need is an SNMP client and the PowerNet MIB from APC and you can do what you want. At $oldjob, I had written a perl script that could switch/powercycle based on computer names, it was basically reading port mappings from a database and sending SNMP commands to the appropriate unit and took just a few hours to write. \n",
                "I am not looking for a recommendation of which one to use, just an answer as the whether anything to do this exists.  We have a few dozen AP7921 & similar units that until now we have managed on an idivudual basis.  Already collect power usage info using SNMP & Cacti but I want to know; is there any software available (Open Source, Free or Commercial) that would allow me to manage and configure a few dozen of these devices?  \n"
            ]
        },
        {
            "id": "261-2-3",
            "pair": [
                "I am not looking for a recommendation of which one to use, just an answer as the whether anything to do this exists.  We have a few dozen AP7921 & similar units that until now we have managed on an idivudual basis.  Already collect power usage info using SNMP & Cacti but I want to know; is there any software available (Open Source, Free or Commercial) that would allow me to manage and configure a few dozen of these devices?  \n",
                "At a minimum I would like to be able to see the state of and switch ports.  Even better if I can see the labels and update port names.  Better still if most of the switches configurable items could be played with from a single pane of glass, as they say.  \n"
            ]
        },
        {
            "id": "261-3-4",
            "pair": [
                "At a minimum I would like to be able to see the state of and switch ports.  Even better if I can see the labels and update port names.  Better still if most of the switches configurable items could be played with from a single pane of glass, as they say.  \n",
                "Looking around the www and on serverfault I can see that a few people suggest OpenNMS for monitoring.  It does not look like that will give me more than Read access though?  \n"
            ]
        },
        {
            "id": "261-4-5",
            "pair": [
                "Looking around the www and on serverfault I can see that a few people suggest OpenNMS for monitoring.  It does not look like that will give me more than Read access though?  \n",
                "I have also started looking into apcupsd but that looks more geared towards controlling UPS systems, so I am unsure if it is suitable for multiport PDUs.  \n"
            ]
        },
        {
            "id": "261-5-6",
            "pair": [
                "I have also started looking into apcupsd but that looks more geared towards controlling UPS systems, so I am unsure if it is suitable for multiport PDUs.  \n",
                "Any reports of success doing the above, software to investigate and so forth are welcome as comments.  Please only put an answer if you can confirm that \"Yes, such software exists\" (even better if you can tell me what).  I have already spent a few hours digging around without much success, I am clearly using a wrong keyword somewhere as I cannot believe that nothing exists to do this,.  "
            ]
        }
    ],
    [
        {
            "id": "262-1-2",
            "pair": [
                "It is extremely difficult to understand what's going on in your code. Try to think of things the following perspective: the goal of programming is to tell other people what you want the computer to do. And what you want to do is match the template to the base as some location. So how can we make that clearer?\n",
                "Rather than character-wise matching for the appropriate inversion, we can just invert the template and then we just have to match for equality. Suppose we have a function:\n"
            ]
        },
        {
            "id": "262-2-3",
            "pair": [
                "Rather than character-wise matching for the appropriate inversion, we can just invert the template and then we just have to match for equality. Suppose we have a function:\n",
                "which flips A/T and C/G. Once we have that inversion, we're just searching for the inverted match_template in base:\n"
            ]
        },
        {
            "id": "262-3-4",
            "pair": [
                "which flips A/T and C/G. Once we have that inversion, we're just searching for the inverted match_template in base:\n",
                "We've now reduced dozens of lines of loops and variables, which are error prone, into a one line standard algorithm which is very easy to both reason about and for other users to understand. I don't know anything about chemistry, but I get this!\n"
            ]
        },
        {
            "id": "262-4-5",
            "pair": [
                "We've now reduced dozens of lines of loops and variables, which are error prone, into a one line standard algorithm which is very easy to both reason about and for other users to understand. I don't know anything about chemistry, but I get this!\n",
                "For minor comments, avoid using namespace std; and keep your indentation consistent. You do not need variables for the string length, since you can always call .size(). Avoid comparing against true and false. That part can be if (!check) { ... } else { ... }\n"
            ]
        },
        {
            "id": "262-5-6",
            "pair": [
                "For minor comments, avoid using namespace std; and keep your indentation consistent. You do not need variables for the string length, since you can always call .size(). Avoid comparing against true and false. That part can be if (!check) { ... } else { ... }\n",
                "Two constant functions AT and CG which call a common private function that refactors all the repletion there.\n"
            ]
        },
        {
            "id": "262-6-7",
            "pair": [
                "Two constant functions AT and CG which call a common private function that refactors all the repletion there.\n",
                "And you need a function to do whatever main is doing."
            ]
        }
    ],
    [
        {
            "id": "263-1-2",
            "pair": [
                "Passwords are stored encrypted. That doesn't mean they can't have access to your account otherwise.\n",
                "If you store passwords within google chrome, and they copy your profile, they do copy your password database too. If they then open google chrome with your profile, they can simply navigate to the various sites and login without even entering a password. If they want, they can then go to settings and change your password. \n"
            ]
        },
        {
            "id": "263-2-3",
            "pair": [
                "If you store passwords within google chrome, and they copy your profile, they do copy your password database too. If they then open google chrome with your profile, they can simply navigate to the various sites and login without even entering a password. If they want, they can then go to settings and change your password. \n",
                "If you fear for this, then you may want to consider using a service such as LastPass. It stores your passwords in the cloud rather than locally, meaning that even with your profile, they cannot get in. Plugins are not stored in your profile and LastPass will require a login after installing it again, even if its data is copied, and you previously had the password cached.\n"
            ]
        },
        {
            "id": "263-3-4",
            "pair": [
                "If you fear for this, then you may want to consider using a service such as LastPass. It stores your passwords in the cloud rather than locally, meaning that even with your profile, they cannot get in. Plugins are not stored in your profile and LastPass will require a login after installing it again, even if its data is copied, and you previously had the password cached.\n",
                "But if someone has access to your pc and they can run chrome, they can simply access your sites through there anyway.\n"
            ]
        },
        {
            "id": "263-4-5",
            "pair": [
                "But if someone has access to your pc and they can run chrome, they can simply access your sites through there anyway.\n",
                "By default google chrome save you passwords in this location of a computer\n"
            ]
        },
        {
            "id": "263-5-6",
            "pair": [
                "By default google chrome save you passwords in this location of a computer\n",
                "C:\\Users\\$username\\AppData\\Local\\Google\\Chrome\\User Data\\Default and it's the Login Data file. \n"
            ]
        },
        {
            "id": "263-6-7",
            "pair": [
                "C:\\Users\\$username\\AppData\\Local\\Google\\Chrome\\User Data\\Default and it's the Login Data file. \n",
                "so if he take the data file and replace this file in his pc , he can sync the password and view the passwords. Thanks"
            ]
        }
    ],
    [
        {
            "id": "264-1-2",
            "pair": [
                "I have a set of rules in my AWS Config dashboard. And, I want to set a AWS CloudWatch alarm to be triggered whenever Config detects non-compliant resource(s).\n",
                "[The plan is to link that alarm to an SNS topic for sending out emails to the team for any non-compliant event/resource]\n"
            ]
        },
        {
            "id": "264-2-3",
            "pair": [
                "[The plan is to link that alarm to an SNS topic for sending out emails to the team for any non-compliant event/resource]\n",
                "Is there a straightforward way to do that? Or is there a workaround for the same?\n"
            ]
        },
        {
            "id": "264-3-4",
            "pair": [
                "Is there a straightforward way to do that? Or is there a workaround for the same?\n",
                "I went through the available metrics in the Cloudwatch dashboard, but haven't found anything related to AWS Config.\n"
            ]
        },
        {
            "id": "264-4-5",
            "pair": [
                "I went through the available metrics in the Cloudwatch dashboard, but haven't found anything related to AWS Config.\n",
                "From what I know of AWS Config there's no way at the moment to directly publish a metric from which you can base an alarm.\n"
            ]
        },
        {
            "id": "264-5-6",
            "pair": [
                "From what I know of AWS Config there's no way at the moment to directly publish a metric from which you can base an alarm.\n",
                "If the notification has to come from a Cloudwatch alarm then AWS Config will let you execute Lambda based on rules set up in AWS Config - link.\n"
            ]
        },
        {
            "id": "264-6-7",
            "pair": [
                "If the notification has to come from a Cloudwatch alarm then AWS Config will let you execute Lambda based on rules set up in AWS Config - link.\n",
                "If you want a simpler option AWS Config also lets you publish directly to an SNS topic when your rule is evaluated - link\n"
            ]
        },
        {
            "id": "264-7-8",
            "pair": [
                "If you want a simpler option AWS Config also lets you publish directly to an SNS topic when your rule is evaluated - link\n",
                "Failing this you can also watch for CloudTrail events and then publish a CloudWatch metric which you can base an alarm on. This is fairly in depth though and if the important part is getting the emails to be looked at by a person then your best bet is probably to have AWS Config publish directly to SNS itself."
            ]
        }
    ],
    [
        {
            "id": "265-1-2",
            "pair": [
                "ClearQAM pretty much \"just works\" on Windows 7.  The real trick is picking out a card that works.  I know that Avermedia and Happauge have a few options that I know of, and there are others.\n",
                "I'd check at AVSforum to confirm, but I'm sure there are plenty of models that will do the trick. You probably want USB, as I'm sure your laptop has PCMCIA, which probably doesn't have much support now.  As long as you have USB 2.0, you'll be fine, even for 2 channels.\n"
            ]
        },
        {
            "id": "265-2-3",
            "pair": [
                "I'd check at AVSforum to confirm, but I'm sure there are plenty of models that will do the trick. You probably want USB, as I'm sure your laptop has PCMCIA, which probably doesn't have much support now.  As long as you have USB 2.0, you'll be fine, even for 2 channels.\n",
                "Is your PC enough?  Yea, probably.  Recording QAM isn't processor intensive AT ALL...it's just a data dump really, at 2MB/s per stream.  Playback is the issue, but I think a Pentium M isn't going to have any real trouble there.  QAM is just MPEG2, and I think the CPU has enough power to do the job.\n"
            ]
        },
        {
            "id": "265-3-4",
            "pair": [
                "Is your PC enough?  Yea, probably.  Recording QAM isn't processor intensive AT ALL...it's just a data dump really, at 2MB/s per stream.  Playback is the issue, but I think a Pentium M isn't going to have any real trouble there.  QAM is just MPEG2, and I think the CPU has enough power to do the job.\n",
                "Bear in mind that HD QAM is about 6GB/hr, so you will need space.  Also, it's best not to try doing too much as Disk I/O can become an issue.  If you use an external drive for recordings, the capacity and I/O issues won't present any real problems.\n"
            ]
        },
        {
            "id": "265-4-5",
            "pair": [
                "Bear in mind that HD QAM is about 6GB/hr, so you will need space.  Also, it's best not to try doing too much as Disk I/O can become an issue.  If you use an external drive for recordings, the capacity and I/O issues won't present any real problems.\n",
                "You can watch ClearQAM-TV even on windows XP if you use software that supports it (for example hauppauge WinTV7).\n"
            ]
        },
        {
            "id": "265-5-6",
            "pair": [
                "You can watch ClearQAM-TV even on windows XP if you use software that supports it (for example hauppauge WinTV7).\n",
                "You need a tuner for each channel you want to watch. (but there are 2 in 1 so calles dualtuner)\n"
            ]
        },
        {
            "id": "265-6-7",
            "pair": [
                "You need a tuner for each channel you want to watch. (but there are 2 in 1 so calles dualtuner)\n",
                "I think your laptop will be able to handle 2 streams if it has cards built in. If you use external USB-TV-cards it will probably be to slow.\n"
            ]
        },
        {
            "id": "265-7-8",
            "pair": [
                "I think your laptop will be able to handle 2 streams if it has cards built in. If you use external USB-TV-cards it will probably be to slow.\n",
                "If you look for a new TV-card you should by one from hauppauge - their cards are good quality for good prices and you most time get for example WinTV 7 for free with it.\n"
            ]
        },
        {
            "id": "265-8-9",
            "pair": [
                "If you look for a new TV-card you should by one from hauppauge - their cards are good quality for good prices and you most time get for example WinTV 7 for free with it.\n",
                "The Windows Media Center often seems to have problems with ClearQAM (see eg. http://thegreenbutton.com/forums/t/79643.aspx) but there are good media-portals like mediaportla or MythTV!"
            ]
        }
    ],
    [
        {
            "id": "266-1-2",
            "pair": [
                "In Quake, you have 3 different kinds of armor: Green, Yellow, and Red. With Green armor, the armor absorbs 1 point of damage for every point of damage the player takes. In that way, it acts like doubling health. However, consider the complexities of that situation.\n",
                "If you have 10 health, and you pick up a 100 health powerup, you go back up to full 100 health. However, if you pick up 100 Green armor, you now effectively have... 20 health. Remember: you die when you run out of health, not armor.\n"
            ]
        },
        {
            "id": "266-2-3",
            "pair": [
                "If you have 10 health, and you pick up a 100 health powerup, you go back up to full 100 health. However, if you pick up 100 Green armor, you now effectively have... 20 health. Remember: you die when you run out of health, not armor.\n",
                "Point #1: It can reward you for maintaining a state of high health.\n"
            ]
        },
        {
            "id": "266-3-4",
            "pair": [
                "Point #1: It can reward you for maintaining a state of high health.\n",
                "Consider the same situation: you have 10 health. You come across 100 health, and then you come across a 50 health pickup. You still have 100 health. Joy.\n"
            ]
        },
        {
            "id": "266-4-5",
            "pair": [
                "Consider the same situation: you have 10 health. You come across 100 health, and then you come across a 50 health pickup. You still have 100 health. Joy.\n",
                "However, if you have 10 health and come across 100 Green armor, and later get a 50 health pickup, you now have effectively 120 health.\n"
            ]
        },
        {
            "id": "266-5-6",
            "pair": [
                "However, if you have 10 health and come across 100 Green armor, and later get a 50 health pickup, you now have effectively 120 health.\n",
                "Point #2: It provides a way of breaking the health cap, while still allowing the cap to exist and matter.\n"
            ]
        },
        {
            "id": "266-6-7",
            "pair": [
                "Point #2: It provides a way of breaking the health cap, while still allowing the cap to exist and matter.\n",
                "Now look at the Red armor. For every 6 damage you take, 5 goes to the Red armor, and 1 to you. Red armor comes in lots of 200. Even if you are at 10 health, you effectively have 60.\n"
            ]
        },
        {
            "id": "266-7-8",
            "pair": [
                "Now look at the Red armor. For every 6 damage you take, 5 goes to the Red armor, and 1 to you. Red armor comes in lots of 200. Even if you are at 10 health, you effectively have 60.\n",
                "Point #3: Different grades of armor allow for different qualities of effects. Some can provide more of a reward in certain situations, while others are more situational rewards.\n"
            ]
        },
        {
            "id": "266-8-9",
            "pair": [
                "Point #3: Different grades of armor allow for different qualities of effects. Some can provide more of a reward in certain situations, while others are more situational rewards.\n",
                "Point #4: It allows you to provide more rewards for player exploration. It's a different kind of reward from health, weapons, ammo, and other powerups.\n"
            ]
        },
        {
            "id": "266-9-10",
            "pair": [
                "Point #4: It allows you to provide more rewards for player exploration. It's a different kind of reward from health, weapons, ammo, and other powerups.\n",
                "Remember: Quake is not a modern-style hyper-linear FPS. It's more of an adventure game where you shoot people. Rewarding exploration is important, so you need a good set of viable powerups of different qualities. Armor is merely another quality to choose from.\n"
            ]
        },
        {
            "id": "266-10-11",
            "pair": [
                "Remember: Quake is not a modern-style hyper-linear FPS. It's more of an adventure game where you shoot people. Rewarding exploration is important, so you need a good set of viable powerups of different qualities. Armor is merely another quality to choose from.\n",
                "This is why modern FPS games don't use it. They don't need it, since they're very linear games.\n"
            ]
        },
        {
            "id": "266-11-12",
            "pair": [
                "This is why modern FPS games don't use it. They don't need it, since they're very linear games.\n",
                "It can be a valid argument that this type of mechanic just replicates something that's already there, and as such is superfluous, and I've had a similar discussion with someone relating to ammo (the thrust of which was: beyond a certain level you may as well make ammo infinite as either way you're never going to run out anyway).\n"
            ]
        },
        {
            "id": "266-12-13",
            "pair": [
                "It can be a valid argument that this type of mechanic just replicates something that's already there, and as such is superfluous, and I've had a similar discussion with someone relating to ammo (the thrust of which was: beyond a certain level you may as well make ammo infinite as either way you're never going to run out anyway).\n",
                "However, this does have implications beyond just raw mechanical function.\n"
            ]
        },
        {
            "id": "266-13-14",
            "pair": [
                "However, this does have implications beyond just raw mechanical function.\n",
                "Armour, and indeed ammo beyond a certain level, serves an additional purpose.  It's a form of payoff for the player; they defeat the boss, or find a secret area, or navigate a nasty trap, or whatever you have, and as payoff they get to stockpile up on extra goodies.  So it's a risk/reward thing (even if the risk is not always optional) that operates on a psychological level as well as on a purely mechanical one.\n"
            ]
        },
        {
            "id": "266-14-15",
            "pair": [
                "Armour, and indeed ammo beyond a certain level, serves an additional purpose.  It's a form of payoff for the player; they defeat the boss, or find a secret area, or navigate a nasty trap, or whatever you have, and as payoff they get to stockpile up on extra goodies.  So it's a risk/reward thing (even if the risk is not always optional) that operates on a psychological level as well as on a purely mechanical one.\n",
                "Players like getting extra goodies.  The specific case of armour that may be mechanically identical to extra health is just a variation on goodies that breaks the monotony of stuffing a map with extra health.  Ammo you know the player is never going to use because they already have plenty - so what?  It functions on that level as a reward so it does it's job."
            ]
        }
    ],
    [
        {
            "id": "267-1-2",
            "pair": [
                "I am trying to set up a multi-user FreeBSD server using nginx.\n",
                "In my /usr/local/etc/nginx/nginx.conf file I set user www www; so that nginx acts as the www user, part of the www group. I also set user = www and group = www in my /usr/local/etc/php-fpm.conf.\n"
            ]
        },
        {
            "id": "267-2-3",
            "pair": [
                "In my /usr/local/etc/nginx/nginx.conf file I set user www www; so that nginx acts as the www user, part of the www group. I also set user = www and group = www in my /usr/local/etc/php-fpm.conf.\n",
                "What I want to achieve is that I (as the administrator) can add users to my system and create a folder for them (and of course the according server entry in the nginx.conf) in /usr/local/www for them to use, a bit like a shared hosting environment (without any automatic setup).\n"
            ]
        },
        {
            "id": "267-3-4",
            "pair": [
                "What I want to achieve is that I (as the administrator) can add users to my system and create a folder for them (and of course the according server entry in the nginx.conf) in /usr/local/www for them to use, a bit like a shared hosting environment (without any automatic setup).\n",
                "After installing nginx and php I created a first test user anon, and created a folder for him /usr/local/www/anonsite.\n"
            ]
        },
        {
            "id": "267-4-5",
            "pair": [
                "After installing nginx and php I created a first test user anon, and created a folder for him /usr/local/www/anonsite.\n",
                "I then performed chown anon:www anonsite to make him the owner, and set the group to www, permission of the folder then looked like this: drwxr-xr-x 3 anon www 4 Apr 11 22:00 anonsite .\n"
            ]
        },
        {
            "id": "267-5-6",
            "pair": [
                "I then performed chown anon:www anonsite to make him the owner, and set the group to www, permission of the folder then looked like this: drwxr-xr-x 3 anon www 4 Apr 11 22:00 anonsite .\n",
                "Creating a info.php in this folder as anon and pointing a browser to it now works. I then tested downloading and extracting grav, however it will only show a blank page (because of my wrong permission setup, I assume). If I change php-fpm.conf user=anon, it works as intended, or alternatively using chmod -R g+w /usr/local/www/anonsite, after extracting the the downloaded grav folder, will make it work as well.\n"
            ]
        },
        {
            "id": "267-6-7",
            "pair": [
                "Creating a info.php in this folder as anon and pointing a browser to it now works. I then tested downloading and extracting grav, however it will only show a blank page (because of my wrong permission setup, I assume). If I change php-fpm.conf user=anon, it works as intended, or alternatively using chmod -R g+w /usr/local/www/anonsite, after extracting the the downloaded grav folder, will make it work as well.\n",
                "And this is where I am stuck currently and can't wrap my head around. Both of these \"fixes\" seem wrong or bad practice to me. If I compare my setup to a shared host provider I use, my webroot folder there only has drwxr-x--- 5 username apache 4096 Apr 2 05:00 username permissions, and after extracting a grav test setup it will work right away (is this because of the way Apache works, maybe?).\n"
            ]
        },
        {
            "id": "267-7-8",
            "pair": [
                "And this is where I am stuck currently and can't wrap my head around. Both of these \"fixes\" seem wrong or bad practice to me. If I compare my setup to a shared host provider I use, my webroot folder there only has drwxr-x--- 5 username apache 4096 Apr 2 05:00 username permissions, and after extracting a grav test setup it will work right away (is this because of the way Apache works, maybe?).\n",
                "Could someone explain to me why that's the case and maybe walk me through the steps to properly set this up, or what I am doing wrong?\n"
            ]
        },
        {
            "id": "267-8-9",
            "pair": [
                "Could someone explain to me why that's the case and maybe walk me through the steps to properly set this up, or what I am doing wrong?\n",
                "Is the approach I tried considered bad practice overall?\n"
            ]
        },
        {
            "id": "267-9-10",
            "pair": [
                "Is the approach I tried considered bad practice overall?\n",
                "nginx will likely be able to read the files as needed withing being explicitly setting the file's group owner as the nginx user\n"
            ]
        },
        {
            "id": "267-10-11",
            "pair": [
                "nginx will likely be able to read the files as needed withing being explicitly setting the file's group owner as the nginx user\n",
                "and for php-fpm the nginx user can simply be set as the listener"
            ]
        }
    ],
    [
        {
            "id": "268-1-2",
            "pair": [
                "Ultimately, the exact representation of your input will be dependent on the tool you are feeding it in.\n",
                "More generally, for text generation, you will want to model your input and output as sequences of tokens. These tokens can be words, sentences, characters, n-grams, whatever floats your boat.\n"
            ]
        },
        {
            "id": "268-2-3",
            "pair": [
                "More generally, for text generation, you will want to model your input and output as sequences of tokens. These tokens can be words, sentences, characters, n-grams, whatever floats your boat.\n",
                "Each token should then be represented by a vector. That vector could be a one-hot encoding of the token, or as pcko1 suggests, a word embedding. Word embeddings are real vectors which represent your token. They can be used in the same way as one-hot vectors, but they have shown to carry more meaning.\n"
            ]
        },
        {
            "id": "268-3-4",
            "pair": [
                "Each token should then be represented by a vector. That vector could be a one-hot encoding of the token, or as pcko1 suggests, a word embedding. Word embeddings are real vectors which represent your token. They can be used in the same way as one-hot vectors, but they have shown to carry more meaning.\n",
                "Ultimately, you will need to have some sort of vector_to_string(vector) and string_to_vector(string) functions, which will map a token to its corresponding vector, and vice versa. That way, you transform your input string into a sequence of vectors, and then, your output which will be a sequence of vectors can be turned back into a string.\n"
            ]
        },
        {
            "id": "268-4-5",
            "pair": [
                "Ultimately, you will need to have some sort of vector_to_string(vector) and string_to_vector(string) functions, which will map a token to its corresponding vector, and vice versa. That way, you transform your input string into a sequence of vectors, and then, your output which will be a sequence of vectors can be turned back into a string.\n",
                "For text generation, it is useful to add a start_of_sentence and end_of_sentence tokens, to know when to stop generating.\n"
            ]
        },
        {
            "id": "268-5-6",
            "pair": [
                "For text generation, it is useful to add a start_of_sentence and end_of_sentence tokens, to know when to stop generating.\n",
                "I'm attempting to generate a response to an input line of text using an LSTM. I've considered various forms of input, including one-hot encoding each character in the line and passing each input line as a vector of one-hot encoded vectors. I've also considered using a dictionary and one-hot encoding each word in the sentence based on its alphabetical position. \n"
            ]
        },
        {
            "id": "268-6-7",
            "pair": [
                "I'm attempting to generate a response to an input line of text using an LSTM. I've considered various forms of input, including one-hot encoding each character in the line and passing each input line as a vector of one-hot encoded vectors. I've also considered using a dictionary and one-hot encoding each word in the sentence based on its alphabetical position. \n",
                "However, I'm not sure about any of this, as I am new to natural language processing in machine learning. What would be the best way to format my input (and my output) for this problem?"
            ]
        }
    ],
    [
        {
            "id": "269-1-2",
            "pair": [
                "I was looking around for inexpensive linux/PHP hosting for a project that is still in development.  I got some recommendations for slicehost.  But then a freind mentioned rackspace cloud servers, and this looked even better. First it's cheaper (assuming I won't have a lot of bandwidth to begin with).  Also it seems quite flexible and simple, i.e. being able to backup and clone an entire server, and just turn it off when I'm not using it.\n",
                "Are there any disadvantages to rackspace cloud hosting versus a VPS like slicehost?  How complex is it to setup and manage vs. a VPS?\n"
            ]
        },
        {
            "id": "269-2-3",
            "pair": [
                "Are there any disadvantages to rackspace cloud hosting versus a VPS like slicehost?  How complex is it to setup and manage vs. a VPS?\n",
                "I use Rackspace Cloud for my server and I can tell you they are a fairly good host (excellent customer support, superb uptime). Setting up a cloud instance with them is a snap, and happens fairly quickly; also, making a backup is very very easy.\n"
            ]
        },
        {
            "id": "269-3-4",
            "pair": [
                "I use Rackspace Cloud for my server and I can tell you they are a fairly good host (excellent customer support, superb uptime). Setting up a cloud instance with them is a snap, and happens fairly quickly; also, making a backup is very very easy.\n",
                "However, it's not all fun and games - their control panel is slow, is not quite as powerful as I've found from some other hosts, they do not (yet) have a programmatic DNS api (and they handle DNS slightly oddly) and their out-of-band console is rather weak.\n"
            ]
        },
        {
            "id": "269-4-5",
            "pair": [
                "However, it's not all fun and games - their control panel is slow, is not quite as powerful as I've found from some other hosts, they do not (yet) have a programmatic DNS api (and they handle DNS slightly oddly) and their out-of-band console is rather weak.\n",
                "You may be interested at also taking a look at Linode; we use Linode within our company for individual applications (for example, we have one cloud instance for our task management system). They hold up very well, although Linode did have some horrific downtime in the last couple of months (they should've fixed the source of this, though); I discovered that they're pretty slow to update their website when such a crisis happens, but their staff and the community in their IRC channel were great and very helpful. In terms of their control panel, my only primary complaint is a lack of granularity in the bandwidth reporting (though I don't touch the control panel often, so I'm quite possibly using it wrong)\n"
            ]
        },
        {
            "id": "269-5-6",
            "pair": [
                "You may be interested at also taking a look at Linode; we use Linode within our company for individual applications (for example, we have one cloud instance for our task management system). They hold up very well, although Linode did have some horrific downtime in the last couple of months (they should've fixed the source of this, though); I discovered that they're pretty slow to update their website when such a crisis happens, but their staff and the community in their IRC channel were great and very helpful. In terms of their control panel, my only primary complaint is a lack of granularity in the bandwidth reporting (though I don't touch the control panel often, so I'm quite possibly using it wrong)\n",
                "If you're after a performance comparison of various hosts, Eivind Uggedal seems to have done a decent comparison of Slicehost, Linode, Amazon EC2, Rackspace Cloud and PrgMr: \n"
            ]
        },
        {
            "id": "269-6-7",
            "pair": [
                "If you're after a performance comparison of various hosts, Eivind Uggedal seems to have done a decent comparison of Slicehost, Linode, Amazon EC2, Rackspace Cloud and PrgMr: \n",
                "http://journal.uggedal.com/vps-performance-comparison"
            ]
        }
    ],
    [
        {
            "id": "27-1-2",
            "pair": [
                "Almost all desktops have this or similar hardware:\n",
                "How do I go about upgrading 500 Desktops and Laptops from Windows 7 to Windows 8 in the most efficient way possible?\n"
            ]
        },
        {
            "id": "27-2-3",
            "pair": [
                "How do I go about upgrading 500 Desktops and Laptops from Windows 7 to Windows 8 in the most efficient way possible?\n",
                "Depending on how similar your hardware is (or at least with those machines that are similar) you could install Windows 8 on one of those machines and make a complete disk backup and copy it over to all the similar machines' disk. Though I'm not really sure what you refer to as \"similar\". I prefer Linux and disk dump (dd) for that, but alike Windows software exists, if you prefer using Windows.\n"
            ]
        },
        {
            "id": "27-3-4",
            "pair": [
                "Depending on how similar your hardware is (or at least with those machines that are similar) you could install Windows 8 on one of those machines and make a complete disk backup and copy it over to all the similar machines' disk. Though I'm not really sure what you refer to as \"similar\". I prefer Linux and disk dump (dd) for that, but alike Windows software exists, if you prefer using Windows.\n",
                "Updating 500 machines isn't something trivial. If all those machines are seperately configured you should maybe change your setup. What about a single boot image you make available through a server to let those 500 machines boot off it? This way you would only have to update one version of Windows, though we don't know anything about the purpose of these machines and how they are currently set up.\n"
            ]
        },
        {
            "id": "27-4-5",
            "pair": [
                "Updating 500 machines isn't something trivial. If all those machines are seperately configured you should maybe change your setup. What about a single boot image you make available through a server to let those 500 machines boot off it? This way you would only have to update one version of Windows, though we don't know anything about the purpose of these machines and how they are currently set up.\n",
                "Nevertheless this is going to be a huge PITA and there's absolutely no reason I can think of that would demand for an upgrade from Windows 7 to 8. Windows 7 is much more stable and reliable, which an enviroment like yours likely relies on. (You should at least use Windows 8.1, but seriously: Stay on Windows 7) Also, (I hope this is needless) never change a running system ;)\n"
            ]
        },
        {
            "id": "27-5-6",
            "pair": [
                "Nevertheless this is going to be a huge PITA and there's absolutely no reason I can think of that would demand for an upgrade from Windows 7 to 8. Windows 7 is much more stable and reliable, which an enviroment like yours likely relies on. (You should at least use Windows 8.1, but seriously: Stay on Windows 7) Also, (I hope this is needless) never change a running system ;)\n",
                "Anyhow, you will need to provide us with more information on your problem."
            ]
        }
    ],
    [
        {
            "id": "270-1-2",
            "pair": [
                "Linux supports virtual memory, that is, using a disk as an extension of RAM so that the effective size of usable memory grows correspondingly. The kernel will write the contents of a currently unused block of memory to the hard disk so that the memory can be used for another purpose. When the original contents are needed again, they are read back into memory. This is all made completely transparent to the user; programs running under Linux only see the larger amount of memory available and don't notice that parts of them reside on the disk from time to time. Of course, reading and writing the hard disk is slower (on the order of a thousand times slower) than using real memory, so the programs don't run as fast. The part of the hard disk that is used as virtual memory is called the swap space.\n",
                "Linux can use either a normal file in the filesystem or a separate partition for swap space. A swap partition is faster, but it is easier to change the size of a swap file (there's no need to repartition the whole hard disk, and possibly install everything from scratch). When you know how much swap space you need, you should go for a swap partition, but if you are uncertain, you can use a swap file first, use the system for a while so that you can get a feel for how much swap you need, and then make a swap partition when you're confident about its size.\n"
            ]
        },
        {
            "id": "270-2-3",
            "pair": [
                "Linux can use either a normal file in the filesystem or a separate partition for swap space. A swap partition is faster, but it is easier to change the size of a swap file (there's no need to repartition the whole hard disk, and possibly install everything from scratch). When you know how much swap space you need, you should go for a swap partition, but if you are uncertain, you can use a swap file first, use the system for a while so that you can get a feel for how much swap you need, and then make a swap partition when you're confident about its size.\n",
                "You should also know that Linux allows one to use several swap partitions and/or swap files at the same time. This means that if you only occasionally need an unusual amount of swap space, you can set up an extra swap file at such times, instead of keeping the whole amount allocated all the time.\n"
            ]
        },
        {
            "id": "270-3-4",
            "pair": [
                "You should also know that Linux allows one to use several swap partitions and/or swap files at the same time. This means that if you only occasionally need an unusual amount of swap space, you can set up an extra swap file at such times, instead of keeping the whole amount allocated all the time.\n",
                "A note on operating system terminology: computer science usually distinguishes between swapping (writing the whole process out to swap space) and paging (writing only fixed size parts, usually a few kilobytes, at a time). Paging is usually more efficient, and that's what Linux does, but traditional Linux terminology talks about swapping anyway.\n"
            ]
        },
        {
            "id": "270-4-5",
            "pair": [
                "A note on operating system terminology: computer science usually distinguishes between swapping (writing the whole process out to swap space) and paging (writing only fixed size parts, usually a few kilobytes, at a time). Paging is usually more efficient, and that's what Linux does, but traditional Linux terminology talks about swapping anyway.\n",
                "Source: http://www.faqs.org/docs/linux_admin/x1752.html\n"
            ]
        },
        {
            "id": "270-5-6",
            "pair": [
                "Source: http://www.faqs.org/docs/linux_admin/x1752.html\n",
                "I found this explanation from Mugurel Sumanariu very clear:"
            ]
        }
    ],
    [
        {
            "id": "271-1-2",
            "pair": [
                "I've seen many questions and answers regarding how to draw tiled maps but I can't really get my head around it. Many answers suggest either loading the visible part of the map, or loading and unloading chunks of the map. I've decided the best option would be to load chunks, but I'm slightly confused as to how this would be implemented.\n",
                "Currently I'm loading the full map to a 2D array of buffered images, then drawing it every time repaint is called. Q1: If I were to load chunks of the map, would I load the map as a whole then draw the necessary chunk(s), or load & unload the chunks as the player moves along, and if so, how?\n"
            ]
        },
        {
            "id": "271-2-3",
            "pair": [
                "Currently I'm loading the full map to a 2D array of buffered images, then drawing it every time repaint is called. Q1: If I were to load chunks of the map, would I load the map as a whole then draw the necessary chunk(s), or load & unload the chunks as the player moves along, and if so, how?\n",
                "My second question regards the camera. I want the player to be in the centre of the X axis and the camera to follow it. I've thought of drawing everything in relation to the map and calculating the position of the camera in relation to the players coordinates on the map. So, to calculate the camera's X position I understand that I should use cameraX = playerX - (canvasWidth/2), but how should I calculate the Y position? I want the camera to only move up when the player reaches cameraHeight/2 but to move down when the player reaches 3/4(cameraHeight). Q2: Should I check for this in the same way I check for collision, and move the camera relative to the movement of the player until the player stops moving, or am I thinking about it in the wrong way?\n"
            ]
        },
        {
            "id": "271-3-4",
            "pair": [
                "My second question regards the camera. I want the player to be in the centre of the X axis and the camera to follow it. I've thought of drawing everything in relation to the map and calculating the position of the camera in relation to the players coordinates on the map. So, to calculate the camera's X position I understand that I should use cameraX = playerX - (canvasWidth/2), but how should I calculate the Y position? I want the camera to only move up when the player reaches cameraHeight/2 but to move down when the player reaches 3/4(cameraHeight). Q2: Should I check for this in the same way I check for collision, and move the camera relative to the movement of the player until the player stops moving, or am I thinking about it in the wrong way?\n",
                "Q1: Would depend on your hardware limitations (usually mainly memory size and the speed of loading new chunks from disk) and your game, but for larger worlds, the latter one (un/loading on the way) will usually be more suitable. Note that if using this option, you will often also want to pre-load some chunks (yet-not-visible) around the borders of the viewport to avoid stuttering when the players moves and you will need to do this in a separate thread (otherwise the game will stop whenever you load a new chunk).\n"
            ]
        },
        {
            "id": "271-4-5",
            "pair": [
                "Q1: Would depend on your hardware limitations (usually mainly memory size and the speed of loading new chunks from disk) and your game, but for larger worlds, the latter one (un/loading on the way) will usually be more suitable. Note that if using this option, you will often also want to pre-load some chunks (yet-not-visible) around the borders of the viewport to avoid stuttering when the players moves and you will need to do this in a separate thread (otherwise the game will stop whenever you load a new chunk).\n",
                "Again, this might be just an over-kill for a simple 2D side-scroller, so you will probably want to consider whether it wouldn't be feasible to go with the first solution.\n"
            ]
        },
        {
            "id": "271-5-6",
            "pair": [
                "Again, this might be just an over-kill for a simple 2D side-scroller, so you will probably want to consider whether it wouldn't be feasible to go with the first solution.\n",
                "Q2: Don't know if I actually understood what you were asking about, but I would change the Y coordinate of the camera only when"
            ]
        }
    ],
    [
        {
            "id": "272-1-2",
            "pair": [
                "Please be noted that you need a Windows 10 installation disc(and a product key) to install Windows 10.\n",
                "First, in your Windows 8, launch Disk management. In there, resize a disk partition for Windows 10 installation.(It should be at least 20GB). Before installing Windows 10, get an USB stick with Ubuntu installer.\n"
            ]
        },
        {
            "id": "272-2-3",
            "pair": [
                "First, in your Windows 8, launch Disk management. In there, resize a disk partition for Windows 10 installation.(It should be at least 20GB). Before installing Windows 10, get an USB stick with Ubuntu installer.\n",
                "Next, insert Windows 10 installation disc into your computer. Make sure you have changed the boot order. Windows 10 will start to install automatically.  When you are choosing installation type, select Custom (advanced). Select the new empty partition you have created. If you select the wrong partition, data and OS in that partition might be wiped.\n"
            ]
        },
        {
            "id": "272-3-4",
            "pair": [
                "Next, insert Windows 10 installation disc into your computer. Make sure you have changed the boot order. Windows 10 will start to install automatically.  When you are choosing installation type, select Custom (advanced). Select the new empty partition you have created. If you select the wrong partition, data and OS in that partition might be wiped.\n",
                "Your computer will reboot multiple times. When the installation is completed, shutdown your computer(NOTE: To prevent data lose, you should restart your computer, or disable fast boot. If you choose to restart your computer, plug in the USB which contains Ubuntu before your Windows starts). Hit the power button, plug in the USB and change the boot order. Select \"Try Ubuntu Without Installing\". Once you are in Ubuntu, open Terminal and run boot-repair. You're done! \n"
            ]
        },
        {
            "id": "272-4-5",
            "pair": [
                "Your computer will reboot multiple times. When the installation is completed, shutdown your computer(NOTE: To prevent data lose, you should restart your computer, or disable fast boot. If you choose to restart your computer, plug in the USB which contains Ubuntu before your Windows starts). Hit the power button, plug in the USB and change the boot order. Select \"Try Ubuntu Without Installing\". Once you are in Ubuntu, open Terminal and run boot-repair. You're done! \n",
                "Before you mark this question as duplicate please read it carefully. I haven't found similar situation like me in superuser. Currently, I have installed windows 8 and ubuntu. It boots using Grub2 in startup I need to select one of them using Grub. Now I want to install windows 10 but the problem is if I install windows 10 then I won't be able to boot to Ubuntu as windows 10 will overwrite grub with its bootloader. Now my question is, how can get all 3 options to choose when I install windows 10? This is different then win7+win10+ubuntu triple boot in the sense that win 10 uses uefi ??"
            ]
        }
    ],
    [
        {
            "id": "273-1-2",
            "pair": [
                "I am attempting to reboot a Windows XP SP2 box (acting as a server, yeah I know winxp isn't a great choice but currently I am unable to change the OS) on a daily schedule.\n",
                "This appears to have been working correctly in the past, running at 3am each morning, however since the time has been changed to 12.05am then 11.55pm, it appears to have not run at all, the log file indicating the error:-\n"
            ]
        },
        {
            "id": "273-2-3",
            "pair": [
                "This appears to have been working correctly in the past, running at 3am each morning, however since the time has been changed to 12.05am then 11.55pm, it appears to have not run at all, the log file indicating the error:-\n",
                "I have confirmed that the Run as user is set correctly with the correct password. Additionally I have tried setting 'Run only if logged in' which failed to correct the error.\n"
            ]
        },
        {
            "id": "273-3-4",
            "pair": [
                "I have confirmed that the Run as user is set correctly with the correct password. Additionally I have tried setting 'Run only if logged in' which failed to correct the error.\n",
                "When run manually, however, it appears to run correctly!\n"
            ]
        },
        {
            "id": "273-4-5",
            "pair": [
                "When run manually, however, it appears to run correctly!\n",
                "If anyone has any ideas what this might by, I would be very grateful!\n"
            ]
        },
        {
            "id": "273-5-6",
            "pair": [
                "If anyone has any ideas what this might by, I would be very grateful!\n",
                "Does it work if you try shutting down the machine with PsShutdown? I use PsShutdown as a scheduled task to shut down ~600 machines at work each night, sheduling the schtasks CLI command in XP.\n"
            ]
        },
        {
            "id": "273-6-7",
            "pair": [
                "Does it work if you try shutting down the machine with PsShutdown? I use PsShutdown as a scheduled task to shut down ~600 machines at work each night, sheduling the schtasks CLI command in XP.\n",
                "And it is probably stating the obvious, but does the user the task is being scheduled to run as have permission to shut down the machine in question?"
            ]
        }
    ],
    [
        {
            "id": "274-1-2",
            "pair": [
                "This means you're hitting the FcgidMaxProcesses limit (which defaults to 100).\n",
                "If your server is CPU-bound - which should be the case for Wordpress (where the database should work correctly, ie. properly indexed, maybe eating a moderate amount of I/O but still using very few CPU) - it won't be able to serve more than two requests concurrently since you have two CPUs.\n"
            ]
        },
        {
            "id": "274-2-3",
            "pair": [
                "If your server is CPU-bound - which should be the case for Wordpress (where the database should work correctly, ie. properly indexed, maybe eating a moderate amount of I/O but still using very few CPU) - it won't be able to serve more than two requests concurrently since you have two CPUs.\n",
                "If you get past this limit, time sharing (aka the scheduler) on the CPUs makes your requests much longer to process. In this case, the demand is directly measured by the 'load average' (run the 'w' command). If it's 4.0 and you have 2 CPUs, request that would have taken usually 1 second to process will need 2 secs. And so on. I repeat, this is true if your server is CPU-bound; check CPU usage via top, if it reports ~100% us (CPU/user) that's it.\n"
            ]
        },
        {
            "id": "274-3-4",
            "pair": [
                "If you get past this limit, time sharing (aka the scheduler) on the CPUs makes your requests much longer to process. In this case, the demand is directly measured by the 'load average' (run the 'w' command). If it's 4.0 and you have 2 CPUs, request that would have taken usually 1 second to process will need 2 secs. And so on. I repeat, this is true if your server is CPU-bound; check CPU usage via top, if it reports ~100% us (CPU/user) that's it.\n",
                "As a rule of thumb, I don't set FcgidMaxProcesses higher than 2x or 4x the number of CPU cores on a server. Past this limit, it's just to slow. Which means that some incoming requests are rejected (clients see 503 or 504) in order to keep your server running in acceptable conditions.\n"
            ]
        },
        {
            "id": "274-4-5",
            "pair": [
                "As a rule of thumb, I don't set FcgidMaxProcesses higher than 2x or 4x the number of CPU cores on a server. Past this limit, it's just to slow. Which means that some incoming requests are rejected (clients see 503 or 504) in order to keep your server running in acceptable conditions.\n",
                "So you might need to spare some CPU (use more aggressive caching, install php-apc, etc.), or add some CPUs. If you need more detailed CPU stats to optimize into your application you may use http://forge.bearstech.com/trac/wiki/PhpTop which I wrote for this very purpose.\n"
            ]
        },
        {
            "id": "274-5-6",
            "pair": [
                "So you might need to spare some CPU (use more aggressive caching, install php-apc, etc.), or add some CPUs. If you need more detailed CPU stats to optimize into your application you may use http://forge.bearstech.com/trac/wiki/PhpTop which I wrote for this very purpose.\n",
                "im relatively new to running my own server and my host don't seem to be giving me up advice or help on the matter either.\n"
            ]
        },
        {
            "id": "274-6-7",
            "pair": [
                "im relatively new to running my own server and my host don't seem to be giving me up advice or help on the matter either.\n",
                "I run a very popular website and after doing some twitter and fb posts to over 2 million fans and followers the server really seemed to lagg and take strain.\n"
            ]
        },
        {
            "id": "274-7-8",
            "pair": [
                "I run a very popular website and after doing some twitter and fb posts to over 2 million fans and followers the server really seemed to lagg and take strain.\n",
                "The server has 2CPU cores and 4GB of ram, 100GB HDD and is running wordpress as the main website. Wordpress has been given more than enough memory to function and i just wanted to know why we was getting such a lagg on our server even during the peak times with our high spec.\n"
            ]
        },
        {
            "id": "274-8-9",
            "pair": [
                "The server has 2CPU cores and 4GB of ram, 100GB HDD and is running wordpress as the main website. Wordpress has been given more than enough memory to function and i just wanted to know why we was getting such a lagg on our server even during the peak times with our high spec.\n",
                "google analytics only showed around 200-250 people on the site (using realtime analytics)\n"
            ]
        },
        {
            "id": "274-9-10",
            "pair": [
                "google analytics only showed around 200-250 people on the site (using realtime analytics)\n",
                "so my hosting company told me to check the error logs and i saw this happening quite a lot..\n"
            ]
        },
        {
            "id": "274-10-11",
            "pair": [
                "so my hosting company told me to check the error logs and i saw this happening quite a lot..\n",
                "[Wed Jan 30 16:07:18 2013] [warn] [client 92.235.67.138] mod_fcgid: can't apply process slot for /var/www/cgi-bin/cgi_wrapper/cgi_wrapper, referer: URL\n"
            ]
        },
        {
            "id": "274-11-12",
            "pair": [
                "[Wed Jan 30 16:07:18 2013] [warn] [client 92.235.67.138] mod_fcgid: can't apply process slot for /var/www/cgi-bin/cgi_wrapper/cgi_wrapper, referer: URL\n",
                "what exactly does this mean, and how can i resolve it, does this have anything to do with the lagg?"
            ]
        }
    ],
    [
        {
            "id": "275-1-2",
            "pair": [
                "Update is called once a frame, this means that the time between calls depends directly on the frame-rate.\n",
                "This problem can be solved by defining the alpha, a, as a function of t, where 0 \\leq t \\leq 10.\n"
            ]
        },
        {
            "id": "275-2-3",
            "pair": [
                "This problem can be solved by defining the alpha, a, as a function of t, where 0 \\leq t \\leq 10.\n",
                "At t = 10, a = 1, and at t = 0, a = 0, so we can define our function as f(t) = 0.1 * t.\n"
            ]
        },
        {
            "id": "275-3-4",
            "pair": [
                "At t = 10, a = 1, and at t = 0, a = 0, so we can define our function as f(t) = 0.1 * t.\n",
                "In Unity, Time.deltaTime expresses how much time in seconds has elapsed since the last frame.\n"
            ]
        },
        {
            "id": "275-4-5",
            "pair": [
                "In Unity, Time.deltaTime expresses how much time in seconds has elapsed since the last frame.\n",
                "These are alpha values for 1/2 second, which is the repeat sequence for the 9 s blinking part of your question (0.1 + 0.1 + 0.3 s). These 9 seconds hold 18 repetitions of 0.5 seconds each. You only specify the 0.5 seconds with this array. As you can see, the values begin with 1, 10 slots later (= 0.1 s later) you have 0, then back to 1, where it stays for 30 numbers (0.3 s). After this, it can repeat.\n"
            ]
        },
        {
            "id": "275-5-6",
            "pair": [
                "These are alpha values for 1/2 second, which is the repeat sequence for the 9 s blinking part of your question (0.1 + 0.1 + 0.3 s). These 9 seconds hold 18 repetitions of 0.5 seconds each. You only specify the 0.5 seconds with this array. As you can see, the values begin with 1, 10 slots later (= 0.1 s later) you have 0, then back to 1, where it stays for 30 numbers (0.3 s). After this, it can repeat.\n",
                "Whenever you visit your function, you have a time T handy (as you stated). \n"
            ]
        },
        {
            "id": "275-6-7",
            "pair": [
                "Whenever you visit your function, you have a time T handy (as you stated). \n",
                "Change your T value to start from 0 and end at 10. Ie. you have values of T like:\n"
            ]
        },
        {
            "id": "275-7-8",
            "pair": [
                "Change your T value to start from 0 and end at 10. Ie. you have values of T like:\n",
                "Use the value of T to index into the array A of 50 elements, and pick your alpha there. Index using this statement (pseudo code):\n"
            ]
        },
        {
            "id": "275-8-9",
            "pair": [
                "Use the value of T to index into the array A of 50 elements, and pick your alpha there. Index using this statement (pseudo code):\n",
                "This will produce a number between 1 and 50, no matter what T is. If for example T=0, IND will be 1. If T=0.05, IND will be 6. If T=7.142857, IND will be 15. If T=9, IND will be 1. Etc. Remove the \"1 +\" if your language uses zero-origin.\n"
            ]
        },
        {
            "id": "275-9-10",
            "pair": [
                "This will produce a number between 1 and 50, no matter what T is. If for example T=0, IND will be 1. If T=0.05, IND will be 6. If T=7.142857, IND will be 15. If T=9, IND will be 1. Etc. Remove the \"1 +\" if your language uses zero-origin.\n",
                "Your functions would look like this (sorry, i don't know the expressions for modulo and floor, ie. round down, in your language):\n"
            ]
        },
        {
            "id": "275-10-11",
            "pair": [
                "Your functions would look like this (sorry, i don't know the expressions for modulo and floor, ie. round down, in your language):\n",
                "Of course, you can now make different sequences, change the lenght of them, make other effects, etc."
            ]
        }
    ],
    [
        {
            "id": "276-1-2",
            "pair": [
                "If the texture is always the same then you should load the texture once in the game life, for example in the Ship class once it\u00b4s LoadContent is called.\n",
                "***I\u00b4m not sure, but I think XNA loads the texture once, and the other calls to the load method will just use the memory loaded texture. Even if that is true, I think calling that method once is beter.\n"
            ]
        },
        {
            "id": "276-2-3",
            "pair": [
                "***I\u00b4m not sure, but I think XNA loads the texture once, and the other calls to the load method will just use the memory loaded texture. Even if that is true, I think calling that method once is beter.\n",
                "I'm making a SpaceWar!-esque game using XNA. I want to limit my ships to 5 active bullets at any time. I have a Bullet DrawableGameComponent and a Ship DrawableGameComponent. My Ship has an array of 5 Bullet.\n"
            ]
        },
        {
            "id": "276-3-4",
            "pair": [
                "I'm making a SpaceWar!-esque game using XNA. I want to limit my ships to 5 active bullets at any time. I have a Bullet DrawableGameComponent and a Ship DrawableGameComponent. My Ship has an array of 5 Bullet.\n",
                "What is the best way to manage the Bullet textures? Specifically, when should I be calling LoadTexture? Right now, my solution is to populate the Bullet array in the Ship's constructor, with LoadTexture being called in the Bullet constructor. The Bullet objects will be disabled/not visible except when they are active. Does the texture really need to be loaded once for each individual instance of the bullet object? This seems like a very processor-intensive operation.\n"
            ]
        },
        {
            "id": "276-4-5",
            "pair": [
                "What is the best way to manage the Bullet textures? Specifically, when should I be calling LoadTexture? Right now, my solution is to populate the Bullet array in the Ship's constructor, with LoadTexture being called in the Bullet constructor. The Bullet objects will be disabled/not visible except when they are active. Does the texture really need to be loaded once for each individual instance of the bullet object? This seems like a very processor-intensive operation.\n",
                "Note: This is a small-scale project, so I'm OK with not implementing a huge texture-management framework since there won't be more than half a dozen or so in the entire game. I'd still like to hear about scalable solutions for future applications, though."
            ]
        }
    ],
    [
        {
            "id": "277-1-2",
            "pair": [
                "You need to add a route manually to cover all the corporate networks. Find the smallest net block that will cover all the corporate IP addresses. For example, the corporate net might be entirely within the range 172.16.22.XXX\u2014172.16.25.XXX. Thus your net block would be 172.16.16.0/20:\n",
                "Note that on my machine, en0 is the Ethernet, en1 is the AirPort.\n"
            ]
        },
        {
            "id": "277-2-3",
            "pair": [
                "Note that on my machine, en0 is the Ethernet, en1 is the AirPort.\n",
                "I have a script which sets up a bunch of routes for my local situation which is a corporate network for all corporatey stuff (time sheets, leave applications, workplace health & safety reporting, etc), with WiFi for the useful stuff (Bing, O'Reilly Safari, etc). I still haven't figured out where I need to put this script to have Mac OS X automatically add the arcane routes I need when the interface comes up though.\n"
            ]
        },
        {
            "id": "277-3-4",
            "pair": [
                "I have a script which sets up a bunch of routes for my local situation which is a corporate network for all corporatey stuff (time sheets, leave applications, workplace health & safety reporting, etc), with WiFi for the useful stuff (Bing, O'Reilly Safari, etc). I still haven't figured out where I need to put this script to have Mac OS X automatically add the arcane routes I need when the interface comes up though.\n",
                "You can setup your Network Service Order to pass most traffic through Wi-fi or Ethernet.\n"
            ]
        },
        {
            "id": "277-4-5",
            "pair": [
                "You can setup your Network Service Order to pass most traffic through Wi-fi or Ethernet.\n",
                "This makes the preferred connection be whatever service you put at the top of the list. Unfortunately, if you want most information to go through Wi-fi except for very specific items, this won't do that. If the Ethernet connection uses a different IP network range than the Wi-fi then it will automatically pass the required resources through Ethernet.\n"
            ]
        },
        {
            "id": "277-5-6",
            "pair": [
                "This makes the preferred connection be whatever service you put at the top of the list. Unfortunately, if you want most information to go through Wi-fi except for very specific items, this won't do that. If the Ethernet connection uses a different IP network range than the Wi-fi then it will automatically pass the required resources through Ethernet.\n",
                "Any services you need to access that have a DNS record pointing to 192.168.5.x will automatically go through Ethernet, but your DNS query WILL still go to 192.168.1.x\n"
            ]
        },
        {
            "id": "277-6-7",
            "pair": [
                "Any services you need to access that have a DNS record pointing to 192.168.5.x will automatically go through Ethernet, but your DNS query WILL still go to 192.168.1.x\n",
                "Any other websites that you services you visit will go through Wi-fi if Wi-fi as at the top.\n"
            ]
        },
        {
            "id": "277-7-8",
            "pair": [
                "Any other websites that you services you visit will go through Wi-fi if Wi-fi as at the top.\n",
                "To change the service order go to System Preferences->Network. Click on the Action menu (the gear at the bottom of the Services list) and choose \"Set Service Order\" Then drag the service you want to have precedence to the top of the list."
            ]
        }
    ],
    [
        {
            "id": "278-1-2",
            "pair": [
                "Using a client that disrespects the private flag on a private tracker does violate your privacy and, potentially, the privacy of other users (if you have peer exchange enabled, for example) and is considered an offense of the highest order by most private trackers. It is also extremely easy to trace back to you, as any competent tracker will notice irregular activity after you join the swarm with such a client. Using a client with a DHT patch will quickly get you on an inter-tracker blacklist.\n",
                "The only instance where using such a client will not negatively affect you or others is if you limit it to public torrents that have had the private flag set erroneously. To make sure you don't accidentally use the patched client with other torrent files, move uTorrent's configuration files from %appdata%\\Roaming\\uTorrent to the same directory as your original uTorrent executable (thus engaging portable mode) before running the patched client.\n"
            ]
        },
        {
            "id": "278-2-3",
            "pair": [
                "The only instance where using such a client will not negatively affect you or others is if you limit it to public torrents that have had the private flag set erroneously. To make sure you don't accidentally use the patched client with other torrent files, move uTorrent's configuration files from %appdata%\\Roaming\\uTorrent to the same directory as your original uTorrent executable (thus engaging portable mode) before running the patched client.\n",
                "There are releases of uTorrent with DHT Patch and I was wondering if they do not pose a privacy threat.\n"
            ]
        },
        {
            "id": "278-3-4",
            "pair": [
                "There are releases of uTorrent with DHT Patch and I was wondering if they do not pose a privacy threat.\n",
                "DHT Patch is basically a patch, that allows you to use DHT on torrents marked with private flag.\n"
            ]
        },
        {
            "id": "278-4-5",
            "pair": [
                "DHT Patch is basically a patch, that allows you to use DHT on torrents marked with private flag.\n",
                "If I'm not wrong, whole idea of private torrents is to allow only auhtorized (for example through registration) users to use torrent. Making it harder to spy on users' downloading habbits.\n"
            ]
        },
        {
            "id": "278-5-6",
            "pair": [
                "If I'm not wrong, whole idea of private torrents is to allow only auhtorized (for example through registration) users to use torrent. Making it harder to spy on users' downloading habbits.\n",
                "And now, there is DHT Patch, which enables to use DHT on those more \"secure\" torrents.\n"
            ]
        },
        {
            "id": "278-6-7",
            "pair": [
                "And now, there is DHT Patch, which enables to use DHT on those more \"secure\" torrents.\n",
                "Isn't it that patched client starts to propagate IP adressess of people who think they use private torrent and make them exposed?"
            ]
        }
    ],
    [
        {
            "id": "279-1-2",
            "pair": [
                "You might want to check the presence of the installed General Ledger schemes. Even though you are not an accountant, there is always a main General Ledger scheme. This one has in almost all countries the code '1' and this is the assumed default for the third parameter of the excel function I_EOL_GL_ACTCLN_CODE.\n",
                "Typically when an accountant transfers a division to entrepreneur, the original General Ledger schemes remain but are not visible due to licensing.\n"
            ]
        },
        {
            "id": "279-2-3",
            "pair": [
                "Typically when an accountant transfers a division to entrepreneur, the original General Ledger schemes remain but are not visible due to licensing.\n",
                "you will probably see that there are differences between the divisions. Use the correct reporting scheme code. You can not alter them from within Exact Online. Maybe Exact support can help you with a temporary upgrade to enable access to the screen General Ledger schemes. Or even better switch to RGS.\n"
            ]
        },
        {
            "id": "279-3-4",
            "pair": [
                "you will probably see that there are differences between the divisions. Use the correct reporting scheme code. You can not alter them from within Exact Online. Maybe Exact support can help you with a temporary upgrade to enable access to the screen General Ledger schemes. Or even better switch to RGS.\n",
                "I am using the following Excel formula to retrieve the Exact Online classification code from a GL Account:\n"
            ]
        },
        {
            "id": "279-4-5",
            "pair": [
                "I am using the following Excel formula to retrieve the Exact Online classification code from a GL Account:\n",
                "where A3 is the division code (the number), B3 is the General Ledger account for example \"1301\" for Debtors and the last argument is reporting scheme code.\n"
            ]
        },
        {
            "id": "279-5-6",
            "pair": [
                "where A3 is the division code (the number), B3 is the General Ledger account for example \"1301\" for Debtors and the last argument is reporting scheme code.\n",
                "For some divisions in Exact Online, I get the correct classification. For approximately 75% of my divisions I get an empty answer.\n"
            ]
        },
        {
            "id": "279-6-7",
            "pair": [
                "For some divisions in Exact Online, I get the correct classification. For approximately 75% of my divisions I get an empty answer.\n",
                "When I look in the screen \"G/L Account Classifications\", I see no differences. It seems although that some very old and very new divisions in terms of date created work, but the rest not."
            ]
        }
    ],
    [
        {
            "id": "28-1-2",
            "pair": [
                "Coming from a game dev perspective, I'd suggest you pursue a entity component based system (ECS) to build up your simulation.\n",
                "An entity is a container for components. Components represent state (and/or) behaviour of an entity.\n"
            ]
        },
        {
            "id": "28-2-3",
            "pair": [
                "An entity is a container for components. Components represent state (and/or) behaviour of an entity.\n",
                "Using this notion of components, you can build up an agent as follows:\n"
            ]
        },
        {
            "id": "28-3-4",
            "pair": [
                "Using this notion of components, you can build up an agent as follows:\n",
                "Given a set of agents, we need to perform actions on them. We can split this functionality across what is usually termed a \"System\". We could have a system that updates all agents that have a position and velocity component. This would be the core boids algorithm. We could have a display system that syncs the position component of an entity with it's sprite component.\n"
            ]
        },
        {
            "id": "28-4-5",
            "pair": [
                "Given a set of agents, we need to perform actions on them. We can split this functionality across what is usually termed a \"System\". We could have a system that updates all agents that have a position and velocity component. This would be the core boids algorithm. We could have a display system that syncs the position component of an entity with it's sprite component.\n",
                "With a setup in place like that, the core update loop would composed of calling updates on these systems.\n"
            ]
        },
        {
            "id": "28-5-6",
            "pair": [
                "With a setup in place like that, the core update loop would composed of calling updates on these systems.\n",
                "By splitting up an object into components, and having systems work on components rather than an entity directly, you have will have much more flexibility over your architecture - GameplayKit in iOS has an ECS model built in. You can refer to the starting guide here for more information about why an ECS pattern is better suited here.\n"
            ]
        },
        {
            "id": "28-6-7",
            "pair": [
                "By splitting up an object into components, and having systems work on components rather than an entity directly, you have will have much more flexibility over your architecture - GameplayKit in iOS has an ECS model built in. You can refer to the starting guide here for more information about why an ECS pattern is better suited here.\n",
                "This question is about the proper way to architect an implementation of Craig Reynolds Boids algorithm. \n"
            ]
        },
        {
            "id": "28-7-8",
            "pair": [
                "This question is about the proper way to architect an implementation of Craig Reynolds Boids algorithm. \n",
                "And a Simulation object that holds an array of agents. Each iteration of the simulation the Simulation object updates the position and velocity of each Agent in its array of Agents. Something like this:\n"
            ]
        },
        {
            "id": "28-8-9",
            "pair": [
                "And a Simulation object that holds an array of agents. Each iteration of the simulation the Simulation object updates the position and velocity of each Agent in its array of Agents. Something like this:\n",
                "This is ok. I can run the simulation and update the Agent models appropriately. However, I also want to draw the models on screen. \n"
            ]
        },
        {
            "id": "28-9-10",
            "pair": [
                "This is ok. I can run the simulation and update the Agent models appropriately. However, I also want to draw the models on screen. \n",
                "The question is whether I should create a new AgentView class as the view representation of an Agent model object. Or whether (1) maintaining a relationship between each Agent model and a corresponding AgentView is too cumbersome and (2) I should just replace the Agent model with AgentView entirely and let AgentView be the sole representation of an agent (sort of blending model and view state). \n"
            ]
        },
        {
            "id": "28-10-11",
            "pair": [
                "The question is whether I should create a new AgentView class as the view representation of an Agent model object. Or whether (1) maintaining a relationship between each Agent model and a corresponding AgentView is too cumbersome and (2) I should just replace the Agent model with AgentView entirely and let AgentView be the sole representation of an agent (sort of blending model and view state). \n",
                "Thanks in advance for your help. If you would like additional clarification just let me know. "
            ]
        }
    ],
    [
        {
            "id": "280-1-2",
            "pair": [
                "I know the question is pretty common, but I not seems to find good answer.\n",
                "Setup is following - there is webserver and there is a folder where Apache (user www-data) create directories and upload / delete files.\n"
            ]
        },
        {
            "id": "280-2-3",
            "pair": [
                "Setup is following - there is webserver and there is a folder where Apache (user www-data) create directories and upload / delete files.\n",
                "The webmaster may decide to add or delete some files.\n"
            ]
        },
        {
            "id": "280-3-4",
            "pair": [
                "The webmaster may decide to add or delete some files.\n",
                "Most hosting setups uses same user for both Apache and webmaster.\n"
            ]
        },
        {
            "id": "280-4-5",
            "pair": [
                "Most hosting setups uses same user for both Apache and webmaster.\n",
                "I was able to do it inside the folder itself by setting chmod 770 and making a group that include both users, but I was not able to do it for subfolders.\n"
            ]
        },
        {
            "id": "280-5-6",
            "pair": [
                "I was able to do it inside the folder itself by setting chmod 770 and making a group that include both users, but I was not able to do it for subfolders.\n",
                "I would use 664 (rw-rw-r--) instead of 770 (rwxrwx---) for files (default is (rw-r--r--), so files can be read by everyone \n"
            ]
        },
        {
            "id": "280-6-7",
            "pair": [
                "I would use 664 (rw-rw-r--) instead of 770 (rwxrwx---) for files (default is (rw-r--r--), so files can be read by everyone \n",
                "For directories (where default is rwxr-xr-x) you need the execution bit to enter the directory and access files.\n"
            ]
        },
        {
            "id": "280-7-8",
            "pair": [
                "For directories (where default is rwxr-xr-x) you need the execution bit to enter the directory and access files.\n",
                "With setgid newly created files automatically belong to the group of the directory (and not to the default group of the user who created the file),\n"
            ]
        },
        {
            "id": "280-8-9",
            "pair": [
                "With setgid newly created files automatically belong to the group of the directory (and not to the default group of the user who created the file),\n",
                "If your directory is /var/www/html and your group with write permission is www-data, this is all you need:\n"
            ]
        },
        {
            "id": "280-9-10",
            "pair": [
                "If your directory is /var/www/html and your group with write permission is www-data, this is all you need:\n",
                "You can add the verbose -v option to the above commands to see what was changed."
            ]
        }
    ],
    [
        {
            "id": "281-1-2",
            "pair": [
                "Inspired by @Paul's answer, I did some research and found that while it is true that stack space does limit the number of concatenations, and that stack space is a function of available memory and thus varies, the following two points are also true:\n",
                "First, I adapted Paul's test code to concatenate strings: \n"
            ]
        },
        {
            "id": "281-2-3",
            "pair": [
                "First, I adapted Paul's test code to concatenate strings: \n",
                "With this test, the highest I could get when running on my not-so-great laptop (only 6 GB of RAM) was:\n"
            ]
        },
        {
            "id": "281-3-4",
            "pair": [
                "With this test, the highest I could get when running on my not-so-great laptop (only 6 GB of RAM) was:\n",
                "Next, I tried grouping the concatenations by using parenthesis such that the operation would be concatenating multiple groups of concatenations. For example:\n"
            ]
        },
        {
            "id": "281-4-5",
            "pair": [
                "Next, I tried grouping the concatenations by using parenthesis such that the operation would be concatenating multiple groups of concatenations. For example:\n",
                "Doing that I was able to go well beyond the previous limits of 3312 and 3513 variables. The updated code is:\n"
            ]
        },
        {
            "id": "281-5-6",
            "pair": [
                "Doing that I was able to go well beyond the previous limits of 3312 and 3513 variables. The updated code is:\n",
                "The maximum values (for me) now are to use 42 for the first REPLICATE, thus using 43 variables per group, and then using 762 for the second REPLICATE, thus using 762 groups of 43 variables each. The initial group is hard-coded with two variables.\n"
            ]
        },
        {
            "id": "281-6-7",
            "pair": [
                "The maximum values (for me) now are to use 42 for the first REPLICATE, thus using 43 variables per group, and then using 762 for the second REPLICATE, thus using 762 groups of 43 variables each. The initial group is hard-coded with two variables.\n",
                "The output now shows that there are 32,768 characters in the @S variable. If I update the initial group to be (@A+@A+@A) instead of just (@A+@A), then I get the following error:\n"
            ]
        },
        {
            "id": "281-7-8",
            "pair": [
                "The output now shows that there are 32,768 characters in the @S variable. If I update the initial group to be (@A+@A+@A) instead of just (@A+@A), then I get the following error:\n",
                "Notice that the error number is different than before. It is now: 8632. AND, I have this same limit whether I use my SQL Server 2012 instance or the SQL Server 2017 instance.\n"
            ]
        },
        {
            "id": "281-8-9",
            "pair": [
                "Notice that the error number is different than before. It is now: 8632. AND, I have this same limit whether I use my SQL Server 2012 instance or the SQL Server 2017 instance.\n",
                "It is probably no coincidence that the upper-limit here \u2014 32,768 \u2014 is the max capacity of SMALLINT (Int16 in .NET) IF starting at 0 (the max value is 32,767 but arrays in many/most programming languages are 0-based).\n"
            ]
        },
        {
            "id": "281-9-10",
            "pair": [
                "It is probably no coincidence that the upper-limit here \u2014 32,768 \u2014 is the max capacity of SMALLINT (Int16 in .NET) IF starting at 0 (the max value is 32,767 but arrays in many/most programming languages are 0-based).\n",
                "Using SELECT statements instead of SETs can improve performance and readability, and may get you around the stated error. So instead of:"
            ]
        }
    ],
    [
        {
            "id": "282-1-2",
            "pair": [
                "Despite being partitioned, the SSD in question is still physically one drive, and is therefore recognized as such in the BIOS and by the SATA controller within your computer. This being said, Windows Bootloader also only recognizes the SSD by the fact that it is a single drive and not in fact a drive separated into 2 partitions, each partition with its own bootable OS; Windows Bootloader only knows the drive by the fact that it can be booted to. This is why the Windows Bootloader doesn't show at boot/only shows one option to boot to.\n",
                "GRUB2, on the other hand, can recognize the SSD as being split into 2 partitions, each with operating systems, and therefore handles them as such. It is not possible, with Windows Bootloader, to boot into separate Operating Systems on a partitioned drive, as Windows Bootloader is incapable of recognizing partitions on a drive.\n"
            ]
        },
        {
            "id": "282-2-3",
            "pair": [
                "GRUB2, on the other hand, can recognize the SSD as being split into 2 partitions, each with operating systems, and therefore handles them as such. It is not possible, with Windows Bootloader, to boot into separate Operating Systems on a partitioned drive, as Windows Bootloader is incapable of recognizing partitions on a drive.\n",
                "I installed Ubuntu onto a partition on my SSD along side an install of Windows 7. However, after booting the PC continued to boot into Windows without offering Ubuntu as an option. I found a solution mentioned here (http://ubuntuforums.org/showthread.php?t=1499828&page=2) which uses GRUB2 for booting but I want to use the Windows bootloader. I found out about EasyBCD, wrote over the GRUB2 boot and followed the tutorials adding a Linux entry to the boot menu selecting the C:. I can then see both boot options in View Settings menu, but after restarting the computer it boots straight back into Windows and the options can no longer be seen in EasyBCD. I have no idea why this is going wrong, any help would be great, thanks in advance."
            ]
        }
    ],
    [
        {
            "id": "283-1-2",
            "pair": [
                "It's quite possible that the file was moved to that location from a different folder on the same volume. When you move a file from a different folder with different permissions to a folder in the same volume the original permissions are retained. \n",
                "This article is a bit old, but the same rules still generally apply as well as workarounds on how to change this behavior. \n"
            ]
        },
        {
            "id": "283-2-3",
            "pair": [
                "This article is a bit old, but the same rules still generally apply as well as workarounds on how to change this behavior. \n",
                "I manage a handful of servers connected to a much larger Active Directory network. All the Windows shares on our servers have a mixture of local account permissions and AD permissions.\n"
            ]
        },
        {
            "id": "283-3-4",
            "pair": [
                "I manage a handful of servers connected to a much larger Active Directory network. All the Windows shares on our servers have a mixture of local account permissions and AD permissions.\n",
                "In this particular case, something confusing has happened.\n"
            ]
        },
        {
            "id": "283-4-5",
            "pair": [
                "In this particular case, something confusing has happened.\n",
                "Has permissions GROUP1:Read, Administrators... etc.\n"
            ]
        },
        {
            "id": "283-5-6",
            "pair": [
                "Has permissions GROUP1:Read, Administrators... etc.\n",
                "Windows claims that USER1's permissions are inherited from the C: drive. Checking each folder downwards including the C: drive, the user's permissions do not exist in any folder.\n"
            ]
        },
        {
            "id": "283-6-7",
            "pair": [
                "Windows claims that USER1's permissions are inherited from the C: drive. Checking each folder downwards including the C: drive, the user's permissions do not exist in any folder.\n",
                "I was able to reset the permissions by copying and pasting the file (it did not inherit USER1!) Strange.\n"
            ]
        },
        {
            "id": "283-7-8",
            "pair": [
                "I was able to reset the permissions by copying and pasting the file (it did not inherit USER1!) Strange.\n",
                "Because USER1 had write access to this file, it caused some automation to fail. USER1 is part of GROUP1 and should only have read access. \n"
            ]
        },
        {
            "id": "283-8-9",
            "pair": [
                "Because USER1 had write access to this file, it caused some automation to fail. USER1 is part of GROUP1 and should only have read access. \n",
                "Can anyone explain this strange behaviour, and how I might prevent it in the future?"
            ]
        }
    ],
    [
        {
            "id": "284-1-2",
            "pair": [
                "It depends of what features you motherboard supports.  To expand a RAID array, it may be necessary to reallocate the data and resync the disks to accommodate the new array.  So I would not rely on it.\n",
                "Probably no.  Hardware RAID implementations are very hardware-dependent.  I doubt there is a standard on how to implement a RAID, so each vendor does its own implementation.  One should think the RAID levels are a low-level standard, but they are, in fact, a conceptual description of how the array organizes data.  The low-level implementation may be different so the RAID arrays are mostly not interchangeable.  If you implement a RAID array with a specific controller, that array will probably only work with that controller.\n"
            ]
        },
        {
            "id": "284-2-3",
            "pair": [
                "Probably no.  Hardware RAID implementations are very hardware-dependent.  I doubt there is a standard on how to implement a RAID, so each vendor does its own implementation.  One should think the RAID levels are a low-level standard, but they are, in fact, a conceptual description of how the array organizes data.  The low-level implementation may be different so the RAID arrays are mostly not interchangeable.  If you implement a RAID array with a specific controller, that array will probably only work with that controller.\n",
                "If one of your drives fail (remember that RAID-5 only supports the failure of one drive at once), there are two possible situations:\n"
            ]
        },
        {
            "id": "284-3-4",
            "pair": [
                "If one of your drives fail (remember that RAID-5 only supports the failure of one drive at once), there are two possible situations:\n",
                "I feel that for a home-based implementation of RAID, you should avoid hardware RAID implementation and rely on a RAID implementation based in software.  They are not that evil as many think they are (the performance gain are almost the same), they are easier to handle, and they are much more interchangeable with new hardware than any hardware implementation.  So think about it.  :-)\n"
            ]
        },
        {
            "id": "284-4-5",
            "pair": [
                "I feel that for a home-based implementation of RAID, you should avoid hardware RAID implementation and rely on a RAID implementation based in software.  They are not that evil as many think they are (the performance gain are almost the same), they are easier to handle, and they are much more interchangeable with new hardware than any hardware implementation.  So think about it.  :-)\n",
                "Windows offers software RAID implementations in Windows 10.  To build a RAID-5 you should use the layout \"Parity\".  I am just not sure if this feature is restricted for some editions of Windows (since you use Windows 10 Home edition), so you will need to check.\n"
            ]
        },
        {
            "id": "284-5-6",
            "pair": [
                "Windows offers software RAID implementations in Windows 10.  To build a RAID-5 you should use the layout \"Parity\".  I am just not sure if this feature is restricted for some editions of Windows (since you use Windows 10 Home edition), so you will need to check.\n",
                "I'm considering to build a RAID5 storage from three 8TB hard drives inside my PC being run under Windows 10 Home (if it matters). In my motherboard's manual I can see instructions how to setup RAID5 and it seems to be easy. My biggest concerns are:"
            ]
        }
    ],
    [
        {
            "id": "285-1-2",
            "pair": [
                "Yes, but it depends on which index key you search by. \n",
                "Think of it like one of those old \"white pages\" phone books.  In a phone book, people are ordered on the pages in the order LastName, FirstName.  That means there are two components to the phone book's index.  \n"
            ]
        },
        {
            "id": "285-2-3",
            "pair": [
                "Think of it like one of those old \"white pages\" phone books.  In a phone book, people are ordered on the pages in the order LastName, FirstName.  That means there are two components to the phone book's index.  \n",
                "If you're looking for all of the people with the last name of \"Smith\", you just find the first \"Smith\" (easy to do since it's in order), then keep reading until you see someone who's not a \"Smith\".  \n"
            ]
        },
        {
            "id": "285-3-4",
            "pair": [
                "If you're looking for all of the people with the last name of \"Smith\", you just find the first \"Smith\" (easy to do since it's in order), then keep reading until you see someone who's not a \"Smith\".  \n",
                "But if you're looking for all of people with the name \"William\", you're going to have a tough time.  You'll have to scan each and every entry in the phone book, collecting answers, even though FirstName is in the phone book's \"index\".\n"
            ]
        },
        {
            "id": "285-4-5",
            "pair": [
                "But if you're looking for all of people with the name \"William\", you're going to have a tough time.  You'll have to scan each and every entry in the phone book, collecting answers, even though FirstName is in the phone book's \"index\".\n",
                "Database indexes (conceptually) work the exact same way.  \n"
            ]
        },
        {
            "id": "285-5-6",
            "pair": [
                "Database indexes (conceptually) work the exact same way.  \n",
                "query planner should use index first_idx ( 'whatever' value is for example 5% of all records). It's faster because index is smaller so there is less read.\n"
            ]
        },
        {
            "id": "285-6-7",
            "pair": [
                "query planner should use index first_idx ( 'whatever' value is for example 5% of all records). It's faster because index is smaller so there is less read.\n",
                "it's obvious that index first_second_idx will be used\n"
            ]
        },
        {
            "id": "285-7-8",
            "pair": [
                "it's obvious that index first_second_idx will be used\n",
                "But if we remove first_idx in both queries planer should use first_second_idx. \n"
            ]
        },
        {
            "id": "285-8-9",
            "pair": [
                "But if we remove first_idx in both queries planer should use first_second_idx. \n",
                "If there is no index for this table full table scan will be triggered.\n"
            ]
        },
        {
            "id": "285-9-10",
            "pair": [
                "If there is no index for this table full table scan will be triggered.\n",
                "So if you have both types of queries from application:"
            ]
        }
    ],
    [
        {
            "id": "286-1-2",
            "pair": [
                "You should not re-invent the wheel here or you will only make more work for yourself in the long run and introduce new security issues. The two options that come to mind are:\n",
                "I would like to make a script which can maintain multiple servers via SSH. I want to control the authentication/authorization in such a manner that authentication is done by gateway and any other access is routed through this ssh server to internal services without any further authentication/authorization requirements.\n"
            ]
        },
        {
            "id": "286-2-3",
            "pair": [
                "I would like to make a script which can maintain multiple servers via SSH. I want to control the authentication/authorization in such a manner that authentication is done by gateway and any other access is routed through this ssh server to internal services without any further authentication/authorization requirements.\n",
                "So if a user A can log into server_1 for example. He can then ssh to server_2 without any other authentication and do what ever he is allowed to do on server_2 (like shut down mysql, upgrade it and restart it. This could be done via some remote shell script). \n"
            ]
        },
        {
            "id": "286-3-4",
            "pair": [
                "So if a user A can log into server_1 for example. He can then ssh to server_2 without any other authentication and do what ever he is allowed to do on server_2 (like shut down mysql, upgrade it and restart it. This could be done via some remote shell script). \n",
                "The problem that I am trying to solve is to come up with a deployment script for a JavaEE system which involves databases and tomcat instances. They need to be shutdown and re-spawned. The requirement is to have a deployment script which has minimal human interaction as possible for both developers and operation."
            ]
        }
    ],
    [
        {
            "id": "287-1-2",
            "pair": [
                "According to my experience the simplest and most detailed statistic tool you can install to trace mysterious system performance issues is http://freecode.com/projects/sysstat aka. sar\n",
                "for sure you want to look at iostat command output as well, specially how much is your %iowait should be below 5-10% under normal system load (below 1.0 or so). \n"
            ]
        },
        {
            "id": "287-2-3",
            "pair": [
                "for sure you want to look at iostat command output as well, specially how much is your %iowait should be below 5-10% under normal system load (below 1.0 or so). \n",
                "look at the ps output if in the STAT column you see D statuses that means those processes are locked and waiting for IO, very likely a hardware issue with the controller or the disk, check S.M.A.R.T stats as well as dmesg and syslog for clues\n"
            ]
        },
        {
            "id": "287-3-4",
            "pair": [
                "look at the ps output if in the STAT column you see D statuses that means those processes are locked and waiting for IO, very likely a hardware issue with the controller or the disk, check S.M.A.R.T stats as well as dmesg and syslog for clues\n",
                "check sar log and identify peak times if ever this happens and try to match those time with disk intensive cron jobs eg backups over network \n"
            ]
        },
        {
            "id": "287-4-5",
            "pair": [
                "check sar log and identify peak times if ever this happens and try to match those time with disk intensive cron jobs eg backups over network \n",
                "you can benchmark your disk performance with bonnie++ \n"
            ]
        },
        {
            "id": "287-5-6",
            "pair": [
                "you can benchmark your disk performance with bonnie++ \n",
                "I'm having some I/O problems on a couple of Linux systems that I administer. They manifest in that processes often block for up to several seconds in such simple syscalls as open(), unlink() or close() on files (which is a problem because some of the involved programs need rather low I/O latency to operate properly). It is true that the systems in question to experience some moderate I/O load, but I can hardly think it would be enough to justify such enormous latency times. Sometimes, the calls can take more than 15 seconds to complete (though more often they might take 1 or 2 or 3 seconds or so).\n"
            ]
        },
        {
            "id": "287-6-7",
            "pair": [
                "I'm having some I/O problems on a couple of Linux systems that I administer. They manifest in that processes often block for up to several seconds in such simple syscalls as open(), unlink() or close() on files (which is a problem because some of the involved programs need rather low I/O latency to operate properly). It is true that the systems in question to experience some moderate I/O load, but I can hardly think it would be enough to justify such enormous latency times. Sometimes, the calls can take more than 15 seconds to complete (though more often they might take 1 or 2 or 3 seconds or so).\n",
                "My question is: How can I find out why this happens? What I would like is some tool that could tell me what the processes in question are blocked by in the kernel, and why that which they sleep on is busy, what is happening with it, and such things. Is there such a tool, or is there some other way of trying to debug what happens?\n"
            ]
        },
        {
            "id": "287-7-8",
            "pair": [
                "My question is: How can I find out why this happens? What I would like is some tool that could tell me what the processes in question are blocked by in the kernel, and why that which they sleep on is busy, what is happening with it, and such things. Is there such a tool, or is there some other way of trying to debug what happens?\n",
                "Alternatively, of course, if you have any clue as to what actually is happening, how can it be avoided?\n"
            ]
        },
        {
            "id": "287-8-9",
            "pair": [
                "Alternatively, of course, if you have any clue as to what actually is happening, how can it be avoided?\n",
                "Now in due time, I have managed to solve this myself, so I can at least follow up on it myself for posterity.\n"
            ]
        },
        {
            "id": "287-9-10",
            "pair": [
                "Now in due time, I have managed to solve this myself, so I can at least follow up on it myself for posterity.\n",
                "Unfortunately, I lost the original problem in a kernel upgrade, but gained a new one instead, even worse in performance, and just as hard to track down. The techniques I found were the following:\n"
            ]
        },
        {
            "id": "287-10-11",
            "pair": [
                "Unfortunately, I lost the original problem in a kernel upgrade, but gained a new one instead, even worse in performance, and just as hard to track down. The techniques I found were the following:\n",
                "First of all, blktrace/blkparse is a tool that I found quite helpful. It allows the tracing of the progress of individual I/O requests with many helpful details, such as the process that submitted the request. It is helpful to put the output on tmpfs, so that the handling of the storage of the trace doesn't start tracing itself.\n"
            ]
        },
        {
            "id": "287-11-12",
            "pair": [
                "First of all, blktrace/blkparse is a tool that I found quite helpful. It allows the tracing of the progress of individual I/O requests with many helpful details, such as the process that submitted the request. It is helpful to put the output on tmpfs, so that the handling of the storage of the trace doesn't start tracing itself.\n",
                "That helped only so far, though, so I compiled a kernel with more debugging functionality. In particular, I found ftrace quite helpful, since it allowed me to trace the poorly performing process inside kernel space, to see what it did and where it blocked. Compiling a debug kernel also provides working WCHAN output for ps as well, which can work as an easier way to see what a process is doing inside the kernel, at least for simpler cases.\n"
            ]
        },
        {
            "id": "287-12-13",
            "pair": [
                "That helped only so far, though, so I compiled a kernel with more debugging functionality. In particular, I found ftrace quite helpful, since it allowed me to trace the poorly performing process inside kernel space, to see what it did and where it blocked. Compiling a debug kernel also provides working WCHAN output for ps as well, which can work as an easier way to see what a process is doing inside the kernel, at least for simpler cases.\n",
                "I was also hoping for LatencyTop to be useful, but I found it quite buggy, and also that it only displayed latency reasons that were too \"high-level\" to be truly useful, unfortunately.\n"
            ]
        },
        {
            "id": "287-13-14",
            "pair": [
                "I was also hoping for LatencyTop to be useful, but I found it quite buggy, and also that it only displayed latency reasons that were too \"high-level\" to be truly useful, unfortunately.\n",
                "Also, I found it more helpful than iostat to simply view the contents of /sys/block/$DEVICE/stat at very close intervals, simply like this:\n"
            ]
        },
        {
            "id": "287-14-15",
            "pair": [
                "Also, I found it more helpful than iostat to simply view the contents of /sys/block/$DEVICE/stat at very close intervals, simply like this:\n",
                "See Documentation/iostats.txt in the kernel source tree for the format of the stat file. Viewing it at close intervals allowed me to see the exact timing and size of I/O bursts and such things.\n"
            ]
        },
        {
            "id": "287-15-16",
            "pair": [
                "See Documentation/iostats.txt in the kernel source tree for the format of the stat file. Viewing it at close intervals allowed me to see the exact timing and size of I/O bursts and such things.\n",
                "In the end, I found out that the problem I had after the kernel upgrade was caused by stable pages, a feature introduced in Linux 3.0, causing, in my case, Berkeley DB to halt for extended periods when dirtying pages in its mmap'ed region files. While it seems possible to patch this feature out, and also that the problems it causes might be fixed in Linux 3.9, I have solved the worst problem I had for now by patching Berkeley DB to allow me to put its region files in a different directory (in my case /dev/shm), allowing me to avoid the problem altogether."
            ]
        }
    ],
    [
        {
            "id": "288-1-2",
            "pair": [
                "I just bought a Dell P2715Q, UHD 10-bit depth monitor, but in my Nvidia control panel (both linux & windows) it will only offer a maximum of 8bpc (24/32 bit depth) setting which will not do.\n",
                "After a bit of digging I found a potential solution which involves creating a custom EDID file that adds support for the 10 bit configuration. Which can be done on Windows by AMD gpu users, and Nvidia Quadro and NVS users however EDID configurations are disabled for GTX cards (for apparently no good reason) and have never been available for iGPUs on windows either.\n"
            ]
        },
        {
            "id": "288-2-3",
            "pair": [
                "After a bit of digging I found a potential solution which involves creating a custom EDID file that adds support for the 10 bit configuration. Which can be done on Windows by AMD gpu users, and Nvidia Quadro and NVS users however EDID configurations are disabled for GTX cards (for apparently no good reason) and have never been available for iGPUs on windows either.\n",
                "However under Linux (even with the official Nvidia drivers) EDID data can be acquired and edited, so I tried doing what it said on that forum (changing the value of a5 to b5) and loading that edited edid on Linux but that did not work (it apparently failed to load the EDID after the edit) so that solution may in fact not work after all.\n"
            ]
        },
        {
            "id": "288-3-4",
            "pair": [
                "However under Linux (even with the official Nvidia drivers) EDID data can be acquired and edited, so I tried doing what it said on that forum (changing the value of a5 to b5) and loading that edited edid on Linux but that did not work (it apparently failed to load the EDID after the edit) so that solution may in fact not work after all.\n",
                "Therefore my question is, how do I force an Nvidia GTX graphics card to set it's color depth to 10 bits per channel on a monitor whose edid (falsely) claims only to support 8bpc under windows 7?\n"
            ]
        },
        {
            "id": "288-4-5",
            "pair": [
                "Therefore my question is, how do I force an Nvidia GTX graphics card to set it's color depth to 10 bits per channel on a monitor whose edid (falsely) claims only to support 8bpc under windows 7?\n",
                "(Bonus points if you can tell me how to do it under linux too but that is not required, I'm sure I can figure something out there eventually)\n"
            ]
        },
        {
            "id": "288-5-6",
            "pair": [
                "(Bonus points if you can tell me how to do it under linux too but that is not required, I'm sure I can figure something out there eventually)\n",
                "I dont know I have the same problem with anything above 8 bit. Windows cant tell it is 10 but accept for a display model Asus predator I saw once... Windows should just let people force the settings 9 times out of 10 windows is screwing up any ways. And yes the nvidia control panel will allow setting 10 bit but I believe but I'm not sure it disables your color profile and takes over...? Which is annoying at times bevause Nvidia Control Panel does not work with monitor color profiles! And since theirs not official information about how it manager color other than the manual settings in the controle panel this is a serious issue that will probably not get an aswer...\n"
            ]
        },
        {
            "id": "288-6-7",
            "pair": [
                "I dont know I have the same problem with anything above 8 bit. Windows cant tell it is 10 but accept for a display model Asus predator I saw once... Windows should just let people force the settings 9 times out of 10 windows is screwing up any ways. And yes the nvidia control panel will allow setting 10 bit but I believe but I'm not sure it disables your color profile and takes over...? Which is annoying at times bevause Nvidia Control Panel does not work with monitor color profiles! And since theirs not official information about how it manager color other than the manual settings in the controle panel this is a serious issue that will probably not get an aswer...\n",
                "The GPU is where you plug in your monitor or TV. The two peices of hardware are directly connected and the nvidia controle panel can get the information about your monitor from the GPU directly through the Nvidia graphics driver bypassing the Monitor driver all to gether. Your monitor has a hardware id and so does your graphics card. Plug and play hardware like a monitor transmits its hardware id through its connection. DP and HDMI are data conections all be it high bandwidth specialized for video they are still data connections. It is critical for monitor opperation for the GPU and monitor to be able to comunicate on more than a one way color transmission or for instance features like vsync would not work. That is why the Nvidia driver and GPU can recognise 10 bit + capability and allow the apropriate setting to be availble even when windows fails. The problem is  simply Windows and the lack of support for hardware from microsoft. This is also microsoft thinking your to stupid to know you have a 10 bit display and assuming if they let you control the setting you will brake the display, crash the driver or have a bad experiance and in turn blame Windows aka the product. Not that transmiting 10 bit video to an 8 bit panel will brake it. You might get strange effects or a blank screen but this is easily solved by the settings automaticly reverting if the user hasn't cleard the prompt with in 15 seconds."
            ]
        }
    ],
    [
        {
            "id": "289-1-2",
            "pair": [
                "Since your map is a grid of fixed size and all you want is \"has been visited\", for the first part you can use a bit-map.  For a 1024x1024 grid your storage size would be 128K and very fast to look up and small enough to keep in memory if you need super fast access (for fog of war rendering, or info overlays).\n",
                "The quest info could be done two ways.  It would be pretty easy to store them in a database and run queries as needed, and probably the best solution overall, @Dan Grossman\n"
            ]
        },
        {
            "id": "289-2-3",
            "pair": [
                "The quest info could be done two ways.  It would be pretty easy to store them in a database and run queries as needed, and probably the best solution overall, @Dan Grossman\n",
                "The other method would be to generate a hash from the quest information then simply store a list of hashes that the player has completed.  Storage would grow dynamically, hash lookup at run time is very fast, and new quests could be added whenever you wanted.\n"
            ]
        },
        {
            "id": "289-3-4",
            "pair": [
                "The other method would be to generate a hash from the quest information then simply store a list of hashes that the player has completed.  Storage would grow dynamically, hash lookup at run time is very fast, and new quests could be added whenever you wanted.\n",
                "A database is an excellent tool for something like this, storing it in any other way would force you to (partially) recreate what a database already solves for you. As was mentioned by Dan in comments, using a relational table that maps the players id to the cell id that has been visited is a good solution. Databases are built for just this sort of thing and querying it for if a cell has been visited is a fast operation.\n"
            ]
        },
        {
            "id": "289-4-5",
            "pair": [
                "A database is an excellent tool for something like this, storing it in any other way would force you to (partially) recreate what a database already solves for you. As was mentioned by Dan in comments, using a relational table that maps the players id to the cell id that has been visited is a good solution. Databases are built for just this sort of thing and querying it for if a cell has been visited is a fast operation.\n",
                "If database storage is costly for you (which it might be on a hosted solution) you can consider compressing the number of fields somewhat. In the case of \"has this cell been visited\" you would normally be forced to travel through the areas sequentially, so to get from A1 to A3 you would have to pass through A2 (or go a path around A2 depending on terrain).\n"
            ]
        },
        {
            "id": "289-5-6",
            "pair": [
                "If database storage is costly for you (which it might be on a hosted solution) you can consider compressing the number of fields somewhat. In the case of \"has this cell been visited\" you would normally be forced to travel through the areas sequentially, so to get from A1 to A3 you would have to pass through A2 (or go a path around A2 depending on terrain).\n",
                "You can use this fact by storing strips into the database instead of each cell individually, for instance (player, row, firstHorizontal, lastHorizontal), this way you can find out if a player has been in a cell by:\n"
            ]
        },
        {
            "id": "289-6-7",
            "pair": [
                "You can use this fact by storing strips into the database instead of each cell individually, for instance (player, row, firstHorizontal, lastHorizontal), this way you can find out if a player has been in a cell by:\n",
                "The update method would be more complicated in this case however, you would have to look for a strip that is next to the current cell and if one is found, update either first or lastHorizontal field as appropriate, or if one isn't found you add a new strip where first and lastHorizontal is the current location.\n"
            ]
        },
        {
            "id": "289-7-8",
            "pair": [
                "The update method would be more complicated in this case however, you would have to look for a strip that is next to the current cell and if one is found, update either first or lastHorizontal field as appropriate, or if one isn't found you add a new strip where first and lastHorizontal is the current location.\n",
                "Quests can be solved in a similar way, (playerID, questID, progress), progress can either be a foreign key to a \"QuestSteps\" table or you could go the oblivion/skyrim approach and store a number from 0 to 100 for every quest that maps to what you have done so far, for instance:\n"
            ]
        },
        {
            "id": "289-8-9",
            "pair": [
                "Quests can be solved in a similar way, (playerID, questID, progress), progress can either be a foreign key to a \"QuestSteps\" table or you could go the oblivion/skyrim approach and store a number from 0 to 100 for every quest that maps to what you have done so far, for instance:\n",
                "Again, if you wish to reduce the number of fields per player you can lump quests together by area, in many cases the player would probably finish all quests in a region before moving on to the next. In this case you can have a RegionCompleted (playerID, regionID), while a player is working on the quests in a region this is blank and you check the \"QuestCompetions\" (playerID, questID, progress) table for each individual quest, once all quest are completed however you add a row to the \"RegionCompleted\" table to mark this, and then remove the rows that correspond to that region from the \"QuestCompletions\"."
            ]
        }
    ],
    [
        {
            "id": "29-1-2",
            "pair": [
                "It really depends on how much data is going to flow through the transaction log.  Look at how big the log gets today.  You need to configure the log to be at least that size when SQL starts up.  For most of my clients they end up with a 3-4 Gig transaction log for the tempdb, which contains just a few VLFs and everything works nice and smoothly.\n",
                "Aaron is right configuring TempDB is dependent on lot of variables such as whether or not you plan on using snapshot isolation etc. Here is a older SQL 2005 white paper from MS/TN that might help you out since a good bit of it still applies. Specifically have a look at the \"Space required for tempdb logging\" so you can see what kinds transactions are logged in the tempdb log so you can set things up accordingly. It is likely going to be the sort of thing that you will have to monitor and tweak over time.\n"
            ]
        },
        {
            "id": "29-2-3",
            "pair": [
                "Aaron is right configuring TempDB is dependent on lot of variables such as whether or not you plan on using snapshot isolation etc. Here is a older SQL 2005 white paper from MS/TN that might help you out since a good bit of it still applies. Specifically have a look at the \"Space required for tempdb logging\" so you can see what kinds transactions are logged in the tempdb log so you can set things up accordingly. It is likely going to be the sort of thing that you will have to monitor and tweak over time.\n",
                "As far as authogrowth for tempdb I have reluctantly disabled this on a reporting server in the past and the behavior I have experienced was that it caused the long running transaction to abort and roll back which instantly freed up the log space but that might depend on the type of transaction which in my case is was massive sorts and join operations (poorly written report queries). I agree with Aaron and would recommend avoiding this if possible especially in high transaction volume situations. "
            ]
        }
    ],
    [
        {
            "id": "290-1-2",
            "pair": [
                "Every keyboard wears out with time, and mine has some age already. The day it fails is coming closer and closer.\n",
                "So I'm slowly starting to look around for a new one. I use the keyboard for gaming and programming, so it gets some pretty solid use. I also tend to eat by the computer, so there's plenty of... uhh... lifeforms down there.\n"
            ]
        },
        {
            "id": "290-2-3",
            "pair": [
                "So I'm slowly starting to look around for a new one. I use the keyboard for gaming and programming, so it gets some pretty solid use. I also tend to eat by the computer, so there's plenty of... uhh... lifeforms down there.\n",
                "Anyway, I was looking at these rubber keyboards. They come pretty cheap (my local computer shop has one for less than $20) and they seem to have some nice properties. They can be easily cleaned, they're quiet, and can be rolled up when needed (plus no worries about spilled drinks).\n"
            ]
        },
        {
            "id": "290-3-4",
            "pair": [
                "Anyway, I was looking at these rubber keyboards. They come pretty cheap (my local computer shop has one for less than $20) and they seem to have some nice properties. They can be easily cleaned, they're quiet, and can be rolled up when needed (plus no worries about spilled drinks).\n",
                "However I'm wondering what their type-ability is. If I can't write on it at a decent speed, the rest of the features don't matter. Not that I'm a fast typer, but being a professional progammer does give a boost to the skill.\n"
            ]
        },
        {
            "id": "290-4-5",
            "pair": [
                "However I'm wondering what their type-ability is. If I can't write on it at a decent speed, the rest of the features don't matter. Not that I'm a fast typer, but being a professional progammer does give a boost to the skill.\n",
                "I couldn't find any reviews on the net so I'm turning to you. Who has used these keyboards and what was your experience? Perhaps there is something else I haven't though of why such a keyboard would not be a good idea?\n"
            ]
        },
        {
            "id": "290-5-6",
            "pair": [
                "I couldn't find any reviews on the net so I'm turning to you. Who has used these keyboards and what was your experience? Perhaps there is something else I haven't though of why such a keyboard would not be a good idea?\n",
                "I was once optimistic about them for the reasons you listed, but found them to be absolutely horrible in use. The keys quite literally wobble back and forth on their downward stroke, so unless you enjoy battling little mounds of jelly, avoid.\n"
            ]
        },
        {
            "id": "290-6-7",
            "pair": [
                "I was once optimistic about them for the reasons you listed, but found them to be absolutely horrible in use. The keys quite literally wobble back and forth on their downward stroke, so unless you enjoy battling little mounds of jelly, avoid.\n",
                "You should go to a hardware outlet and give it a shot. I really can't recommend the things for anybody. I think typing would be marginally faster than an on-screen keyboard.\n"
            ]
        },
        {
            "id": "290-7-8",
            "pair": [
                "You should go to a hardware outlet and give it a shot. I really can't recommend the things for anybody. I think typing would be marginally faster than an on-screen keyboard.\n",
                "There are probably no reviews because the things cost a few bucks and nobody loves them enough to express it on a review site. Simply another buck or two thrown away."
            ]
        }
    ],
    [
        {
            "id": "291-1-2",
            "pair": [
                "You're right that this isn't normal. For diagnosing problems like this, my first line of diagnosis under Windows is a combination of Procmon and Process Explorer, both free tools from the Sysinternals suite. \n",
                "Do a Google for Sysinternals and download the suite from Microsoft. Procmon will tell you what processes are accessing what items in the filesystem and keys in the registry, while Process Explorer is like Task Managers on super-steroids. Plus the Sysinternals suite includes a lot of other handy extras.\n"
            ]
        },
        {
            "id": "291-2-3",
            "pair": [
                "Do a Google for Sysinternals and download the suite from Microsoft. Procmon will tell you what processes are accessing what items in the filesystem and keys in the registry, while Process Explorer is like Task Managers on super-steroids. Plus the Sysinternals suite includes a lot of other handy extras.\n",
                "These tools don't always point out what the problem is, but they might help you narrow down what is causing the issue.\n"
            ]
        },
        {
            "id": "291-3-4",
            "pair": [
                "These tools don't always point out what the problem is, but they might help you narrow down what is causing the issue.\n",
                "Hi since monday morning my apache server started using %100 of cpu and all the sites are responding so slow. I know it's not a good idea to use windows server 2003 and apache server together but I had to use windows. I tryed to check the status with apache status, but it does'nt show which proccess uses how much cpu (there is no cpu usage column in my apache status report). According to apache status I get mostly 2-3 requests/second. My server is VDS(IBM blade server) with 1 gb memory and 40 gb hdd. I had no problems last saturday everyting started modnay morning, Please help me to identify the problem. What tools can I use to find out what's the problem."
            ]
        }
    ],
    [
        {
            "id": "292-1-2",
            "pair": [
                "I have some tables in my MS word docs. Many times when the table spans multiple pages and when there is an equation in the cell on the boundary on two pages, the table's bottom boundary will not simply move down completely to the bottom of the first page to display all equations that are there in the cell on the first page. This can be seen in the screenshot below:\n",
                "Above there is an equation in the cell in the first page. The cell continues on 2nd page also. As you can see, the bottom boundary of the table on first page is not moved completely to the bottom of the first page thus hiding the equations. However, I can click inside hidden equations and also hover on them. The equation body/frame/container responds to these mouse events, but the equation itself is not shown, only the equations body/frame/container is shown/highlighted. I usually do Enter, that is put new lines, so that the whole equation is moved to new page but inside same cell.\n"
            ]
        },
        {
            "id": "292-2-3",
            "pair": [
                "Above there is an equation in the cell in the first page. The cell continues on 2nd page also. As you can see, the bottom boundary of the table on first page is not moved completely to the bottom of the first page thus hiding the equations. However, I can click inside hidden equations and also hover on them. The equation body/frame/container responds to these mouse events, but the equation itself is not shown, only the equations body/frame/container is shown/highlighted. I usually do Enter, that is put new lines, so that the whole equation is moved to new page but inside same cell.\n",
                "Select the row with the equation that is pushed off of the page and then from the Table Layout tab select Properties > Row and tick the box to allow row to break across pages."
            ]
        }
    ],
    [
        {
            "id": "293-1-2",
            "pair": [
                "If it's complaining about services, then your best option is to look at the windows services.\n",
                "Try opening your start menu, typing services.msc and hitting enter. This brings up the service management window. From here you can look for any audio related services that are disabled and enable them. After that restart your PC and see if it works. If it doesn't, disable it again and try any others.\n"
            ]
        },
        {
            "id": "293-2-3",
            "pair": [
                "Try opening your start menu, typing services.msc and hitting enter. This brings up the service management window. From here you can look for any audio related services that are disabled and enable them. After that restart your PC and see if it works. If it doesn't, disable it again and try any others.\n",
                "Don't enable them all at once - go through them one by one until you find which one to keep. it is a long process, but keep in mind too many unnecessary services can really slow your PC down.\n"
            ]
        },
        {
            "id": "293-3-4",
            "pair": [
                "Don't enable them all at once - go through them one by one until you find which one to keep. it is a long process, but keep in mind too many unnecessary services can really slow your PC down.\n",
                "A number of people had the same problem and it was caused by AVG quarantining an audio file after the most recent Windows update. Restoring the files from the Virus Vault, and rebooting solved the problem. However, I inadvertently missed restoring some of the quarantined files having deleted them so my fix was to completely uninstall AVG, reboot, then back up my recents and do a system restore to before the update. And sound is back! Hope this helps others with the same issue. Thanks everyone for your help!"
            ]
        }
    ],
    [
        {
            "id": "294-1-2",
            "pair": [
                "If the VPN is configured so that only traffic to the office goes through it, tracking is unlikely - but beware of DNS servers - if these are reassigned to the VPN then they can see what you are looking up.\n",
                "If the VPN is configured to push a default route, all traffic is directed through the VPN and could be intercepted. (https traffic is hard to intercept, but not impossible if a root certificate has been added to your PC)\n"
            ]
        },
        {
            "id": "294-2-3",
            "pair": [
                "If the VPN is configured to push a default route, all traffic is directed through the VPN and could be intercepted. (https traffic is hard to intercept, but not impossible if a root certificate has been added to your PC)\n",
                "You can get a feel for how traffic is routed by doing traceroutes and working out how it goes, and/or using the route command and getting the same info. (More accurate but harder to understand)\n"
            ]
        },
        {
            "id": "294-3-4",
            "pair": [
                "You can get a feel for how traffic is routed by doing traceroutes and working out how it goes, and/or using the route command and getting the same info. (More accurate but harder to understand)\n",
                "None of this allows them to look into your hard drive or see active programs on your local PC, so if you close all personal applications while looking at the VPN, and don't have any file shares on your PC, you should be OK (unless they take specific  measures to hack into your PC - which would be easier across a VPN but by no means trivial.)\n"
            ]
        },
        {
            "id": "294-4-5",
            "pair": [
                "None of this allows them to look into your hard drive or see active programs on your local PC, so if you close all personal applications while looking at the VPN, and don't have any file shares on your PC, you should be OK (unless they take specific  measures to hack into your PC - which would be easier across a VPN but by no means trivial.)\n",
                "I often connect to my work computer remotely when I am at home using my home PC(the work computer is at home with me on internal IP 192.168.xx.xx).\n"
            ]
        },
        {
            "id": "294-5-6",
            "pair": [
                "I often connect to my work computer remotely when I am at home using my home PC(the work computer is at home with me on internal IP 192.168.xx.xx).\n",
                "I then connect to VPN from my RDPed work computer.\n"
            ]
        },
        {
            "id": "294-6-7",
            "pair": [
                "I then connect to VPN from my RDPed work computer.\n",
                "My question is can my employer track my activity on home computer which I am using to Remote desktop in to tthe work computer."
            ]
        }
    ],
    [
        {
            "id": "295-1-2",
            "pair": [
                "I have a really weird problem with my DNS. My domain name (strugee.net) is unresolvable from some networks, and resolvable from others.\n",
                "For example, on my home network (same network the server's on):\n"
            ]
        },
        {
            "id": "295-2-3",
            "pair": [
                "For example, on my home network (same network the server's on):\n",
                "However, if I log in to a server I have on Digital Ocean, the domain fails to resolve:\n"
            ]
        },
        {
            "id": "295-3-4",
            "pair": [
                "However, if I log in to a server I have on Digital Ocean, the domain fails to resolve:\n",
                "But, going directly to the authoritative nameservers works just fine:\n"
            ]
        },
        {
            "id": "295-4-5",
            "pair": [
                "But, going directly to the authoritative nameservers works just fine:\n",
                "It's pretty clear that there's a problem with some large network somewhere that's failing to resolve my domain, but I can't seem to figure out where. I skimmed the dig manpage for options that might help, but didn't find anything particularly useful.\n"
            ]
        },
        {
            "id": "295-5-6",
            "pair": [
                "It's pretty clear that there's a problem with some large network somewhere that's failing to resolve my domain, but I can't seem to figure out where. I skimmed the dig manpage for options that might help, but didn't find anything particularly useful.\n",
                "I'm on Namecheap both as a domain registrar as well as DNS hosting. I have the DNSSEC option turned on. I haven't made any changes to my DNS settings recently.\n"
            ]
        },
        {
            "id": "295-6-7",
            "pair": [
                "I'm on Namecheap both as a domain registrar as well as DNS hosting. I have the DNSSEC option turned on. I haven't made any changes to my DNS settings recently.\n",
                "How can I debug this problem and find the offending nameserver?\n"
            ]
        },
        {
            "id": "295-7-8",
            "pair": [
                "How can I debug this problem and find the offending nameserver?\n",
                "While you are indeed seeing that the authoritative name servers are responding correctly, you need to follow up the entire chain of DNS resolution. This is, walk down the whole DNS hierachy from the root servers up.\n"
            ]
        },
        {
            "id": "295-8-9",
            "pair": [
                "While you are indeed seeing that the authoritative name servers are responding correctly, you need to follow up the entire chain of DNS resolution. This is, walk down the whole DNS hierachy from the root servers up.\n",
                "This basically checks that the public DNS servers are working, and you're doing the same thing that your DNS resolver should be doing. So you should be getting the same answers as above in your Digital Ocean server unless something's wrong with their DNS resolver:\n"
            ]
        },
        {
            "id": "295-9-10",
            "pair": [
                "This basically checks that the public DNS servers are working, and you're doing the same thing that your DNS resolver should be doing. So you should be getting the same answers as above in your Digital Ocean server unless something's wrong with their DNS resolver:\n",
                "If the first two queries fail, it's the DNS on Digital Ocean's side failing. Check your /etc/resolv.conf and try querying the secondary DNS server. If the secondary one works, just switch the order for resolvers and try again."
            ]
        }
    ],
    [
        {
            "id": "296-1-2",
            "pair": [
                "There are short term and medium term ways to troubleshoot this problem.  \n",
                "In addition, if you want useful help from this site, I would suggest providing more information. Your site configuration files, logfiles, and the error messages you saw when running commands. e.g. cut and paste the text of the output from the graceful restart command\n"
            ]
        },
        {
            "id": "296-2-3",
            "pair": [
                "In addition, if you want useful help from this site, I would suggest providing more information. Your site configuration files, logfiles, and the error messages you saw when running commands. e.g. cut and paste the text of the output from the graceful restart command\n",
                "The best way to troubleshoot a problem that has happened previously, and is not happening now, is through logfiles.  \n"
            ]
        },
        {
            "id": "296-3-4",
            "pair": [
                "The best way to troubleshoot a problem that has happened previously, and is not happening now, is through logfiles.  \n",
                "The main apache2 error logfile is at /var/log/apache2/error.log \n"
            ]
        },
        {
            "id": "296-4-5",
            "pair": [
                "The main apache2 error logfile is at /var/log/apache2/error.log \n",
                "and you may have a VirtualHost specific error log configured;\n"
            ]
        },
        {
            "id": "296-5-6",
            "pair": [
                "and you may have a VirtualHost specific error log configured;\n",
                "Errors relating to service restarts will be logged to the journal;\n"
            ]
        },
        {
            "id": "296-6-7",
            "pair": [
                "Errors relating to service restarts will be logged to the journal;\n",
                "To look at a particular period of time, use --since and --until\n"
            ]
        },
        {
            "id": "296-7-8",
            "pair": [
                "To look at a particular period of time, use --since and --until\n",
                "Your description suggests some sort of resource exhaustion problem, which accumulates over time. So either memory, file descriptors, or potentially apache is unable to serve the requests due to lack of cpu, io, etc and they queue and timeout.\n"
            ]
        },
        {
            "id": "296-8-9",
            "pair": [
                "Your description suggests some sort of resource exhaustion problem, which accumulates over time. So either memory, file descriptors, or potentially apache is unable to serve the requests due to lack of cpu, io, etc and they queue and timeout.\n",
                "So generally its useful to track these values, using some tool installed on the box. Personally I would use munin, because I am familiar with it, but it's quite old but it will do the trick.\n"
            ]
        },
        {
            "id": "296-9-10",
            "pair": [
                "So generally its useful to track these values, using some tool installed on the box. Personally I would use munin, because I am familiar with it, but it's quite old but it will do the trick.\n",
                "Another tool to track cpu, io, memory, is the sysstat package, which will log useful system statistics, which you can compare to your downtime periods.\n"
            ]
        },
        {
            "id": "296-10-11",
            "pair": [
                "Another tool to track cpu, io, memory, is the sysstat package, which will log useful system statistics, which you can compare to your downtime periods.\n",
                "I have a client with an ecommerce site and what the issue is Apache stops randomly like once in 3-7 days. We then get from 20mins - 2 hours of downtime until we get to know this through uptime robot. And it is not the problem of the network as we have tried 3 different hosts - Contabo, OVH, Shinjiru. Graceful restart fails. I have been searching for the cause on forums but till now found nothing. \n"
            ]
        },
        {
            "id": "296-11-12",
            "pair": [
                "I have a client with an ecommerce site and what the issue is Apache stops randomly like once in 3-7 days. We then get from 20mins - 2 hours of downtime until we get to know this through uptime robot. And it is not the problem of the network as we have tried 3 different hosts - Contabo, OVH, Shinjiru. Graceful restart fails. I have been searching for the cause on forums but till now found nothing. \n",
                "Even tried fixing by deleting lines from logrotate by seeing this tutorial  Apache automatically stops each sunday. Why? but still no luck! \n"
            ]
        },
        {
            "id": "296-12-13",
            "pair": [
                "Even tried fixing by deleting lines from logrotate by seeing this tutorial  Apache automatically stops each sunday. Why? but still no luck! \n",
                "Apache logs too don't tell anything. We have done the resetup 4-5 times but still. \n"
            ]
        },
        {
            "id": "296-13-14",
            "pair": [
                "Apache logs too don't tell anything. We have done the resetup 4-5 times but still. \n",
                "We are using php7.0 with SOAP module with a Wordpress site powered with Woocommerce. Admin panel is Webmin and operating system is ubuntu 16.04. \n"
            ]
        },
        {
            "id": "296-14-15",
            "pair": [
                "We are using php7.0 with SOAP module with a Wordpress site powered with Woocommerce. Admin panel is Webmin and operating system is ubuntu 16.04. \n",
                "client complains a lot as a lot of clients have been lost as google ads delists the site if it finds it to be down and it takes 3-4 days to again get the listing but then again apache issue. please help "
            ]
        }
    ],
    [
        {
            "id": "297-1-2",
            "pair": [
                "You can create a (fake?) chart with exactly the appearance that you want:\n",
                "You can use variations on this. For instance, you can add extra points to your dummy series, with corresponding labels. Gridlines would match the dummy series.\n"
            ]
        },
        {
            "id": "297-2-3",
            "pair": [
                "You can use variations on this. For instance, you can add extra points to your dummy series, with corresponding labels. Gridlines would match the dummy series.\n",
                "You can use this technique to create an arbitrary number of axis interruptions. The formula for the \"fake\" Y-values would be more complicated, with IFs to detect the interval corresponding to each point, and suitable linear transformations to account for the change in scale for each interval (assuming linear scales; no mixing linear-log). But that is all.\n"
            ]
        },
        {
            "id": "297-3-4",
            "pair": [
                "You can use this technique to create an arbitrary number of axis interruptions. The formula for the \"fake\" Y-values would be more complicated, with IFs to detect the interval corresponding to each point, and suitable linear transformations to account for the change in scale for each interval (assuming linear scales; no mixing linear-log). But that is all.\n",
                "PS: see also the links below. I still think my alternative is better.\n"
            ]
        },
        {
            "id": "297-4-5",
            "pair": [
                "PS: see also the links below. I still think my alternative is better.\n",
                "http://peltiertech.com/broken-y-axis-in-excel-chart/\n"
            ]
        },
        {
            "id": "297-5-6",
            "pair": [
                "http://peltiertech.com/broken-y-axis-in-excel-chart/\n",
                "http://ksrowell.com/blog-visualizing-data/2013/08/12/how-to-simulate-a-broken-axis-value-axis/\n"
            ]
        },
        {
            "id": "297-6-7",
            "pair": [
                "http://ksrowell.com/blog-visualizing-data/2013/08/12/how-to-simulate-a-broken-axis-value-axis/\n",
                "http://www.tushar-mehta.com/excel/newsgroups/broken_y_axis/tutorial/index.html#Rescale%20and%20hide%20the%20y-axis\n"
            ]
        },
        {
            "id": "297-7-8",
            "pair": [
                "http://www.tushar-mehta.com/excel/newsgroups/broken_y_axis/tutorial/index.html#Rescale%20and%20hide%20the%20y-axis\n",
                "I agree with @JonPeltier's point of doing this being a bad idea, conceptually.  However, if you must, there is a simple solution.  \n"
            ]
        },
        {
            "id": "297-8-9",
            "pair": [
                "I agree with @JonPeltier's point of doing this being a bad idea, conceptually.  However, if you must, there is a simple solution.  \n",
                "Graph the entire range and save or capture the graph as an image.  Open the image in an image manipulation program, like Photoshop or GIMP.  Then cut out the area you don't want, move the upper range closer, and add jagged lines to the Y axis at the edges of the break to indicate the discontinuity.  "
            ]
        }
    ],
    [
        {
            "id": "298-1-2",
            "pair": [
                "I'm planning on setting a dual-boot machine with Windows (8.1) and Debian sitting on different partitions of an SSD.\n",
                "The problem here is my 1.5TB storage drive. I'm planning to use Windows and Debian equally, so I need to format this drive in a way that it works as good as possible on both systems. There's also an additional requirement: Windows should be able to flawlessly work with user folders moved to the storage drive.\n"
            ]
        },
        {
            "id": "298-2-3",
            "pair": [
                "The problem here is my 1.5TB storage drive. I'm planning to use Windows and Debian equally, so I need to format this drive in a way that it works as good as possible on both systems. There's also an additional requirement: Windows should be able to flawlessly work with user folders moved to the storage drive.\n",
                "1) FAT32 is not actually an option, but worth mentioning for others to see, if your storage partition is 32GB or smaller than this is perfect;\n"
            ]
        },
        {
            "id": "298-3-4",
            "pair": [
                "1) FAT32 is not actually an option, but worth mentioning for others to see, if your storage partition is 32GB or smaller than this is perfect;\n",
                "2) NTFS. Both Windows and Linux can work with this filesystem, but Linux will probably show very high CPU usage while working with NTFS partitions;\n"
            ]
        },
        {
            "id": "298-4-5",
            "pair": [
                "2) NTFS. Both Windows and Linux can work with this filesystem, but Linux will probably show very high CPU usage while working with NTFS partitions;\n",
                "3) Ext4 seems to be the best option, but I can't find any info about how well does Windows handle it. Ext2sd, as well as Paragon software, don't have any information on performance, I'm afraid of it being worse than NTFS in Linux. Also I have no idea how well does it combine with Windows user folders.\n"
            ]
        },
        {
            "id": "298-5-6",
            "pair": [
                "3) Ext4 seems to be the best option, but I can't find any info about how well does Windows handle it. Ext2sd, as well as Paragon software, don't have any information on performance, I'm afraid of it being worse than NTFS in Linux. Also I have no idea how well does it combine with Windows user folders.\n",
                "Are there any other options I've overlooked? Perhaps, NTFS isn't that bad on a fairly powerful machine? Is EXT4 perfect for this scenario, or it's going to a huge pain to make it work decently on Windows?\n"
            ]
        },
        {
            "id": "298-6-7",
            "pair": [
                "Are there any other options I've overlooked? Perhaps, NTFS isn't that bad on a fairly powerful machine? Is EXT4 perfect for this scenario, or it's going to a huge pain to make it work decently on Windows?\n",
                "You can't use EXT4 for Windows based systems, you will need to partition the drive into 2 or more partitions, Windows will install on NTFS partitions, these will be created during the installation process, at that time you can specify the size of the partition at the time of install.  Linux install after Windows installation will look at the current configuration of the drive and suggest either replacing Windows or install \"alongside\" Windows, when you choose that option Linux will take available space and create their own partition type, default is EXT4 but you can change that type to BTRFS or other type at the time of installation."
            ]
        }
    ],
    [
        {
            "id": "299-1-2",
            "pair": [
                "You should type 192.168.100.1 this should let you into the modem web console (just like your router) once you are inside browse through option and figure it out. Mine says service provider is allowing 2 computers to be connected under the same modem MAC address. But only one computer is recognized. So it could only be two things either your ISP provider is a fraud and violated your privacy or the technical guys who install the modem can clone your stuff. If you live in an apartment then you are screwed like me. They use a server (Ubuntu) to pipe through and filter your Internet. You can only get what they want you to see. They can control a lot of stuff. I'm sure that your ISP knows all about it. Hope this helps. \n",
                "Most of the \"MAC Cloning\" writeups you find on the internet are now out-of-date, as security enhancements from way back in the DOCSIS 1.1 days have given ISPs a way to prevent these attacks.\n"
            ]
        },
        {
            "id": "299-2-3",
            "pair": [
                "Most of the \"MAC Cloning\" writeups you find on the internet are now out-of-date, as security enhancements from way back in the DOCSIS 1.1 days have given ISPs a way to prevent these attacks.\n",
                "To be a lot more specific, there is a mechanism called BPI+ which, if your ISP requires it, will ensure that people attempting to clone MAC addresses can NOT sign on. The problem is, it won't be too easy to find the person at your ISP who can actually confirm whether or not they require all modems to use BPI+. But, odds are, they do require it.\n"
            ]
        },
        {
            "id": "299-3-4",
            "pair": [
                "To be a lot more specific, there is a mechanism called BPI+ which, if your ISP requires it, will ensure that people attempting to clone MAC addresses can NOT sign on. The problem is, it won't be too easy to find the person at your ISP who can actually confirm whether or not they require all modems to use BPI+. But, odds are, they do require it.\n",
                "Years ago, it was common to find networks that'd let you sign on without BPI, but it's pretty rare today, so the most likely theory is that your home network really was correctly implicated in this DMCA complaint."
            ]
        }
    ],
    [
        {
            "id": "3-1-2",
            "pair": [
                "This is not possible the way I want to implement this with the currently supported versions of JunOS. Yes, there are other ways to reach the goal as were mentioned in the other answer and in comments, but my specific question was to be able to signal blackholing using a BGP session. \n",
                "Dynamic Creation of firewall rules is not only to redirect the traffic for a blachole but also to allow inter-datacenter, inter corporate and intranet traffic geo-distributed, to have firewall policies defined with ip abstraction, using some labelling on the dynamic routing to populate firewall objects and then to allow for the control of the traffic.\n"
            ]
        },
        {
            "id": "3-2-3",
            "pair": [
                "Dynamic Creation of firewall rules is not only to redirect the traffic for a blachole but also to allow inter-datacenter, inter corporate and intranet traffic geo-distributed, to have firewall policies defined with ip abstraction, using some labelling on the dynamic routing to populate firewall objects and then to allow for the control of the traffic.\n",
                "I searched for a solution like that some years ago with Juniper but I did not have any feedback. \n"
            ]
        },
        {
            "id": "3-3-4",
            "pair": [
                "I searched for a solution like that some years ago with Juniper but I did not have any feedback. \n",
                "There are some scripting on Junos which may help on the subject but then the object population was not completely done in response to routing exchange events. There was also no possibility to tag direct routes in order to classify them according to the needs:\n"
            ]
        },
        {
            "id": "3-4-5",
            "pair": [
                "There are some scripting on Junos which may help on the subject but then the object population was not completely done in response to routing exchange events. There was also no possibility to tag direct routes in order to classify them according to the needs:\n",
                "FlowSpec could be a help and a firewall which could connect directly to the mpls core as a PE would be also a help."
            ]
        }
    ],
    [
        {
            "id": "30-1-2",
            "pair": [
                "I am advocating to implement a solid backup concept before facing data loss.\n",
                "Having onsite and offsite backups is one pillar of such a concept. However I learned that the metadata is sometimes as valueable as the actual file contents.\n"
            ]
        },
        {
            "id": "30-2-3",
            "pair": [
                "Having onsite and offsite backups is one pillar of such a concept. However I learned that the metadata is sometimes as valueable as the actual file contents.\n",
                "E.g. I had a data backup and a recovered btrfs filesystem and needed to decide whether to restore the backup or to keep the recovered data. As I was lacking checksums of the files, I decided to restore from offsite backup. \n"
            ]
        },
        {
            "id": "30-3-4",
            "pair": [
                "E.g. I had a data backup and a recovered btrfs filesystem and needed to decide whether to restore the backup or to keep the recovered data. As I was lacking checksums of the files, I decided to restore from offsite backup. \n",
                "After the restore I learned, that the file creation date, ownership and permissions are also crucial to keep track of. Luckily, my data is structured and I could set defaults per script.\n"
            ]
        },
        {
            "id": "30-4-5",
            "pair": [
                "After the restore I learned, that the file creation date, ownership and permissions are also crucial to keep track of. Luckily, my data is structured and I could set defaults per script.\n",
                "However to improve my backup plan, I want to store the metadata of my files.\n"
            ]
        },
        {
            "id": "30-5-6",
            "pair": [
                "However to improve my backup plan, I want to store the metadata of my files.\n",
                "The first action was to sha256sum and stat the data and save it to disk.\n"
            ]
        },
        {
            "id": "30-6-7",
            "pair": [
                "The first action was to sha256sum and stat the data and save it to disk.\n",
                "How could you index and store the metadata of the files on Linux in an easier and more efficient way?\n"
            ]
        },
        {
            "id": "30-7-8",
            "pair": [
                "How could you index and store the metadata of the files on Linux in an easier and more efficient way?\n",
                "I've looked into git-annex which looks promising and I've seen people use updatedb to keep track of files (also metdadata?) but maybe someone has better options?\n"
            ]
        },
        {
            "id": "30-8-9",
            "pair": [
                "I've looked into git-annex which looks promising and I've seen people use updatedb to keep track of files (also metdadata?) but maybe someone has better options?\n",
                "You could use a system integrity tool like tripwire, AIDE, etc. to store checksums and metadata for your files. These tools are designed to detect changes to file metadata and contents, so they create an index of this information."
            ]
        }
    ],
    [
        {
            "id": "300-1-2",
            "pair": [
                "Other than the time you need to make sure your PHP cli environment is sane on a number of machines there is no downside.\n",
                "I done mostly web stuff and some of our daily crons have been involved. It's been nice to create ~/server/cron/whatever1.mylanguage which can then piggyback off all the site code we've already written for database connections, logging, etc rather than writing all that code again. Or more often than not, not writing that code at all and complicating troubleshooting when it all goes wrong.\n"
            ]
        },
        {
            "id": "300-2-3",
            "pair": [
                "I done mostly web stuff and some of our daily crons have been involved. It's been nice to create ~/server/cron/whatever1.mylanguage which can then piggyback off all the site code we've already written for database connections, logging, etc rather than writing all that code again. Or more often than not, not writing that code at all and complicating troubleshooting when it all goes wrong.\n",
                "Additionally if it's part of your site code it'll get maintained better and be in source control which many crons never make it into. \n"
            ]
        },
        {
            "id": "300-3-4",
            "pair": [
                "Additionally if it's part of your site code it'll get maintained better and be in source control which many crons never make it into. \n",
                "We've got some CentOS/MySQL VMs that need regular maintenance activities.  Cron is the obvious answer for scheduling, but there's a specific set of events and some significant looping/branching logic involved in the actual shutdown/check/backup/startup.  The box admins don't want to touch this, so it's falling to the DBAs, who are not shell experts by any stretch.\n"
            ]
        },
        {
            "id": "300-4-5",
            "pair": [
                "We've got some CentOS/MySQL VMs that need regular maintenance activities.  Cron is the obvious answer for scheduling, but there's a specific set of events and some significant looping/branching logic involved in the actual shutdown/check/backup/startup.  The box admins don't want to touch this, so it's falling to the DBAs, who are not shell experts by any stretch.\n",
                "We use several apps that have PHP scripts from cron, and have written/maintained similar scripts as well.  Since we've got way more PHP talent in house than *sh, I am wondering if PHP-CLI would be a better choice for us, in terms of effective development and maintenance. \n"
            ]
        },
        {
            "id": "300-5-6",
            "pair": [
                "We use several apps that have PHP scripts from cron, and have written/maintained similar scripts as well.  Since we've got way more PHP talent in house than *sh, I am wondering if PHP-CLI would be a better choice for us, in terms of effective development and maintenance. \n",
                "Are there any downsides/trade-offs that would make this a bad idea?"
            ]
        }
    ],
    [
        {
            "id": "31-1-2",
            "pair": [
                "You can avoid this iteration if you track the location of the tail element.  \n",
                "Of course, you have to maintain the tail in other places as well.  \n"
            ]
        },
        {
            "id": "31-2-3",
            "pair": [
                "Of course, you have to maintain the tail in other places as well.  \n",
                "Since both branches update tail, I moved that out of the if/else.  \n"
            ]
        },
        {
            "id": "31-3-4",
            "pair": [
                "Since both branches update tail, I moved that out of the if/else.  \n",
                "I notice that you use this. to specify object fields.  That actually isn't necessary unless there is a conflict with a parameter or local variable.  Of course, if you simply prefer it that way to make it obvious which variables are object fields, you can.  \n"
            ]
        },
        {
            "id": "31-4-5",
            "pair": [
                "I notice that you use this. to specify object fields.  That actually isn't necessary unless there is a conflict with a parameter or local variable.  Of course, if you simply prefer it that way to make it obvious which variables are object fields, you can.  \n",
                "You can also update the tail when you dequeue to an empty queue, but you don't need to do so for functionality.  It does allow the garbage collector to collect the node though.  \n"
            ]
        },
        {
            "id": "31-5-6",
            "pair": [
                "You can also update the tail when you dequeue to an empty queue, but you don't need to do so for functionality.  It does allow the garbage collector to collect the node though.  \n",
                "As I've never implemented a queue, I decided to create a very simple implementation using a linked list approach.\n"
            ]
        },
        {
            "id": "31-6-7",
            "pair": [
                "As I've never implemented a queue, I decided to create a very simple implementation using a linked list approach.\n",
                "I create a singly linked list, using a private Node class that references some data and a previous Node,\n"
            ]
        },
        {
            "id": "31-7-8",
            "pair": [
                "I create a singly linked list, using a private Node class that references some data and a previous Node,\n",
                "When enqueuing data, I check to see if the linked list is empty or not (by checking to see if the head of the list is null).\n"
            ]
        },
        {
            "id": "31-8-9",
            "pair": [
                "When enqueuing data, I check to see if the linked list is empty or not (by checking to see if the head of the list is null).\n",
                "If it's empty, point the head to a newly-created Node with the input data. If the list is not empty, I iterate through\n"
            ]
        },
        {
            "id": "31-9-10",
            "pair": [
                "If it's empty, point the head to a newly-created Node with the input data. If the list is not empty, I iterate through\n",
                "the list until I reach the end (i.e. the previous Node is null) and then I insert a newly-created Node.\n"
            ]
        },
        {
            "id": "31-10-11",
            "pair": [
                "the list until I reach the end (i.e. the previous Node is null) and then I insert a newly-created Node.\n",
                "When dequeuing, I throw an IllegalStateException if the queue is empty. If the queue is not empty, I set the new head of\n"
            ]
        },
        {
            "id": "31-11-12",
            "pair": [
                "When dequeuing, I throw an IllegalStateException if the queue is empty. If the queue is not empty, I set the new head of\n",
                "the queue to be the old head's previous value, and return the old head's data value."
            ]
        }
    ],
    [
        {
            "id": "32-1-2",
            "pair": [
                "In essence, you are asking how to hack the network to intercept the network content.\n",
                "This simply cannot be done other than how you already found out.\n"
            ]
        },
        {
            "id": "32-2-3",
            "pair": [
                "This simply cannot be done other than how you already found out.\n",
                "Wireshark is the way to go, but as the content is encrypted, that's basically all you can do.\n"
            ]
        },
        {
            "id": "32-3-4",
            "pair": [
                "Wireshark is the way to go, but as the content is encrypted, that's basically all you can do.\n",
                "Alternatively you would want to go on the server where the database is and extract it from there. You would need to make the database publicly accessible, which is a HUGE security risk. You will need your administrators help to open the ports, but they will quite likely say no because of the security implications.\n"
            ]
        },
        {
            "id": "32-4-5",
            "pair": [
                "Alternatively you would want to go on the server where the database is and extract it from there. You would need to make the database publicly accessible, which is a HUGE security risk. You will need your administrators help to open the ports, but they will quite likely say no because of the security implications.\n",
                "I have an application that runs on Citrix Workspace and generally I runs application from .ica extension file format. So I can access the database using this application. \n"
            ]
        },
        {
            "id": "32-5-6",
            "pair": [
                "I have an application that runs on Citrix Workspace and generally I runs application from .ica extension file format. So I can access the database using this application. \n",
                "My main purpose is to know this application's network traffic to build my own mobile (like Android, iOS) application project for easy access without extra programs like Citrix Workspace.\n"
            ]
        },
        {
            "id": "32-6-7",
            "pair": [
                "My main purpose is to know this application's network traffic to build my own mobile (like Android, iOS) application project for easy access without extra programs like Citrix Workspace.\n",
                "So after some researching, I decided to capture network traffic via WireShark and I saw that traffic is encrypted. So that again researching on Internet about 2 days ( I really don't know how actually https work so I searched about it.). And a few method I tried and only SSLKEYLOGFILE variable method worked on my Ubuntu 18.04 desktop but it only supports for browser but the application that I need is to run on Citrix Workspace. I can't get ssl.log file. Also I found a link how to decrypt it but it requires a private key that I haven't.\n"
            ]
        },
        {
            "id": "32-7-8",
            "pair": [
                "So after some researching, I decided to capture network traffic via WireShark and I saw that traffic is encrypted. So that again researching on Internet about 2 days ( I really don't know how actually https work so I searched about it.). And a few method I tried and only SSLKEYLOGFILE variable method worked on my Ubuntu 18.04 desktop but it only supports for browser but the application that I need is to run on Citrix Workspace. I can't get ssl.log file. Also I found a link how to decrypt it but it requires a private key that I haven't.\n",
                "What I am wondering is that, without owner of server or root access of server, is there any methods to get TCP packets ( that encrypted using TLSv1.2 protocol) as decrypted to know that which url is requesting from app and which method (GET,POST,etc.) is using and what the payload is sent. So I can start build mobile platform application."
            ]
        }
    ],
    [
        {
            "id": "33-1-2",
            "pair": [
                "You can configure this by setting the \"Connections in primary role\" setting when configuring read-only routing. You have 2 options:\n",
                "This setting isn't configured at the login level though - this relies on what you said in the first part of your question, that your logins are using ApplicationIntent=ReadOnly in their connection string. \n"
            ]
        },
        {
            "id": "33-2-3",
            "pair": [
                "This setting isn't configured at the login level though - this relies on what you said in the first part of your question, that your logins are using ApplicationIntent=ReadOnly in their connection string. \n",
                "If they don't ask for that specifically - meaning, if they don't declare that they're only going to write - then SQL Server has no fast way of knowing that they have no writeable permissions in any database, on any object. After all, they could even execute a stored procedure signed with a cert, and that cert could allow them to do writes.\n"
            ]
        },
        {
            "id": "33-3-4",
            "pair": [
                "If they don't ask for that specifically - meaning, if they don't declare that they're only going to write - then SQL Server has no fast way of knowing that they have no writeable permissions in any database, on any object. After all, they could even execute a stored procedure signed with a cert, and that cert could allow them to do writes.\n",
                "So if you really want to stop them from logging in even if they don't use ApplicationIntent=ReadOnly, then I'd do it by simply disabling their logins on the primary. Then, run an Agent job every minute on every replica, checking to see if this particular replica is a primary. If a failover has occurred, and this replica is now a primary, disable the reporting-only logins. If the replica is a secondary, and the reporting-only logins are disabled, then enable them.\n"
            ]
        },
        {
            "id": "33-4-5",
            "pair": [
                "So if you really want to stop them from logging in even if they don't use ApplicationIntent=ReadOnly, then I'd do it by simply disabling their logins on the primary. Then, run an Agent job every minute on every replica, checking to see if this particular replica is a primary. If a failover has occurred, and this replica is now a primary, disable the reporting-only logins. If the replica is a secondary, and the reporting-only logins are disabled, then enable them.\n",
                "This will of course run into problems if you really do want to disable someone's login - it'll instantly be enabled again in a minute.\n"
            ]
        },
        {
            "id": "33-5-6",
            "pair": [
                "This will of course run into problems if you really do want to disable someone's login - it'll instantly be enabled again in a minute.\n",
                "We are configuring an availability group with a readable secondary replica.\n"
            ]
        },
        {
            "id": "33-6-7",
            "pair": [
                "We are configuring an availability group with a readable secondary replica.\n",
                "A login can connect to the secondary replica using \"ApplicationIntent=ReadOnly\" in its connection string. But we came across a problem that could make our headache in future.\n"
            ]
        },
        {
            "id": "33-7-8",
            "pair": [
                "A login can connect to the secondary replica using \"ApplicationIntent=ReadOnly\" in its connection string. But we came across a problem that could make our headache in future.\n",
                "If the secondary replica is unavailable due to any causes when the login establishes connection, this login will be directed to the primary replica whatever it's using \"ApplicationIntent=ReadOnly\" or not.\n"
            ]
        },
        {
            "id": "33-8-9",
            "pair": [
                "If the secondary replica is unavailable due to any causes when the login establishes connection, this login will be directed to the primary replica whatever it's using \"ApplicationIntent=ReadOnly\" or not.\n",
                "So, my question - is there any way to ban particular logins from connecting to the primary replica if they configured for working with the secondary one?\n"
            ]
        },
        {
            "id": "33-9-10",
            "pair": [
                "So, my question - is there any way to ban particular logins from connecting to the primary replica if they configured for working with the secondary one?\n",
                "I mean if the primary replica isn't able to direct the connection to the secondary one the connection should be closed instead of going on working on the primary replica."
            ]
        }
    ],
    [
        {
            "id": "34-1-2",
            "pair": [
                "It sounds like your server is also an Open Directory Master (running slapd), if that is the case the hashes are stored encrypted by the password server and are not really accessible. \n",
                "If the server is not also an OD Master, things are easier and digging around in the default node with dscl as @bourneN5years mentioned is a place to start. The files for the local node can be found in /var/db/dslocal/nodes/Default\n"
            ]
        },
        {
            "id": "34-2-3",
            "pair": [
                "If the server is not also an OD Master, things are easier and digging around in the default node with dscl as @bourneN5years mentioned is a place to start. The files for the local node can be found in /var/db/dslocal/nodes/Default\n",
                "It may be cleaner, if you can have jenkins pass the login info to the auth webapp in Server 5.2. \n"
            ]
        },
        {
            "id": "34-3-4",
            "pair": [
                "It may be cleaner, if you can have jenkins pass the login info to the auth webapp in Server 5.2. \n",
                "It should provide http style auth at http://localhost:4444/auth\n"
            ]
        },
        {
            "id": "34-4-5",
            "pair": [
                "It should provide http style auth at http://localhost:4444/auth\n",
                "Is there a way of getting the password hash for a named LDAP user where the user is defined within Open Directory on a MacOS Server running OSX Sierra and MacOS Server 5.2?\n"
            ]
        },
        {
            "id": "34-5-6",
            "pair": [
                "Is there a way of getting the password hash for a named LDAP user where the user is defined within Open Directory on a MacOS Server running OSX Sierra and MacOS Server 5.2?\n",
                "I am setting up a CI/CD node using Jenkins within a Docker container that will run on a server that runs MacOS Server.  I want that Jenkins container to be secured and to be secured using the LDAP open directory of the server, i.e. a user wanting to modify the Jenkins configuration needs to use their own network user/password to log in to Jenkins.\n"
            ]
        },
        {
            "id": "34-6-7",
            "pair": [
                "I am setting up a CI/CD node using Jenkins within a Docker container that will run on a server that runs MacOS Server.  I want that Jenkins container to be secured and to be secured using the LDAP open directory of the server, i.e. a user wanting to modify the Jenkins configuration needs to use their own network user/password to log in to Jenkins.\n",
                "As part of configuring this, I need to copy an XML file (config.xml) into the Jenkins home directory and this file needs to contain the hashed password of the user that secures the LDAP system.  I think the password hash is stored in an encrypted directory and therefore I want to know how to retrieve it.\n"
            ]
        },
        {
            "id": "34-7-8",
            "pair": [
                "As part of configuring this, I need to copy an XML file (config.xml) into the Jenkins home directory and this file needs to contain the hashed password of the user that secures the LDAP system.  I think the password hash is stored in an encrypted directory and therefore I want to know how to retrieve it.\n",
                "I want the whole of the deployment script to be automated, and so I need to be able to retrieve the hash (or recreate it) for the named user so that it can be injected into the XML file that will be put into the Jenkins home directory."
            ]
        }
    ],
    [
        {
            "id": "35-1-2",
            "pair": [
                "On a lot of consumer internet networks, I can just set my IP to that of the neighbors and have their IP, so yeah, IP's can be spoofed. Colocated servers also often share one subnet among different customers. Just DOS the machine so it goes down, take over its IP and you're done...\n",
                "Anyway, it depends on your situation. Do you have data which you expect to be stolen, or tried to be stolen? Then you need more security than IP whitelisting. However, will it be a 'normal' (web)server, then usually even IP restrictions are only necessary for flakey software like PHPMyadmin. Software like SSH for instance won't just be cracked, because OpenSSH is strictly audited. Even DenyHosts (deny IP's that try to login frequently) is unnecessary and mostly annoying (I've been blocked out of my own machines quite frequently...).\n"
            ]
        },
        {
            "id": "35-2-3",
            "pair": [
                "Anyway, it depends on your situation. Do you have data which you expect to be stolen, or tried to be stolen? Then you need more security than IP whitelisting. However, will it be a 'normal' (web)server, then usually even IP restrictions are only necessary for flakey software like PHPMyadmin. Software like SSH for instance won't just be cracked, because OpenSSH is strictly audited. Even DenyHosts (deny IP's that try to login frequently) is unnecessary and mostly annoying (I've been blocked out of my own machines quite frequently...).\n",
                "My experience is that if you don't have data someone else wants, your biggest problem is automated scans for things like flakey PHP sites to send spam through. The most simple security measures, like IP whitelisting or running on a different port, are often enough for that.\n"
            ]
        },
        {
            "id": "35-3-4",
            "pair": [
                "My experience is that if you don't have data someone else wants, your biggest problem is automated scans for things like flakey PHP sites to send spam through. The most simple security measures, like IP whitelisting or running on a different port, are often enough for that.\n",
                "I am using ConfigServer Security & Firewall (CSF) to limit port access to whitelist IP addresses. However I have heard that IP addresses can be spoofed. How wide-spread is this problem, and is it something I should be concerned with?"
            ]
        }
    ],
    [
        {
            "id": "36-1-2",
            "pair": [
                "You need a vhost in nginx only if you want nginx to respond differently based on the hostname. So, for example, if you ALWAYS want nginx to proxy the request to apache, then no vhost is needed. If you want it to ALWAYS proxy the request to apache if the filename doesn't end in \".png\", and/or the requested file doesn't exist in a given directory, then still, you don't need a virtual host.\n",
                "But if you want nginx to search for files in different locations based on the hostname, then unless there's an easy, straightforward way you can tell nginx to convert the hostname into a file location, then you'll probably end up setting up a virtual host for each hostname -- that way you can tell it where to find the files.\n"
            ]
        },
        {
            "id": "36-2-3",
            "pair": [
                "But if you want nginx to search for files in different locations based on the hostname, then unless there's an easy, straightforward way you can tell nginx to convert the hostname into a file location, then you'll probably end up setting up a virtual host for each hostname -- that way you can tell it where to find the files.\n",
                "I would create a vhost in both apache and nginx for each. If you really have a lot of vhosts you can probably write a quick shell script to generate one set of initial configurations from the other set. \n"
            ]
        },
        {
            "id": "36-3-4",
            "pair": [
                "I would create a vhost in both apache and nginx for each. If you really have a lot of vhosts you can probably write a quick shell script to generate one set of initial configurations from the other set. \n",
                "In the long run, I think this is the simplest option, as it makes it easier to make changes on a site level after the system is configured."
            ]
        }
    ],
    [
        {
            "id": "37-1-2",
            "pair": [
                "We recently had a very strange problem.  Two users share a 1gbps switch, and their network connection has been getting worse and worse (it does not disconnect, but has gotten REALLY slow, down to bytes/second).  We tried three different known-good switches, with no luck, even with only one person plugged into the switch, and even when forced to 10mbps.  The offices around theirs (all using 1gbps switches) have no problems.\n",
                "The thing is, when we hook either of them up directly to the wall, the connection is fine!  Knowing that with only one computer, a hub is basically the same as a repeater, I replaced their switch with a 10mbps hub - presto!  No more problems.\n"
            ]
        },
        {
            "id": "37-2-3",
            "pair": [
                "The thing is, when we hook either of them up directly to the wall, the connection is fine!  Knowing that with only one computer, a hub is basically the same as a repeater, I replaced their switch with a 10mbps hub - presto!  No more problems.\n",
                "My question is, what possible circumstances could cause a hub to work when a switch doesn't?\n"
            ]
        },
        {
            "id": "37-3-4",
            "pair": [
                "My question is, what possible circumstances could cause a hub to work when a switch doesn't?\n",
                "Maybe the hardware at the other side of the wall is having trouble negotiating the connecton with the swiches but not with the hubs. I found one time that one switch negotiated the connection at full duplex and the other one at half and had LOTs of weird problems.\n"
            ]
        },
        {
            "id": "37-4-5",
            "pair": [
                "Maybe the hardware at the other side of the wall is having trouble negotiating the connecton with the swiches but not with the hubs. I found one time that one switch negotiated the connection at full duplex and the other one at half and had LOTs of weird problems.\n",
                "The better way to diagnostic the problem I can think of is with a fluke device called Net Tool. You can find it here"
            ]
        }
    ],
    [
        {
            "id": "38-1-2",
            "pair": [
                "Yes.  The CA in your case would have to be private.\n",
                "If you think about it, I could also have a service called hello-world.production. As a public CA would be in the trust-anchor store of most/all clients, if a public CA issued a certificate to both you and I, clients connecting wouldn't know who they're connecting to.  A public CA can only issue certificates to entities which are globally unique. Commercial CAs generally rely on the global uniqueness of DNS or email addresses (depending on certificate usage) for this.\n"
            ]
        },
        {
            "id": "38-2-3",
            "pair": [
                "If you think about it, I could also have a service called hello-world.production. As a public CA would be in the trust-anchor store of most/all clients, if a public CA issued a certificate to both you and I, clients connecting wouldn't know who they're connecting to.  A public CA can only issue certificates to entities which are globally unique. Commercial CAs generally rely on the global uniqueness of DNS or email addresses (depending on certificate usage) for this.\n",
                "Your private CA, on the other hand, will only be in the trust-anchor store of your clients, and it'll only issue certificates to your instance of hello-world.production.  If your clients somehow try to connect to my instance, it will fail as I won't have a certificate issued by a private CA your clients trust.\n"
            ]
        },
        {
            "id": "38-3-4",
            "pair": [
                "Your private CA, on the other hand, will only be in the trust-anchor store of your clients, and it'll only issue certificates to your instance of hello-world.production.  If your clients somehow try to connect to my instance, it will fail as I won't have a certificate issued by a private CA your clients trust.\n",
                "There are many ways to distribute a certificate to a client's trust-anchor store:  \n"
            ]
        },
        {
            "id": "38-4-5",
            "pair": [
                "There are many ways to distribute a certificate to a client's trust-anchor store:  \n",
                "If you don't want the trouble of running a private CA, you could consider a managed private CA.  AWS, Digicert, Entrust and Sectigo provide this service at a cost.\n"
            ]
        },
        {
            "id": "38-5-6",
            "pair": [
                "If you don't want the trouble of running a private CA, you could consider a managed private CA.  AWS, Digicert, Entrust and Sectigo provide this service at a cost.\n",
                "I really love AWS Cloud Map (Service Discovery in AWS). It integrates with the AWS DNS to make service discovery very simple. Just register an instance of a service and you can discover the service via normal DNS resolution. For example curl http://hello-world.production would connect to a production instance of the hello-world service. Great.\n"
            ]
        },
        {
            "id": "38-6-7",
            "pair": [
                "I really love AWS Cloud Map (Service Discovery in AWS). It integrates with the AWS DNS to make service discovery very simple. Just register an instance of a service and you can discover the service via normal DNS resolution. For example curl http://hello-world.production would connect to a production instance of the hello-world service. Great.\n",
                "My problem is that I want to secure my traffic between my microservices using HTTPS. However, how would I generate a certificate/key for something like hello-world.production? Is a private CA the only option? I know AWS has support for that but would I need to inject the root certificate for my private CA into all my micro-services? Ideally I would avoid running a private CA. If that's the best way, is there a way to ensure all my internal services have the CA's root cert?"
            ]
        }
    ],
    [
        {
            "id": "39-1-2",
            "pair": [
                "I am experimenting different loss functions for my regression model. I noticed that in the sklearn, there are:\n",
                "sklearn.linear_model.HuberRegressor and sklearn.linear_model.ElasticNet\n"
            ]
        },
        {
            "id": "39-2-3",
            "pair": [
                "sklearn.linear_model.HuberRegressor and sklearn.linear_model.ElasticNet\n",
                "To me, both use the combination of L1 and L2 loss. What exactly is the difference of this? Thanks!\n"
            ]
        },
        {
            "id": "39-3-4",
            "pair": [
                "To me, both use the combination of L1 and L2 loss. What exactly is the difference of this? Thanks!\n",
                "Yes, many loss functions in regression models are using a combination of L1 and L2 for different purposes. To realize the difference, I start with Ridge regression. \n"
            ]
        },
        {
            "id": "39-4-5",
            "pair": [
                "Yes, many loss functions in regression models are using a combination of L1 and L2 for different purposes. To realize the difference, I start with Ridge regression. \n",
                "Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients. The ridge coefficients minimize a penalized residual sum of squares. \n"
            ]
        },
        {
            "id": "39-5-6",
            "pair": [
                "Ridge regression addresses some of the problems of Ordinary Least Squares by imposing a penalty on the size of the coefficients. The ridge coefficients minimize a penalized residual sum of squares. \n",
                "The HuberRegressor is different to Ridge because it applies a linear loss to samples that are classified as outliers. A sample is classified as an inlier if the absolute error of that sample is lesser than a certain threshold.\n"
            ]
        },
        {
            "id": "39-6-7",
            "pair": [
                "The HuberRegressor is different to Ridge because it applies a linear loss to samples that are classified as outliers. A sample is classified as an inlier if the absolute error of that sample is lesser than a certain threshold.\n",
                "ElasticNet is a linear regression model trained with both $\\iota_1$ and $\\iota_2$-norm regularization of the coefficients. This combination allows for learning a sparse model where few of the weights are non-zero like Lasso, while still maintaining the regularization properties of Ridge. Elastic-net is useful when there are multiple features which are correlated with one another. Lasso is likely to pick one of these at random, while elastic-net is likely to pick both."
            ]
        }
    ],
    [
        {
            "id": "4-1-2",
            "pair": [
                "We have central HQ building and a lot of small branch offices connecting via VPN and want to implement AD (If you can believe we still haven't). We want everyone to log in using domain accounts and be policed centrally. \n",
                "We are OK with having a RODC in a branch office with like 10 computers. But we have these small branches with two to four PCs only. Some of these branches connect to HQ via IPSec  site-to-site VPN, some via remote access (client-based) VPN.\n"
            ]
        },
        {
            "id": "4-2-3",
            "pair": [
                "We are OK with having a RODC in a branch office with like 10 computers. But we have these small branches with two to four PCs only. Some of these branches connect to HQ via IPSec  site-to-site VPN, some via remote access (client-based) VPN.\n",
                "So there is no problem with ones that have local RODC or connecting to HQ DCs via VPN router. But how about small branches? We don't really want to set up a machine there, neither we want to invest into Windows Server licenses or fancy network equipment. \n"
            ]
        },
        {
            "id": "4-3-4",
            "pair": [
                "So there is no problem with ones that have local RODC or connecting to HQ DCs via VPN router. But how about small branches? We don't really want to set up a machine there, neither we want to invest into Windows Server licenses or fancy network equipment. \n",
                "Also, the problem is that we cannot access HQ DCs via VPN because we are not logged in and connected to HQ internal network yet, so DCs aren't reachable.\n"
            ]
        },
        {
            "id": "4-4-5",
            "pair": [
                "Also, the problem is that we cannot access HQ DCs via VPN because we are not logged in and connected to HQ internal network yet, so DCs aren't reachable.\n",
                "What is typically done in that situation if it is needed to have central management over policies on those PCs? Or is it better to let 'em loose and use local policies and accounts in this situation?\n"
            ]
        },
        {
            "id": "4-5-6",
            "pair": [
                "What is typically done in that situation if it is needed to have central management over policies on those PCs? Or is it better to let 'em loose and use local policies and accounts in this situation?\n",
                "DirectAccess would be ideal for you, but requires a certain amount of infrastructure at HQ.\n"
            ]
        },
        {
            "id": "4-6-7",
            "pair": [
                "DirectAccess would be ideal for you, but requires a certain amount of infrastructure at HQ.\n",
                "First, I would set up site-to-site VPN links from every site to HQ.  No money for fancy network equipment?  That's absolutely fine, as an IPSEC site-to-site VPN is not a fancy or demanding endeavour, you can do it with whatever SOHO routers take your fancy (we use Draytek).\n"
            ]
        },
        {
            "id": "4-7-8",
            "pair": [
                "First, I would set up site-to-site VPN links from every site to HQ.  No money for fancy network equipment?  That's absolutely fine, as an IPSEC site-to-site VPN is not a fancy or demanding endeavour, you can do it with whatever SOHO routers take your fancy (we use Draytek).\n",
                "Now you need to test your bandwidth and latency from branch office to HQ - there's a trade-off you're going to have to decide here between slow-logins and implementing group policy.  Scoping your GPOs carefully should help.  If latency is bad then you may have to settle for only authenticating to HQ DC once, applying policy then and then taking down the site-link and using cached credentials for login.  (The users are fine logging in with cached credentials indefinitely if no DC is available).\n"
            ]
        },
        {
            "id": "4-8-9",
            "pair": [
                "Now you need to test your bandwidth and latency from branch office to HQ - there's a trade-off you're going to have to decide here between slow-logins and implementing group policy.  Scoping your GPOs carefully should help.  If latency is bad then you may have to settle for only authenticating to HQ DC once, applying policy then and then taking down the site-link and using cached credentials for login.  (The users are fine logging in with cached credentials indefinitely if no DC is available).\n",
                "You won't necessarily get all your GPOs to apply out of the box, as the GP client detects a 'slow link' and prevents application of some GP settings (such as folder redirection, software installation).  Slow Link Detection\n"
            ]
        },
        {
            "id": "4-9-10",
            "pair": [
                "You won't necessarily get all your GPOs to apply out of the box, as the GP client detects a 'slow link' and prevents application of some GP settings (such as folder redirection, software installation).  Slow Link Detection\n",
                "I don't understand what you're saying here.  If necessary you can set up a VPN on the user's PC and they can connect it before logging in."
            ]
        }
    ],
    [
        {
            "id": "40-1-2",
            "pair": [
                "diskpart only allows \"converting\" empty disks, so you'll need to delete all partitions using the clear command. If the disk has data that you want to keep, then your only choices are to upgrade to a Windows version that has mbr2gpt, or use third-party software.\n",
                "The most common third-party tool for in-place conversion is gdisk (aka gptfdisk), which costs $0.00. It has a Windows .exe version but is primarily a Linux tool; you can find it already present in many \"live CDs/USBs\" such as the Gparted CD.\n"
            ]
        },
        {
            "id": "40-2-3",
            "pair": [
                "The most common third-party tool for in-place conversion is gdisk (aka gptfdisk), which costs $0.00. It has a Windows .exe version but is primarily a Linux tool; you can find it already present in many \"live CDs/USBs\" such as the Gparted CD.\n",
                "At this point you can delete the Windows partitions and reinstall. Alternatively, you can manually create an EFI System Partition and use bcdboot (as documented in other threads) to convert the existing Windows installation to UEFI mode.\n"
            ]
        },
        {
            "id": "40-3-4",
            "pair": [
                "At this point you can delete the Windows partitions and reinstall. Alternatively, you can manually create an EFI System Partition and use bcdboot (as documented in other threads) to convert the existing Windows installation to UEFI mode.\n",
                "But if you don't want Windows Update to be slow, then why are you installing a three-year-old release in the first place?... Better just make a new Windows 10.1809 USB stick so that you'll get the features immediately after installation. (Preferably using the Media Creation Tool if you want UEFI support \u2013 I can no longer recommend Rufus.)\n"
            ]
        },
        {
            "id": "40-4-5",
            "pair": [
                "But if you don't want Windows Update to be slow, then why are you installing a three-year-old release in the first place?... Better just make a new Windows 10.1809 USB stick so that you'll get the features immediately after installation. (Preferably using the Media Creation Tool if you want UEFI support \u2013 I can no longer recommend Rufus.)\n",
                "No need to reinstall at all. Recent Windows 10 versions ship with the MBR2GPT tool.\n"
            ]
        },
        {
            "id": "40-5-6",
            "pair": [
                "No need to reinstall at all. Recent Windows 10 versions ship with the MBR2GPT tool.\n",
                "The tool needs to be invoked from an elevated command prompt. The quickest way to get one is Win, type cmd, press Ctrl+Shift+Enter."
            ]
        }
    ],
    [
        {
            "id": "41-1-2",
            "pair": [
                "At a minimum, your HDD is too full and that will certainly slow down your computer. This will also cause everything to slow down. \n",
                "The first thing I would do is get another HDD and off load at least 60% (more if possible) of what's on your \"C\" drive.\n"
            ]
        },
        {
            "id": "41-2-3",
            "pair": [
                "The first thing I would do is get another HDD and off load at least 60% (more if possible) of what's on your \"C\" drive.\n",
                "My computer boot up slowly and have a strange behaviour:\n"
            ]
        },
        {
            "id": "41-3-4",
            "pair": [
                "My computer boot up slowly and have a strange behaviour:\n",
                "SOMETIMES when I boot, from a RANDOM moment after going in to desktop, it will start to \"block\" all programs that requires administrator privilege from running(when attempted nothing happens), and it looks like the boot up process just stopped. sometimes if the moment happens pretty early, I get infinite desktop screen with only a cursor, sometimes it happens in the middle of opening all startup programs. Sometimes ALL programs will not run(when attempted nothing happens). Sometimes when I try to restart it stuck at logging off infinitely.\n"
            ]
        },
        {
            "id": "41-4-5",
            "pair": [
                "SOMETIMES when I boot, from a RANDOM moment after going in to desktop, it will start to \"block\" all programs that requires administrator privilege from running(when attempted nothing happens), and it looks like the boot up process just stopped. sometimes if the moment happens pretty early, I get infinite desktop screen with only a cursor, sometimes it happens in the middle of opening all startup programs. Sometimes ALL programs will not run(when attempted nothing happens). Sometimes when I try to restart it stuck at logging off infinitely.\n",
                "This kind of strange behaviours happens more and more frequently. I use to encounter it every 10-20 reboots, and then 3-4 reboots, now I have to reboot multiple times to see a boot without problem.\n"
            ]
        },
        {
            "id": "41-5-6",
            "pair": [
                "This kind of strange behaviours happens more and more frequently. I use to encounter it every 10-20 reboots, and then 3-4 reboots, now I have to reboot multiple times to see a boot without problem.\n",
                "I forgot to make a disk D so I filled 99 percent of my disk C. I think this is the reason why I have a slow boot up. \n"
            ]
        },
        {
            "id": "41-6-7",
            "pair": [
                "I forgot to make a disk D so I filled 99 percent of my disk C. I think this is the reason why I have a slow boot up. \n",
                "I have attempted system scan, chkdsk, malware scan in safe mode, no problems at all."
            ]
        }
    ],
    [
        {
            "id": "42-1-2",
            "pair": [
                "All ports are treated equal, however, in case of PoE, it is important to understand that PoE switches typically cannot power all ports at their maximum power at the same time.\n",
                "I.e. NETSWITCH-24POE-2 recommended by Vicon provides up to 15.4W of power per port, but only 190W in total, meaning only 12 ports can supply full power PoE at the same time. It is possible that each group of 4 ports has its own power limit, hence the recommendation that you use one of the ports in the group for a non-PoE connection.\n"
            ]
        },
        {
            "id": "42-2-3",
            "pair": [
                "I.e. NETSWITCH-24POE-2 recommended by Vicon provides up to 15.4W of power per port, but only 190W in total, meaning only 12 ports can supply full power PoE at the same time. It is possible that each group of 4 ports has its own power limit, hence the recommendation that you use one of the ports in the group for a non-PoE connection.\n",
                "Your DGS\u20111026MP switch has a total output of 370W so if your cameras use up to 15W you are safe to use any and all ports for your cameras.\n"
            ]
        },
        {
            "id": "42-3-4",
            "pair": [
                "Your DGS\u20111026MP switch has a total output of 370W so if your cameras use up to 15W you are safe to use any and all ports for your cameras.\n",
                "I am setting up a motion capture system (vicon vantage), and the cameras are PoE, and all plug into a switch (DGS\u20111026MP, 26 Port Gigabit Max PoE Switch) to feed a PC. \n"
            ]
        },
        {
            "id": "42-4-5",
            "pair": [
                "I am setting up a motion capture system (vicon vantage), and the cameras are PoE, and all plug into a switch (DGS\u20111026MP, 26 Port Gigabit Max PoE Switch) to feed a PC. \n",
                "The instructions say to not plug in the cameras into the upper right port of each group of four ports (see picture) and to reserve those ports for the PC or other switches. Is there a reason for this? I thought switches treated all ports as equivalent?"
            ]
        }
    ],
    [
        {
            "id": "43-1-2",
            "pair": [
                "As Ivan States you would setup a CNAME for the domain you want to redirect to point to your server. You cannot re-direct using a path as, so you would just create a CName for example.com to point to myserver.com, then you can go to example.com/myapp if you wish.\n",
                "To get IIS to handle this, you can do 1 of 2 things. If all requests will be going to the same place, not dependant on the URL, then you can just have one site that listens for all requests on port 80 (or 443 if its https), and your done.\n"
            ]
        },
        {
            "id": "43-2-3",
            "pair": [
                "To get IIS to handle this, you can do 1 of 2 things. If all requests will be going to the same place, not dependant on the URL, then you can just have one site that listens for all requests on port 80 (or 443 if its https), and your done.\n",
                "If however each domain, will need a different site, then you can create multiple sites in IIS, set them to listen on port 80 or 443, but for a specific Host Header, that way that site will only respond to requests for that particualr URL.\n"
            ]
        },
        {
            "id": "43-3-4",
            "pair": [
                "If however each domain, will need a different site, then you can create multiple sites in IIS, set them to listen on port 80 or 443, but for a specific Host Header, that way that site will only respond to requests for that particualr URL.\n",
                "The CNAME they would add is just the domain name, you cannot also use pathnames. And it wouldn't be a redirection per se, as your web server will respond directly to their domains.\n"
            ]
        },
        {
            "id": "43-4-5",
            "pair": [
                "The CNAME they would add is just the domain name, you cannot also use pathnames. And it wouldn't be a redirection per se, as your web server will respond directly to their domains.\n",
                "On your end, you'd need to tell your web server to respond to that domain (or better yet, respond to any domain, and let it be handled by your app).\n"
            ]
        },
        {
            "id": "43-5-6",
            "pair": [
                "On your end, you'd need to tell your web server to respond to that domain (or better yet, respond to any domain, and let it be handled by your app).\n",
                "If you're using Apache, it's pretty easy, as by default it will respond with the default Virtual Host you have defined.\n"
            ]
        },
        {
            "id": "43-6-7",
            "pair": [
                "If you're using Apache, it's pretty easy, as by default it will respond with the default Virtual Host you have defined.\n",
                "For IIS unfortunately I don't know how it would work."
            ]
        }
    ],
    [
        {
            "id": "44-1-2",
            "pair": [
                "If you have access to shaders, you can invert the drawing logic. Normally you loop through tiles and draw each one. When you're zoomed out there are a lot of tiles to draw. Zooming out is slower. Inverting the logic, you can loop through pixels and figure out which tile that pixel is on, and that tells you which sprite to draw, and then you look up a color in the sprite. This is weird but it means that no matter what zoom level, you have the same number of draw calls (just one quad), and the same amount of work to do, at least for the tiles. The objects on top of the map don't work with this trick so you'll still have to draw them separately.\n",
                "To make it work you put the game map into a texture. The texture color tells you which tile to use. You also have the tile sprites in another texture. For example if you have 32x32 pixel tiles, and 16x16 of them, you can put that into a 512x512 texture. If the game map is 200x200 you can put that into a 200x200 texture, where each color is {red = 0.0 for column 0 to red = 1.0 for column 15, green = 0.0 for row 0 to 1.0 to row 15}. \n"
            ]
        },
        {
            "id": "44-2-3",
            "pair": [
                "To make it work you put the game map into a texture. The texture color tells you which tile to use. You also have the tile sprites in another texture. For example if you have 32x32 pixel tiles, and 16x16 of them, you can put that into a 512x512 texture. If the game map is 200x200 you can put that into a 200x200 texture, where each color is {red = 0.0 for column 0 to red = 1.0 for column 15, green = 0.0 for row 0 to 1.0 to row 15}. \n",
                "If the tile to draw is in column 3, row 9, then you'd store {red = 3/15, green=9/15} as the color. The shader then looks up the tile coordinates first. It would see that it needs to look in the spritesheet at column 3 (which starts at pixel x=3*32) and row 9 (which starts at pixel y=9*32). It then calculates which pixel of the sprite it needs to draw, and looks that color up in the spritesheet.\n"
            ]
        },
        {
            "id": "44-3-4",
            "pair": [
                "If the tile to draw is in column 3, row 9, then you'd store {red = 3/15, green=9/15} as the color. The shader then looks up the tile coordinates first. It would see that it needs to look in the spritesheet at column 3 (which starts at pixel x=3*32) and row 9 (which starts at pixel y=9*32). It then calculates which pixel of the sprite it needs to draw, and looks that color up in the spritesheet.\n",
                "I had tried this out a long time ago with this flash demo, and then I tried to find other people who had the same idea and found this javascript demo and a blog post about the technique. Some of the comments on that blog post suggest that it may not actually be that efficient of a shader, so if you're going to try this it's worth measuring the performance.\n"
            ]
        },
        {
            "id": "44-4-5",
            "pair": [
                "I had tried this out a long time ago with this flash demo, and then I tried to find other people who had the same idea and found this javascript demo and a blog post about the technique. Some of the comments on that blog post suggest that it may not actually be that efficient of a shader, so if you're going to try this it's worth measuring the performance.\n",
                "You can create something like level of details for this.\n"
            ]
        },
        {
            "id": "44-5-6",
            "pair": [
                "You can create something like level of details for this.\n",
                "I. Create tiles that will cover the same amount of screen but have smaller resolution. You stay with same number of draw calls but with less data per draw.\n"
            ]
        },
        {
            "id": "44-6-7",
            "pair": [
                "I. Create tiles that will cover the same amount of screen but have smaller resolution. You stay with same number of draw calls but with less data per draw.\n",
                "II. Create tiles that are bigger so you get less draw calls.\n"
            ]
        },
        {
            "id": "44-7-8",
            "pair": [
                "II. Create tiles that are bigger so you get less draw calls.\n",
                "III. Merge this two methods and create bigger tiles but also with less data per tile of normal size.\n"
            ]
        },
        {
            "id": "44-8-9",
            "pair": [
                "III. Merge this two methods and create bigger tiles but also with less data per tile of normal size.\n",
                "Third option is the best in my opinion. When you zoom out there will be lack of details so there is no reason to render tiles with all data.\n"
            ]
        },
        {
            "id": "44-9-10",
            "pair": [
                "Third option is the best in my opinion. When you zoom out there will be lack of details so there is no reason to render tiles with all data.\n",
                "Of course this will take more memory and you will need to calculate which tile render depend on distance from camera.\n"
            ]
        },
        {
            "id": "44-10-11",
            "pair": [
                "Of course this will take more memory and you will need to calculate which tile render depend on distance from camera.\n",
                "Also if this is one big image and you can write shader for rendering you can just change vertices uv's in shader. With this you can render just one quad (2 triangles) and I think this is the best solution ;)"
            ]
        }
    ],
    [
        {
            "id": "45-1-2",
            "pair": [
                "I think you should use both, VLANs and Subnets, if you really want simulate small corporate network. For example:\n",
                "With VLANs you could easily configure VM network interfaces.\n"
            ]
        },
        {
            "id": "45-2-3",
            "pair": [
                "With VLANs you could easily configure VM network interfaces.\n",
                "I am currently using ESXi and a Dell R610 to simulate a 100% vitalized environment. It is going to be setup like a small company. I plan on having a webserver for the company, possibly email server, a extranet, intranet, DMZ, etc. This is going to be setup so I can practice ethical hacking and making my way from a few different options such as establishing foothold through a external web server, simulating being within the network as a employee, etc. My question would be should I be using subnets or VLANs to segment each network off? So like the web server should be able to communicate with everyone, but some private servers in the intranet of the company should not be accessed by the web server. So if someone were to gain access to the web server, they would need to keep pivoting deeper into the network until they get onto a box that has a link to the private servers. So should I be looking at VLANs or subnets for this?"
            ]
        }
    ],
    [
        {
            "id": "46-1-2",
            "pair": [
                "If you can connect to PostgreSQL from the original machine you can see what port it's listening on:\n",
                "At a guess I'd say listen_addresses will be localhost, so it's not accepting TCP/IP connections from outside network interfaces.\n"
            ]
        },
        {
            "id": "46-2-3",
            "pair": [
                "At a guess I'd say listen_addresses will be localhost, so it's not accepting TCP/IP connections from outside network interfaces.\n",
                "Another possibility is a network firewall on the database host that's not letting you connect. tcptraceroute to the port from the remote host, see what the result is.\n"
            ]
        },
        {
            "id": "46-3-4",
            "pair": [
                "Another possibility is a network firewall on the database host that's not letting you connect. tcptraceroute to the port from the remote host, see what the result is.\n",
                "First, it's not at all certain that it's freeciv that's listening on 5555. Nmap uses a file called nmap-services that lists the common application for each port (if the port is registered with IANA, it will usually be whatever application was registered for that port). But it's quite possible to use another port for any service, as you've noticed...\n"
            ]
        },
        {
            "id": "46-4-5",
            "pair": [
                "First, it's not at all certain that it's freeciv that's listening on 5555. Nmap uses a file called nmap-services that lists the common application for each port (if the port is registered with IANA, it will usually be whatever application was registered for that port). But it's quite possible to use another port for any service, as you've noticed...\n",
                "If you run nmap with the option -sV, it will actually talk to the port and try different protocol to see which one matches. That will of course take a bit longer than running it with sT, and may trigger intrusion detection alarms if any are in place.\n"
            ]
        },
        {
            "id": "46-5-6",
            "pair": [
                "If you run nmap with the option -sV, it will actually talk to the port and try different protocol to see which one matches. That will of course take a bit longer than running it with sT, and may trigger intrusion detection alarms if any are in place.\n",
                "In this particular case, I'd lay a bet that it's port 5555 that is being used - because it's unlikely that a database server to be running freeciv, and it's common for humans to use a portnumer like 5555, 3333, 6666 etc. So if I were you, I'd start by doing pgsql -p 5555 and if that doesn't work, do nmap -sV to find out which of the other ports it is.\n"
            ]
        },
        {
            "id": "46-6-7",
            "pair": [
                "In this particular case, I'd lay a bet that it's port 5555 that is being used - because it's unlikely that a database server to be running freeciv, and it's common for humans to use a portnumer like 5555, 3333, 6666 etc. So if I were you, I'd start by doing pgsql -p 5555 and if that doesn't work, do nmap -sV to find out which of the other ports it is.\n",
                "(Of course, at its root this isn't a technical problem but a social one. Your DB admin should tell you what port to use; if they don't, your boss should make them do so. But I, too, have been in places where nmap is both faster and more reliable...)"
            ]
        }
    ],
    [
        {
            "id": "47-1-2",
            "pair": [
                "DO NOT TRY THE 3.3V realays with the rpi without a transistor. Switching a relay on requires quite a bit of current, and gpio pins can supply very little current(I think less than 8ma). if you try to operate a relay with a raspberry pi gpio pin, you will most likely break the gpio pin(and possibly other parts as well) on your pi. The solution is to use a transistor, as LotPings mentions. A transistor works by taking a very small current and using it to switch a much larger current. There is no way around using a transistor and diode to control a relay with the rpi, unless your relay is a relay module with a transistor already built in. As for the code, use python's rpi.gpio library. It is really simple. To get you started, google raspberry pi rpi.gpio hello world\n",
                "Hope this helps, and let me know if you have any questions!\n"
            ]
        },
        {
            "id": "47-2-3",
            "pair": [
                "Hope this helps, and let me know if you have any questions!\n",
                "Does anyone happen to have a similar example for a python code when pressing a button a relay is turned on which is powering a motor to open a gate for 5 seconds and then closes by giving a signal to another relay which is connected with opposite positive and negative to the motor to close the gate (   this picture shows how i an going to try and connect the motor and relays) would i also be possible to add motion detection so if there would be motion the second relay isn't powered so the gate wouldn't close "
            ]
        }
    ],
    [
        {
            "id": "48-1-2",
            "pair": [
                "I want to see what domain name is being looked up when I use Visual Studio's help (using the F1 key) because the first time I use it, I get a Unable to Service Request error page, but the second time I get the page I'm requesting.  This issue is documented here.\n",
                "I have a hosts file that blocks certain sites of questionable ethics, so I would like to know what site that I'm blocking is causing this without having to start unblocking many sites to stop this from happening and report my findings to that same bug.\n"
            ]
        },
        {
            "id": "48-2-3",
            "pair": [
                "I have a hosts file that blocks certain sites of questionable ethics, so I would like to know what site that I'm blocking is causing this without having to start unblocking many sites to stop this from happening and report my findings to that same bug.\n",
                "You could use wireshark to listen on port 53 (using capture filters, but mostly just port 53). It would give you an idea of which DNS requests are being made and answered in general (though you would maybe have to comb a bit through them if many applications are requesting in that same period.)\n"
            ]
        },
        {
            "id": "48-3-4",
            "pair": [
                "You could use wireshark to listen on port 53 (using capture filters, but mostly just port 53). It would give you an idea of which DNS requests are being made and answered in general (though you would maybe have to comb a bit through them if many applications are requesting in that same period.)\n",
                "This also requires you to have enough permissions on your machine (which you may not have, depending on the situation.)\n"
            ]
        },
        {
            "id": "48-4-5",
            "pair": [
                "This also requires you to have enough permissions on your machine (which you may not have, depending on the situation.)\n",
                "Still, it seems to be an issue with the time it takes for the DNS to answer. The second time it would be in the cache and it would be instant, the first one, however, takes far longer than the application expects."
            ]
        }
    ],
    [
        {
            "id": "49-1-2",
            "pair": [
                "I have a management pack that discovers and monitors certain objects. Everything is working as expected in the configuration where the direct communication between the agent and the management server is possible. \n",
                "The problem is when I'm using a SCOM Gateway configuration. Most of the functionalities remain unaffected, except for one discovery. This discovery creates the largest discovery data by far. This discovery discovers multiple instances of a single class at once, and for a small number of these instances the discovery works. When the number of objects reaches a certain number the discovery stops working. By using debug mode I can identify that the discovery is being triggered but the data doesnt seem to be written into the OM database. \n"
            ]
        },
        {
            "id": "49-2-3",
            "pair": [
                "The problem is when I'm using a SCOM Gateway configuration. Most of the functionalities remain unaffected, except for one discovery. This discovery creates the largest discovery data by far. This discovery discovers multiple instances of a single class at once, and for a small number of these instances the discovery works. When the number of objects reaches a certain number the discovery stops working. By using debug mode I can identify that the discovery is being triggered but the data doesnt seem to be written into the OM database. \n",
                "Is there some kind of limitation to the size of single discovery data object that is specific to SCOM Gateway? I would like to mention that the discovery for same number of objects is working when using standard Management Server, it only fails when we are using SCOM Gateway? I have managed to identify that the discovery certainly fails when the size of discovery data reaches ~20MB. Is there some kind of limitation that was placed for security reasons and can this limitation be overridden trough changing of some of the registry values related to SCOM?\n"
            ]
        },
        {
            "id": "49-3-4",
            "pair": [
                "Is there some kind of limitation to the size of single discovery data object that is specific to SCOM Gateway? I would like to mention that the discovery for same number of objects is working when using standard Management Server, it only fails when we are using SCOM Gateway? I have managed to identify that the discovery certainly fails when the size of discovery data reaches ~20MB. Is there some kind of limitation that was placed for security reasons and can this limitation be overridden trough changing of some of the registry values related to SCOM?\n",
                "Any help or further hints would be greatly appreciated. \n"
            ]
        },
        {
            "id": "49-4-5",
            "pair": [
                "Any help or further hints would be greatly appreciated. \n",
                "From my reading the Gateway server is a glorified Agent. So if you were to look into any limitations check out what specific limitations there may be for SCOM Agents which are acting as proxies perhaps?"
            ]
        }
    ],
    [
        {
            "id": "5-1-2",
            "pair": [
                "Moser's proof of the constructive Lovasz Local Lemma.  He basically shows that, under the conditions of the local lemma, the second-simplest algorithm for SAT you can think of works.  (The first simplest might be to just try a random assignment until one works.  The second simplest is took pick a random assignment, find an unsatisfied clause, satisfy it, then see what other clauses you broke, recurse, and repeat until done.)  The proof that this runs in polynomial time is perhaps the most elegant use of information theory (or Kolmogorov complexity, whatever you want to call it in this case) I've ever seen.\n",
                "Average-Case Analysis of Algorithms Using Kolmogorov Complexity by Jiang, Li, Vitanyi. \n"
            ]
        },
        {
            "id": "5-2-3",
            "pair": [
                "Average-Case Analysis of Algorithms Using Kolmogorov Complexity by Jiang, Li, Vitanyi. \n",
                "'Analyzing the average-case complexity of algorithms is a very practical but very difficult problem in computer science. In the past few years we have demonstrated that Kolmogorov complexity is an important tool for analyzing the average-case complexity of algorithms. We have developed the incompressibility method [7]. In this paper we use several simple examples to further demonstrate the power and simplicity of such method. We prove bounds on the average-case number of stacks (queues) required for sorting sequential or parallel Queueusort or Stacksort.' \n"
            ]
        },
        {
            "id": "5-3-4",
            "pair": [
                "'Analyzing the average-case complexity of algorithms is a very practical but very difficult problem in computer science. In the past few years we have demonstrated that Kolmogorov complexity is an important tool for analyzing the average-case complexity of algorithms. We have developed the incompressibility method [7]. In this paper we use several simple examples to further demonstrate the power and simplicity of such method. We prove bounds on the average-case number of stacks (queues) required for sorting sequential or parallel Queueusort or Stacksort.' \n",
                "See also e.g. Kolmogorov Complexity and a Triangle Problem of the Heilbronn Type."
            ]
        }
    ],
    [
        {
            "id": "50-1-2",
            "pair": [
                "I would say that the best option is to use Facebook ads or Google ads. I have had pretty good success using FB for advertising because you can set a daily spending limit and you can target your (fairly close) audience. I would also suggest a facebook page for your game. \n",
                "I personally am not a huge FB fan but I can vouch that doing that really helps.\n"
            ]
        },
        {
            "id": "50-2-3",
            "pair": [
                "I personally am not a huge FB fan but I can vouch that doing that really helps.\n",
                "Let me ask you: How do you intend to make money with your game?\n"
            ]
        },
        {
            "id": "50-3-4",
            "pair": [
                "Let me ask you: How do you intend to make money with your game?\n",
                "If your game just have advertisement as the revenue source, I highly discourage you to invest even $0.01 in marketing. You will spend money foolishly. I know because I've been there.\n"
            ]
        },
        {
            "id": "50-4-5",
            "pair": [
                "If your game just have advertisement as the revenue source, I highly discourage you to invest even $0.01 in marketing. You will spend money foolishly. I know because I've been there.\n",
                "Why? You will expend at least $0.10 / click. Suppose that 1,000 users click in ads you set to acquire users. You expended $100.\n"
            ]
        },
        {
            "id": "50-5-6",
            "pair": [
                "Why? You will expend at least $0.10 / click. Suppose that 1,000 users click in ads you set to acquire users. You expended $100.\n",
                "Now you must be aware that the eCPM of ads inside your game will pay you at most $25/1,000 clicks. But usually it's around $0.10/1,000  clicks.\n"
            ]
        },
        {
            "id": "50-6-7",
            "pair": [
                "Now you must be aware that the eCPM of ads inside your game will pay you at most $25/1,000 clicks. But usually it's around $0.10/1,000  clicks.\n",
                "If all acquired users clicked in the ads inside your game, you would get at most $25. A loss of $75.\n"
            ]
        },
        {
            "id": "50-7-8",
            "pair": [
                "If all acquired users clicked in the ads inside your game, you would get at most $25. A loss of $75.\n",
                "But notice that the CTR (how many clicks per 100 users apps/games usually get) is around 2%.\n"
            ]
        },
        {
            "id": "50-8-9",
            "pair": [
                "But notice that the CTR (how many clicks per 100 users apps/games usually get) is around 2%.\n",
                "So, you can expect that from 1,000 users you will get around 20 clicks. For the highest eCPM ($25/1,000 clicks) you would earn $0.50. A loss of $99.50.\n"
            ]
        },
        {
            "id": "50-9-10",
            "pair": [
                "So, you can expect that from 1,000 users you will get around 20 clicks. For the highest eCPM ($25/1,000 clicks) you would earn $0.50. A loss of $99.50.\n",
                "Let me repeat once more: only invest money in marketing if your application has in-app purchase in it and you expect to earn much more than you will invest.\n"
            ]
        },
        {
            "id": "50-10-11",
            "pair": [
                "Let me repeat once more: only invest money in marketing if your application has in-app purchase in it and you expect to earn much more than you will invest.\n",
                "Note: for in-app purchase, it's expected that around 7% of active user base will expend money. All other users won't expend anything."
            ]
        }
    ],
    [
        {
            "id": "51-1-2",
            "pair": [
                "What kind of routers are you using? It sounds like you're just using SOHO type routers? You might want to look at getting better routers and switches with management built in and monitorable through SNMP.\n",
                "That said I'd also put in a proxy server that can log activity and block certain traffic. Proxying can help some of your speed woes, blocking can limit others.\n"
            ]
        },
        {
            "id": "51-2-3",
            "pair": [
                "That said I'd also put in a proxy server that can log activity and block certain traffic. Proxying can help some of your speed woes, blocking can limit others.\n",
                "Upgraded routers can also handle traffic shaping and limiting, as well as QoS. If you must do it on the \"cheap\", you could start using a Linux box (there are several turnkey solutions) to do the traffic monitoring and shaping. Install, configure, set it as the gateway for everyone's system to route through. An inexpensive box can also do the proxying work for you, and you could have options for VPN access.\n"
            ]
        },
        {
            "id": "51-3-4",
            "pair": [
                "Upgraded routers can also handle traffic shaping and limiting, as well as QoS. If you must do it on the \"cheap\", you could start using a Linux box (there are several turnkey solutions) to do the traffic monitoring and shaping. Install, configure, set it as the gateway for everyone's system to route through. An inexpensive box can also do the proxying work for you, and you could have options for VPN access.\n",
                "We ran a SquidGuard box for awhile to filter and proxy traffic. Turned out it was also pretty good at helping track down certain malware on the network when we filtered for certain broadcasts that were scattering through the routing tables from a particular (infected) client. It was also great for getting browsing activity reports.\n"
            ]
        },
        {
            "id": "51-4-5",
            "pair": [
                "We ran a SquidGuard box for awhile to filter and proxy traffic. Turned out it was also pretty good at helping track down certain malware on the network when we filtered for certain broadcasts that were scattering through the routing tables from a particular (infected) client. It was also great for getting browsing activity reports.\n",
                "Just make sure any filtering or whatnot is allowed in your policies and employees are made aware of network monitoring. Sometimes it's the law, other times it's just a nice courtesy to your users to be reminded they're using company resources, not personal resources.\n"
            ]
        },
        {
            "id": "51-5-6",
            "pair": [
                "Just make sure any filtering or whatnot is allowed in your policies and employees are made aware of network monitoring. Sometimes it's the law, other times it's just a nice courtesy to your users to be reminded they're using company resources, not personal resources.\n",
                "We have been assessing the PacketLogic device from Procera Networks and it's been going very well."
            ]
        }
    ],
    [
        {
            "id": "52-1-2",
            "pair": [
                "The thing to keep in mind is that processes are not the same as windows. You may have a single process with many windows, or many processes with no windows at all. I'd agree this sounds like you have too many processes open.\n",
                "Try rebooting the machine. This will at the very least shut down all the extra processes. If it recurs, then you need to investigate what exactly is spawning all these processes and not closing them.\n"
            ]
        },
        {
            "id": "52-2-3",
            "pair": [
                "Try rebooting the machine. This will at the very least shut down all the extra processes. If it recurs, then you need to investigate what exactly is spawning all these processes and not closing them.\n",
                "In your Activity Monitor, change the dropdown at the top to All Processes and look at what is running. If you see the same process listed 10, 20, 50, or more times, then that is likely your culprit. (Some applications, like Google Chrome, do spawn many processes in normal operation. You should not have more instances than you do tabs, however).\n"
            ]
        },
        {
            "id": "52-3-4",
            "pair": [
                "In your Activity Monitor, change the dropdown at the top to All Processes and look at what is running. If you see the same process listed 10, 20, 50, or more times, then that is likely your culprit. (Some applications, like Google Chrome, do spawn many processes in normal operation. You should not have more instances than you do tabs, however).\n",
                "I'm getting an error message in the Mac Terminal when I try to run several different processes.  I did some googling and looking on this site, and found out that it might be related to having too many processes running at one time. However, I'm getting these error messages when I only have a few windows open (much fewer than I was accustomed to having). Looking in activity Monitor, my %User number is at around 25%, and the %System number is around 15%. In the past, I have had both much much higher (until the people at the Apple store told me to keep an eye on it). So with these numbers lower now, what explains the Resource temporarily unavailable error message?"
            ]
        }
    ],
    [
        {
            "id": "53-1-2",
            "pair": [
                "There's no such thing as a 'dual stack protocol'. \n",
                "Fundamentally, you need IPv4 support since your network dosen't 'talk' ipv6 natively at the segment you're in, and the bits must flow somehow.\n"
            ]
        },
        {
            "id": "53-2-3",
            "pair": [
                "Fundamentally, you need IPv4 support since your network dosen't 'talk' ipv6 natively at the segment you're in, and the bits must flow somehow.\n",
                "You want ipv6 cause, well, that's why you're tunneling, right? As such your network needs to support ipv6 from the system that's the tunnel endpoint, and any and all clients connected to that.\n"
            ]
        },
        {
            "id": "53-3-4",
            "pair": [
                "You want ipv6 cause, well, that's why you're tunneling, right? As such your network needs to support ipv6 from the system that's the tunnel endpoint, and any and all clients connected to that.\n",
                "Since ipv4 and ipv6 run seperately (not counting various 4 to 6 transition methods), having a dual stack setup is a consequence of the necessary pieces needed to run a tunnel, rather than something you need first. \n"
            ]
        },
        {
            "id": "53-4-5",
            "pair": [
                "Since ipv4 and ipv6 run seperately (not counting various 4 to 6 transition methods), having a dual stack setup is a consequence of the necessary pieces needed to run a tunnel, rather than something you need first. \n",
                "You can run dual stack without a tunnel broker (I do!), or run applications purely in ipv4 space (my Ubuntu systems often have ipv6 disabled for apt, cause its proven buggy) or run applications in purely ipv6 space, or any mix of the above.\n"
            ]
        },
        {
            "id": "53-5-6",
            "pair": [
                "You can run dual stack without a tunnel broker (I do!), or run applications purely in ipv4 space (my Ubuntu systems often have ipv6 disabled for apt, cause its proven buggy) or run applications in purely ipv6 space, or any mix of the above.\n",
                "It's because you need both IPv4 and IPv6. The tunnel uses IPv6, but the tunnel sources are IPv4 addresses. A tunnel is a virtual interface with it own protocol addressing, but it uses other, real interfaces with their own addressing.\n"
            ]
        },
        {
            "id": "53-6-7",
            "pair": [
                "It's because you need both IPv4 and IPv6. The tunnel uses IPv6, but the tunnel sources are IPv4 addresses. A tunnel is a virtual interface with it own protocol addressing, but it uses other, real interfaces with their own addressing.\n",
                "For example, your IPv6 tunnel needs IPv6 enabled for the IPv6 addressing on the tunnel, but the tunnel is a virtual interface that uses a real IPv4 interface to tunnel the IPv6 traffic across an IPv4 network."
            ]
        }
    ],
    [
        {
            "id": "54-1-2",
            "pair": [
                "In SQL Server it is best to store DataTime as one field. If you create an index on DataTime column it can be used as Date search and as DateTime search. Therefore if you need to limit all records that exist for the specific date, you can still use the index without having to do anything special. If you need to query for time portion you will not be able to use the same index and therefore if you have a business case where you care more about the time of the day than DateTime, you should store it separately as you will need to create an index on it and improve performance.\n",
                "Indeed, that's a pity there is no standard cross-DBMS type for this (like INT and VARCHAR are for integers and string values). The 2 cross-database approaches I have met so far are using VARCHAR/CHAR columns to store DataTime values as strings formatted according to the ISO 8601 (more convenient, human-readable) standard and using BIGINT to store them as POSIX timestamps (stored more efficiently, faster, easier to manipulate mathematically)."
            ]
        }
    ],
    [
        {
            "id": "55-1-2",
            "pair": [
                "I have the need to deploy Rails 3 apps, using RVM and gemsets, and am expecting \u201cpublic\u201d traffic (i.e. this is not an internal-only app).  I also must use Apache as the public interface to my app.\n",
                "I understand that Passenger Standalone can help accomplish the rails/RVM end, and I have successfully set it up in my development environment.\n"
            ]
        },
        {
            "id": "55-2-3",
            "pair": [
                "I understand that Passenger Standalone can help accomplish the rails/RVM end, and I have successfully set it up in my development environment.\n",
                "My question is how viable this setup is for a production deployment.  Is deploying via Apache configured to ReverseProxy to my passenger-powered Rails app going to create problems?  Since I'm designing the production deployment now, I want to understand if I should spend the additional time to set up Passenger connected to Apache and have that Passenger communicate with Passenger Standalone instance running my Rails app.\n"
            ]
        },
        {
            "id": "55-3-4",
            "pair": [
                "My question is how viable this setup is for a production deployment.  Is deploying via Apache configured to ReverseProxy to my passenger-powered Rails app going to create problems?  Since I'm designing the production deployment now, I want to understand if I should spend the additional time to set up Passenger connected to Apache and have that Passenger communicate with Passenger Standalone instance running my Rails app.\n",
                "I haven't used that specific setup in production, though I've used Apache's reverse proxy for a lightly loaded site (no a different non-Passenger backend).\n"
            ]
        },
        {
            "id": "55-4-5",
            "pair": [
                "I haven't used that specific setup in production, though I've used Apache's reverse proxy for a lightly loaded site (no a different non-Passenger backend).\n",
                "The only (mild) downside I found to Apache's reverse proxy was that if it detected the backend being down, it would take a short while after the backend was restarted before Apache would start forwarding requests to it again.\n"
            ]
        },
        {
            "id": "55-5-6",
            "pair": [
                "The only (mild) downside I found to Apache's reverse proxy was that if it detected the backend being down, it would take a short while after the backend was restarted before Apache would start forwarding requests to it again.\n",
                "If you have the ability to configure the Apache server for Passenger, it's not difficult to do, and once configured you don't need to administer Apache any more than before (other than for Passenger updates). When you update your Rails app, simply touching the tmp/restart.txt file will restart the Passenger instance."
            ]
        }
    ],
    [
        {
            "id": "56-1-2",
            "pair": [
                "Only 1% of the tables (that I see) use or need PARTITION.\n",
                "This may sound naive, however, a partitioned table is essentially no different from any other table from the outside:\n"
            ]
        },
        {
            "id": "56-2-3",
            "pair": [
                "This may sound naive, however, a partitioned table is essentially no different from any other table from the outside:\n",
                "The big difference is the speed at which a query can be run against a table when using an appropriate where clause.\n"
            ]
        },
        {
            "id": "56-3-4",
            "pair": [
                "The big difference is the speed at which a query can be run against a table when using an appropriate where clause.\n",
                "For example: if you have 1000 lego blocks, equally distributed between 3 different colors, and you are looking for one that you know is green and has your name written on it, you are only going to \"partition\" your search through the legos by looking only at the GREEN lego blocks. You are only going to look through 1/3 of your whole set. \n"
            ]
        },
        {
            "id": "56-4-5",
            "pair": [
                "For example: if you have 1000 lego blocks, equally distributed between 3 different colors, and you are looking for one that you know is green and has your name written on it, you are only going to \"partition\" your search through the legos by looking only at the GREEN lego blocks. You are only going to look through 1/3 of your whole set. \n",
                "Similarly, if you partition a table according to a certain column, date is always a good example, and you are trying to find an entry in that table where the approximate time is known, you mysql will only attempt to search only in a subset (a partition if you will) of the whole table. So to answer your question: ensure that if you DO use partitioning, that the majority of your queries will include a where clause referring to the partitioning key. \n"
            ]
        },
        {
            "id": "56-5-6",
            "pair": [
                "Similarly, if you partition a table according to a certain column, date is always a good example, and you are trying to find an entry in that table where the approximate time is known, you mysql will only attempt to search only in a subset (a partition if you will) of the whole table. So to answer your question: ensure that if you DO use partitioning, that the majority of your queries will include a where clause referring to the partitioning key. \n",
                "If my lego example was to abstract, or anything else sounds wierd: i would advice reading up on: http://dev.mysql.com/doc/refman/5.5/en/partitioning-overview.html"
            ]
        }
    ],
    [
        {
            "id": "57-1-2",
            "pair": [
                "If you are behind a NAT it is very likely that when the Teredo server tries to open a connection to you, but your NAT isn't forwarding the right port(s) on to you.\n",
                "I'd find out which ports are required, or if possible forward all ports to your machine to test that it is indeed a port forwarding issue.\n"
            ]
        },
        {
            "id": "57-2-3",
            "pair": [
                "I'd find out which ports are required, or if possible forward all ports to your machine to test that it is indeed a port forwarding issue.\n",
                "Check go6's wiki on Teredo and see if the section on \"Teredo IPv6 addressing\" gives you any insight to what else you might need to look into. You should be able to look at your assigned IPv6 address and then determine the ports you need forwarding.\n"
            ]
        },
        {
            "id": "57-3-4",
            "pair": [
                "Check go6's wiki on Teredo and see if the section on \"Teredo IPv6 addressing\" gives you any insight to what else you might need to look into. You should be able to look at your assigned IPv6 address and then determine the ports you need forwarding.\n",
                "Check if the machine with native IPv6 is not behind a stateful firewall which blocks icmpv6.\n"
            ]
        },
        {
            "id": "57-4-5",
            "pair": [
                "Check if the machine with native IPv6 is not behind a stateful firewall which blocks icmpv6.\n",
                "I had a similar problem, where a machine in a network with 6to4 was not able to connect to a machine using teredo in another network. Explicitly allowing icmpv6 in the IPv6 firewall in the 6to4 gateway (instead of just using ip6table's -m state --state ESTABLISHED,RELATED) was enough to cure the issue. This happens because teredo sends a IPv6 ping to the \"native\" (6to4 in this case) host, in a way iptables does not identifies as \"RELATED\", and will not work if that ping packet is dropped or rejected.\n"
            ]
        },
        {
            "id": "57-5-6",
            "pair": [
                "I had a similar problem, where a machine in a network with 6to4 was not able to connect to a machine using teredo in another network. Explicitly allowing icmpv6 in the IPv6 firewall in the 6to4 gateway (instead of just using ip6table's -m state --state ESTABLISHED,RELATED) was enough to cure the issue. This happens because teredo sends a IPv6 ping to the \"native\" (6to4 in this case) host, in a way iptables does not identifies as \"RELATED\", and will not work if that ping packet is dropped or rejected.\n",
                "Blocking ICMP in general tends to only cause headaches, but I (mistakenly as it turned out) believed netfilter's state tracker would be enough to allow all needed ICMP packets."
            ]
        }
    ],
    [
        {
            "id": "58-1-2",
            "pair": [
                "I have a program that I'm writing that needs to use a priority queue as part of an algorithm. I specifically need to order (String, Integer) pairs for example (Bread, 3), (Beer, 5), (Eggs,2), etc.\n",
                "I'd appreciate any comments on my code style and how I've written my class.\n"
            ]
        },
        {
            "id": "58-2-3",
            "pair": [
                "I'd appreciate any comments on my code style and how I've written my class.\n",
                "You don't need a custom constructor.  This will declare and initialize it.  \n"
            ]
        },
        {
            "id": "58-3-4",
            "pair": [
                "You don't need a custom constructor.  This will declare and initialize it.  \n",
                "I prefer the name heap to heapArray.  It's simpler and more accurate.  \n"
            ]
        },
        {
            "id": "58-4-5",
            "pair": [
                "I prefer the name heap to heapArray.  It's simpler and more accurate.  \n",
                "In the latest Java, you don't have to specify Pair<String, Integer> twice.  It's smart enough to figure it out if you just say <>.  \n"
            ]
        },
        {
            "id": "58-5-6",
            "pair": [
                "In the latest Java, you don't have to specify Pair<String, Integer> twice.  It's smart enough to figure it out if you just say <>.  \n",
                "In general, it is preferable to use interfaces as types rather than implementations.  Among other reasons, it allows you to change implementations easily.  \n"
            ]
        },
        {
            "id": "58-6-7",
            "pair": [
                "In general, it is preferable to use interfaces as types rather than implementations.  Among other reasons, it allows you to change implementations easily.  \n",
                "What's this do?  If it was called isEmpty, I'd think it was returning whether or not the heap was empty.  As is, I would expect empty to do something, perhaps clear the heap.  \n"
            ]
        },
        {
            "id": "58-7-8",
            "pair": [
                "What's this do?  If it was called isEmpty, I'd think it was returning whether or not the heap was empty.  As is, I would expect empty to do something, perhaps clear the heap.  \n",
                "Whatever it's supposed to do, it doesn't seem to be doing it.  \n"
            ]
        },
        {
            "id": "58-8-9",
            "pair": [
                "Whatever it's supposed to do, it doesn't seem to be doing it.  \n",
                "Changing != to > handles index values less than 1.  And it's free.  We're already doing a comparison.  Why not do the better one?  \n"
            ]
        },
        {
            "id": "58-9-10",
            "pair": [
                "Changing != to > handles index values less than 1.  And it's free.  We're already doing a comparison.  Why not do the better one?  \n",
                "We don't need getParentData.  We have to calculate parentIndex anyway, so we can just fetch directly.  \n"
            ]
        },
        {
            "id": "58-10-11",
            "pair": [
                "We don't need getParentData.  We have to calculate parentIndex anyway, so we can just fetch directly.  \n",
                "I added some extra whitespace, because I find code easier to read that way.  \n"
            ]
        },
        {
            "id": "58-11-12",
            "pair": [
                "I added some extra whitespace, because I find code easier to read that way.  \n",
                "We don't have to explicitly say that we want to put things in the last position.  That's how the single argument add works already.  \n"
            ]
        },
        {
            "id": "58-12-13",
            "pair": [
                "We don't have to explicitly say that we want to put things in the last position.  That's how the single argument add works already.  \n",
                "I personally am not crazy about the half-cuddled else {, and it's not the Java standard.  So I fully cuddled:  } else {.  \n"
            ]
        },
        {
            "id": "58-13-14",
            "pair": [
                "I personally am not crazy about the half-cuddled else {, and it's not the Java standard.  So I fully cuddled:  } else {.  \n",
                "We only create nullElement in the one edge case now.  The rest of the time, we don't bother.  But we can actually do better.  Consider \n"
            ]
        },
        {
            "id": "58-14-15",
            "pair": [
                "We only create nullElement in the one edge case now.  The rest of the time, we don't bother.  But we can actually do better.  Consider \n",
                "This will create the null element the one time you need it, at the beginning.  And this is the kind of thing that you do in a constructor.  \n"
            ]
        },
        {
            "id": "58-15-16",
            "pair": [
                "This will create the null element the one time you need it, at the beginning.  And this is the kind of thing that you do in a constructor.  \n",
                "You don't need the explicit super().  Java's smart enough to do that for you when you're just calling the default constructor.  \n"
            ]
        },
        {
            "id": "58-16-17",
            "pair": [
                "You don't need the explicit super().  Java's smart enough to do that for you when you're just calling the default constructor.  \n",
                "And because we previously changed shiftUp to handle the empty case, we don't need to prevent calling shiftUp in that case.  \n"
            ]
        },
        {
            "id": "58-17-18",
            "pair": [
                "And because we previously changed shiftUp to handle the empty case, we don't need to prevent calling shiftUp in that case.  \n",
                "I'm not sure that we need the null element.  The math is a little more complex without it but still doable.  \n"
            ]
        },
        {
            "id": "58-18-19",
            "pair": [
                "I'm not sure that we need the null element.  The math is a little more complex without it but still doable.  \n",
                "This way we set left and right before using them.  So we can't accidentally pick the wrong value one place.  \n"
            ]
        },
        {
            "id": "58-19-20",
            "pair": [
                "This way we set left and right before using them.  So we can't accidentally pick the wrong value one place.  \n",
                "If this is an exercise to improve your understanding of how a heap works, that's fine.  There's a reinvent-the-wheel tag that you can use to let us know.  But if you just needed to solve the problem, an easier way is to use a PriorityQueue with a custom Comparator or a custom type that extends Comparable.  Then Java will handle all the fiddly bits of managing the heap.  \n"
            ]
        },
        {
            "id": "58-20-21",
            "pair": [
                "If this is an exercise to improve your understanding of how a heap works, that's fine.  There's a reinvent-the-wheel tag that you can use to let us know.  But if you just needed to solve the problem, an easier way is to use a PriorityQueue with a custom Comparator or a custom type that extends Comparable.  Then Java will handle all the fiddly bits of managing the heap.  \n",
                "If the order is backwards, switch a and b in the return line.  \n"
            ]
        },
        {
            "id": "58-21-22",
            "pair": [
                "If the order is backwards, switch a and b in the return line.  \n",
                "I forget if you have to implement equals as well.  \n"
            ]
        },
        {
            "id": "58-22-23",
            "pair": [
                "I forget if you have to implement equals as well.  \n",
                "Then you can call queue.add and queue.poll to insert and remove items.  \n"
            ]
        },
        {
            "id": "58-23-24",
            "pair": [
                "Then you can call queue.add and queue.poll to insert and remove items.  \n",
                "You don't provide test code, so I haven't tried to compile and test any of this.  Beware of typos, etc."
            ]
        }
    ],
    [
        {
            "id": "59-1-2",
            "pair": [
                "It's correct as far as it goes, but there's more. The view vector goes from the shading point to the eye point, but it's normalized (divided by its length so that it has length 1). So if you're shading $p$ and the eye is at the origin, the view vector is $\\tfrac{-p}{|-p|}$.\n",
                "It's not always this simple, because the \"eye\" isn't always at the origin. In a scanline rasteriser, you use the view transform to move things to the origin, because that's the only way to make the projection work - but not in a ray-tracer. The camera can be anywhere in a ray-tracer, and when you trace secondary rays (for reflections, refractions, or some other effects), it's definitely not in the same place as the camera.\n"
            ]
        },
        {
            "id": "59-2-3",
            "pair": [
                "It's not always this simple, because the \"eye\" isn't always at the origin. In a scanline rasteriser, you use the view transform to move things to the origin, because that's the only way to make the projection work - but not in a ray-tracer. The camera can be anywhere in a ray-tracer, and when you trace secondary rays (for reflections, refractions, or some other effects), it's definitely not in the same place as the camera.\n",
                "In the case you're working on right now, it sounds like you can just normalize $-p$, but you need to remember that it's really the direction from the shading point to the eye point, so that you can do the right thing when the eye point isn't the origin.\n"
            ]
        },
        {
            "id": "59-3-4",
            "pair": [
                "In the case you're working on right now, it sounds like you can just normalize $-p$, but you need to remember that it's really the direction from the shading point to the eye point, so that you can do the right thing when the eye point isn't the origin.\n",
                "All the books and reference I have read say that the view vector is calculated by subtracting the point where eye is at, from the point where we want to calculate light. But since, eye is at (0, 0, 0) the view vector would be just the negative of the point where eye is at? Is my understanding correct?"
            ]
        }
    ],
    [
        {
            "id": "6-1-2",
            "pair": [
                "Your computer is not truly \"switched off\" when it is shut down.\n",
                "As a result there are a couple of low power lines that are always on. Specifically the \"5v_standby\" which is used to keep an amount of devices operational so that your computer can wake itself up. This would include timers, so that it can wake up on a schedule, or networks so it can support Wake-on-LAN.\n"
            ]
        },
        {
            "id": "6-2-3",
            "pair": [
                "As a result there are a couple of low power lines that are always on. Specifically the \"5v_standby\" which is used to keep an amount of devices operational so that your computer can wake itself up. This would include timers, so that it can wake up on a schedule, or networks so it can support Wake-on-LAN.\n",
                "What this means is that your power supply can not truly switch off either. This means that there is always some small amount of conversion from mains down to low voltages required.\n"
            ]
        },
        {
            "id": "6-3-4",
            "pair": [
                "What this means is that your power supply can not truly switch off either. This means that there is always some small amount of conversion from mains down to low voltages required.\n",
                "Chances are what you are hearing is the few small switch mode power supplies inductors \"coil whine\". This noise will always be there in your computer, but with the main CPU and components turned off the fans will stop and the computer is actually quiet enough for you to hear it.\n"
            ]
        },
        {
            "id": "6-4-5",
            "pair": [
                "Chances are what you are hearing is the few small switch mode power supplies inductors \"coil whine\". This noise will always be there in your computer, but with the main CPU and components turned off the fans will stop and the computer is actually quiet enough for you to hear it.\n",
                "I have recently build a new computer and every time the system is switched off there is a silent buzzing sound coming sound from my pc. Couldn't locate the exact source of sound but I am pretty sure it is not a PSU issue as I plugged a different one into the system and it does the same sound. I suspect the motherboard is making the sound, but I want to be sure before sending it back. "
            ]
        }
    ],
    [
        {
            "id": "60-1-2",
            "pair": [
                "ALB's have many other advantages than just SNI.. they support HTTP/2, path-based routing to multiple target groups (useful in some scenarios), and as far as I'm aware they're cheaper than classic ELB's.\n",
                "And yes, Elastic Beanstalk DOES support ALB's. Unfortunately, you can a) only select to use it at environment creation time, and b) only do this via the new UI. I've posted some gripes about UI and EB in general having really gotten into using it over the last few weeks. Hopefully they'll fix these issues in the future. In the meantime, you should at least be able to save your current environment's configuration, launch a new one from the saved configuration (changing the load balancer type before creation), and then switch the CNAMEs once it's up and running.\n"
            ]
        },
        {
            "id": "60-2-3",
            "pair": [
                "And yes, Elastic Beanstalk DOES support ALB's. Unfortunately, you can a) only select to use it at environment creation time, and b) only do this via the new UI. I've posted some gripes about UI and EB in general having really gotten into using it over the last few weeks. Hopefully they'll fix these issues in the future. In the meantime, you should at least be able to save your current environment's configuration, launch a new one from the saved configuration (changing the load balancer type before creation), and then switch the CNAMEs once it's up and running.\n",
                "Note also: your environment must be configured to use a VPC in order to use ALB's.\n"
            ]
        },
        {
            "id": "60-3-4",
            "pair": [
                "Note also: your environment must be configured to use a VPC in order to use ALB's.\n",
                "For a SaaS application where customers can use their own custom domain names, I will be using lets encrypt to generate the certs for customer domains.\n"
            ]
        },
        {
            "id": "60-4-5",
            "pair": [
                "For a SaaS application where customers can use their own custom domain names, I will be using lets encrypt to generate the certs for customer domains.\n",
                "I am currently using elastic beanstalk (which uses ELB).\n"
            ]
        },
        {
            "id": "60-5-6",
            "pair": [
                "I am currently using elastic beanstalk (which uses ELB).\n",
                "Is it possible for me to continue to use beanstalk?\n"
            ]
        },
        {
            "id": "60-6-7",
            "pair": [
                "Is it possible for me to continue to use beanstalk?\n",
                "Seeing as elastic load balancer will have to handle the cert verification, and my certs will be on my instances I guess I have to somehow route traffic to my instances myself.\n"
            ]
        },
        {
            "id": "60-7-8",
            "pair": [
                "Seeing as elastic load balancer will have to handle the cert verification, and my certs will be on my instances I guess I have to somehow route traffic to my instances myself.\n",
                "Are there any work arounds that would be worth while, could I do this?\n"
            ]
        },
        {
            "id": "60-8-9",
            "pair": [
                "Are there any work arounds that would be worth while, could I do this?\n",
                "The SSL will be terminated at my ec2 instance running haproxy."
            ]
        }
    ],
    [
        {
            "id": "61-1-2",
            "pair": [
                "If you have not already, take a look at a time series DBMS, since it is optimized for storing and querying data where the primary focus is the date/time type.  Typically time series databases are used for recording data in the minute/second/sub-second ranges, so I'm not sure if it is still appropriate for hourly increments.  That said, this type of DBMS seems to be worth looking into.  Currently InfluxDB seems to be the most established and widely used time series database.\n",
                "Clearly this is not a NoSQL problem, but I would suggest that while an RDBMS solution would work, I think an OLAP approach will fit much better and given the very limited data ranges involved, I would strongly suggest investigating the use of a column based DB rather then row based one. Think about it this way, you may have 1.7 billion pieces of data, but you still only need 5 bits to index every possible value of hour or day of month. \n"
            ]
        },
        {
            "id": "61-2-3",
            "pair": [
                "Clearly this is not a NoSQL problem, but I would suggest that while an RDBMS solution would work, I think an OLAP approach will fit much better and given the very limited data ranges involved, I would strongly suggest investigating the use of a column based DB rather then row based one. Think about it this way, you may have 1.7 billion pieces of data, but you still only need 5 bits to index every possible value of hour or day of month. \n",
                "I have experience with a similar problem domain where Sybase IQ (now SAP IQ) is used to store up to 300 million counters an hour of telecoms equipment performance management data, but I doubt if you have the budget for that sort of solution. In the open source arena, MariaDB ColumnStore is a very promising candidate, but I would recommend also investigating MonetDB.\n"
            ]
        },
        {
            "id": "61-3-4",
            "pair": [
                "I have experience with a similar problem domain where Sybase IQ (now SAP IQ) is used to store up to 300 million counters an hour of telecoms equipment performance management data, but I doubt if you have the budget for that sort of solution. In the open source arena, MariaDB ColumnStore is a very promising candidate, but I would recommend also investigating MonetDB.\n",
                "Since query performance is a major driver for you, give consideration to how queries will be phrased. This is where OLAP and RDBMS show their greatest differences:- with OLAP you normalize for query performance, not to reduce repetition, reduce storage or even to enforce consistency. So in addition to the original timestamp (you did remember to capture its timezone I hope?) have a separate field for the UTC timestamp, other ones for the date and time, and yet more for the year, month, day, hour, minute and UTC offset. If you have additional information about locations, feel free to keep that in a separate location table that can be looked up on demand and feel free to keep the key to that table in your main record but keep the full location name in your main table as well, after all, all possible locations still only take 10 bits to index and every reference you do not have to follow to get the data to be reported is time saved on your query. \n"
            ]
        },
        {
            "id": "61-4-5",
            "pair": [
                "Since query performance is a major driver for you, give consideration to how queries will be phrased. This is where OLAP and RDBMS show their greatest differences:- with OLAP you normalize for query performance, not to reduce repetition, reduce storage or even to enforce consistency. So in addition to the original timestamp (you did remember to capture its timezone I hope?) have a separate field for the UTC timestamp, other ones for the date and time, and yet more for the year, month, day, hour, minute and UTC offset. If you have additional information about locations, feel free to keep that in a separate location table that can be looked up on demand and feel free to keep the key to that table in your main record but keep the full location name in your main table as well, after all, all possible locations still only take 10 bits to index and every reference you do not have to follow to get the data to be reported is time saved on your query. \n",
                "As a final suggestion, use separate tables for popular aggregated data and use batch jobs to populate them, that way you don't have to repeat the exercise for each and every report that uses an aggregated value and makes queries that compare current to historical or historical to historical much easier and much, much faster."
            ]
        }
    ],
    [
        {
            "id": "62-1-2",
            "pair": [
                "Glenn mostly talks about physics-driven games, ie. first person shooters and driving games. These have different requirements to real time strategy games where precise unit positions at every logic step are important. So the communications strategies are necessarily different.\n",
                "\"Real-time\" means different things in different contexts. Games are not 'hard' real time in that if a message is late, the whole thing breaks. (At least, there is no good reason for a game to be so demanding, as a software-only system should be able to recover from processing delays, unlike a nuclear power station or a piece of medical equipment for example.) Games are really 'soft' or 'firm' real time. (Definitions at Wikipedia as usual.) The type of game makes a difference as to how quickly you need the information, whether you can lose information and get away with it, etc. Suffice to say that TCP is good enough for many games, but for other games, UDP is preferable. \n"
            ]
        },
        {
            "id": "62-2-3",
            "pair": [
                "\"Real-time\" means different things in different contexts. Games are not 'hard' real time in that if a message is late, the whole thing breaks. (At least, there is no good reason for a game to be so demanding, as a software-only system should be able to recover from processing delays, unlike a nuclear power station or a piece of medical equipment for example.) Games are really 'soft' or 'firm' real time. (Definitions at Wikipedia as usual.) The type of game makes a difference as to how quickly you need the information, whether you can lose information and get away with it, etc. Suffice to say that TCP is good enough for many games, but for other games, UDP is preferable. \n",
                "He would send enough information to reconstruct the relevant game state of any unit that has changed. \n"
            ]
        },
        {
            "id": "62-3-4",
            "pair": [
                "He would send enough information to reconstruct the relevant game state of any unit that has changed. \n",
                "Most games don't fulfil the criteria for 3, so they use 1 and 2 instead. Many RTS games however can, and do, make use of 3.\n"
            ]
        },
        {
            "id": "62-4-5",
            "pair": [
                "Most games don't fulfil the criteria for 3, so they use 1 and 2 instead. Many RTS games however can, and do, make use of 3.\n",
                "Also, it doesn't necessarily have to be \"every frame\". The concept of a frame is also nebulous. Is it a frame of rendering? Is it a batch of logic? Is it a frame of network data being sent? Do the three always align one-to-one or do you get variable graphics rate with fixed logic rates? Some games, especially real time strategy games like Starcraft 2, or games with replay ability (as you touch upon) like to keep everything in perfect lockstep by having regular network updates (which may or may not match 'frames' in other senses) but this is not a requirement for all games. Many games just send out updates on a semi-regular basis, depending on how far behind they're willing to let the clients run.\n"
            ]
        },
        {
            "id": "62-5-6",
            "pair": [
                "Also, it doesn't necessarily have to be \"every frame\". The concept of a frame is also nebulous. Is it a frame of rendering? Is it a batch of logic? Is it a frame of network data being sent? Do the three always align one-to-one or do you get variable graphics rate with fixed logic rates? Some games, especially real time strategy games like Starcraft 2, or games with replay ability (as you touch upon) like to keep everything in perfect lockstep by having regular network updates (which may or may not match 'frames' in other senses) but this is not a requirement for all games. Many games just send out updates on a semi-regular basis, depending on how far behind they're willing to let the clients run.\n",
                "Many games won't necessarily treat a rendering frame as a logical frame. They might have 60FPS in graphics but only have 10 logic updates a second, and send 1 network update for each one. But even 30 network updates per second is reasonable if you use the 'send input' method, certainly.\n"
            ]
        },
        {
            "id": "62-6-7",
            "pair": [
                "Many games won't necessarily treat a rendering frame as a logical frame. They might have 60FPS in graphics but only have 10 logic updates a second, and send 1 network update for each one. But even 30 network updates per second is reasonable if you use the 'send input' method, certainly.\n",
                "It's not so much that there are distinct techniques, but several different constraints on the systems, and the importance of each constraint will vary from game to game. So you just have to pick a system that works for you.\n"
            ]
        },
        {
            "id": "62-7-8",
            "pair": [
                "It's not so much that there are distinct techniques, but several different constraints on the systems, and the importance of each constraint will vary from game to game. So you just have to pick a system that works for you.\n",
                "The main technique you need to be aware of is the \"1500 archers\" technique. It was famously used by Age of Empires, but is also used in other games such as the (open source) OpenTTD (based on Transport Tycoon Deluxe).\n"
            ]
        },
        {
            "id": "62-8-9",
            "pair": [
                "The main technique you need to be aware of is the \"1500 archers\" technique. It was famously used by Age of Empires, but is also used in other games such as the (open source) OpenTTD (based on Transport Tycoon Deluxe).\n",
                "To be clear: using this technique, you do not need to send ANY game state while playing the game. The whole game state is sent at initial startup, connect and resync. The only things you need to regularly send are time signals and synch checks. Only player-commands are normally sent from the client to the server and vice versa. If a player executes no commands (as is the case on most ticks), no data need to be sent.\n"
            ]
        },
        {
            "id": "62-9-10",
            "pair": [
                "To be clear: using this technique, you do not need to send ANY game state while playing the game. The whole game state is sent at initial startup, connect and resync. The only things you need to regularly send are time signals and synch checks. Only player-commands are normally sent from the client to the server and vice versa. If a player executes no commands (as is the case on most ticks), no data need to be sent.\n",
                "http://www.gamasutra.com/view/feature/3094/1500_archers_on_a_288_network_.php\n"
            ]
        },
        {
            "id": "62-10-11",
            "pair": [
                "http://www.gamasutra.com/view/feature/3094/1500_archers_on_a_288_network_.php\n",
                "Update: Kylotan calls this \"technique 3\" in the answer."
            ]
        }
    ],
    [
        {
            "id": "63-1-2",
            "pair": [
                "I recently started playing with VM and saw that I have 2 options when creating a VM, I can use a Virtual Disk or a Physical Disk.\n",
                "I belive using the physical disk would be faster and better and I would like to confirm if this is true ?\n"
            ]
        },
        {
            "id": "63-2-3",
            "pair": [
                "I belive using the physical disk would be faster and better and I would like to confirm if this is true ?\n",
                "Another thing I was wondering is if I have to make the partitions myself of if the VMWare will do it for me ?\n"
            ]
        },
        {
            "id": "63-3-4",
            "pair": [
                "Another thing I was wondering is if I have to make the partitions myself of if the VMWare will do it for me ?\n",
                "For example if I have a 500GB disk that has 400 being used by the host OS so I have 100 spare size would it let me repartition those 100 or it would either ask for the entire 100 or for me to point an already created partition to be used ?\n"
            ]
        },
        {
            "id": "63-4-5",
            "pair": [
                "For example if I have a 500GB disk that has 400 being used by the host OS so I have 100 spare size would it let me repartition those 100 or it would either ask for the entire 100 or for me to point an already created partition to be used ?\n",
                "You may run into problems, like driver issues or booting issues! \n"
            ]
        },
        {
            "id": "63-5-6",
            "pair": [
                "You may run into problems, like driver issues or booting issues! \n",
                "http://www.vmware.com/support/ws55/doc/ws_disk_dualboot_scsi_issues.html\n"
            ]
        },
        {
            "id": "63-6-7",
            "pair": [
                "http://www.vmware.com/support/ws55/doc/ws_disk_dualboot_scsi_issues.html\n",
                "I have yet to actually try this, but most would use a physical disk as a storage disk for the virtual machine. If it's for the OS then you will get the same or maybe better performance by running it as a virtual disk. I have actually seen a higher disk performance score in the Windows Experience index on a virtual machine, as opposed to the host OS index score...\n"
            ]
        },
        {
            "id": "63-7-8",
            "pair": [
                "I have yet to actually try this, but most would use a physical disk as a storage disk for the virtual machine. If it's for the OS then you will get the same or maybe better performance by running it as a virtual disk. I have actually seen a higher disk performance score in the Windows Experience index on a virtual machine, as opposed to the host OS index score...\n",
                "If the 100GB is an unpartitioned (raw) part of the disk, then yes Vmware would partition it for you. If it's just unused space on your primary partition then you would need to shrink your main partition down to 400 and have the 100GB as raw.\n"
            ]
        },
        {
            "id": "63-8-9",
            "pair": [
                "If the 100GB is an unpartitioned (raw) part of the disk, then yes Vmware would partition it for you. If it's just unused space on your primary partition then you would need to shrink your main partition down to 400 and have the 100GB as raw.\n",
                "If it were me, I would format that 100GB partition as an area to store the VM files and virtual disks. "
            ]
        }
    ],
    [
        {
            "id": "64-1-2",
            "pair": [
                "Before anything : your code looks nice and seems to be properly commented which is a good point. It could have been a good idea to give a pointer to some reference (wikipedia or anything else) as your implementation does not seem to rely on the usual sieve but on some kind of optimisation of it.\n",
                "This being said, from my point of view, a bit too long as you are using too many blank lines and too many lines for comment. There is only so much text I can fit on my screens, I'd rather read this :\n"
            ]
        },
        {
            "id": "64-2-3",
            "pair": [
                "This being said, from my point of view, a bit too long as you are using too many blank lines and too many lines for comment. There is only so much text I can fit on my screens, I'd rather read this :\n",
                "Everything else I was about to say has just been said by vnp.\n"
            ]
        },
        {
            "id": "64-3-4",
            "pair": [
                "Everything else I was about to say has just been said by vnp.\n",
                "As there has already been said something on newlines and braces I will just highlight the \"normal java\" way of placing braces. As I highlighted in multiple answers of mine, the usual way is to place the opening brace on the same line as the opening statement and the closing brace on a separate line.\n"
            ]
        },
        {
            "id": "64-4-5",
            "pair": [
                "As there has already been said something on newlines and braces I will just highlight the \"normal java\" way of placing braces. As I highlighted in multiple answers of mine, the usual way is to place the opening brace on the same line as the opening statement and the closing brace on a separate line.\n",
                "And in this short code-sample there are again 3 things I want to put out a comment on:\n"
            ]
        },
        {
            "id": "64-5-6",
            "pair": [
                "And in this short code-sample there are again 3 things I want to put out a comment on:\n",
                "Single operation if-statements should have braces placed, even though they are not required. Why? Have you heard of apple's goto fail; bug? They forgot to place the braces and broke a core functionality of iOS.\n"
            ]
        },
        {
            "id": "64-6-7",
            "pair": [
                "Single operation if-statements should have braces placed, even though they are not required. Why? Have you heard of apple's goto fail; bug? They forgot to place the braces and broke a core functionality of iOS.\n",
                "Your Numbers[i] contains true. Why not evaluate that directly??\n"
            ]
        },
        {
            "id": "64-7-8",
            "pair": [
                "Your Numbers[i] contains true. Why not evaluate that directly??\n",
                "is actually exactly the same as the following, given Numbers is of type boolean[]\n"
            ]
        },
        {
            "id": "64-8-9",
            "pair": [
                "is actually exactly the same as the following, given Numbers is of type boolean[]\n",
                "the Naming convention for java fields, methods and variables is camelCase. This means they usually start with a lowercase letter:"
            ]
        }
    ],
    [
        {
            "id": "65-1-2",
            "pair": [
                "This seems similar to wanting to extend the range of Bad Router.\n",
                "If your Good Router has a Repeater mode, you can connect it to Bad Router, then have your devices connect to Good Router. The Good Router I use right now, ASUS RT-AC68U has this Repeater mode:\n"
            ]
        },
        {
            "id": "65-2-3",
            "pair": [
                "If your Good Router has a Repeater mode, you can connect it to Bad Router, then have your devices connect to Good Router. The Good Router I use right now, ASUS RT-AC68U has this Repeater mode:\n",
                "You'll be asked for the wireless password to Bad Router, then you can create your own WLAN.\n"
            ]
        },
        {
            "id": "65-3-4",
            "pair": [
                "You'll be asked for the wireless password to Bad Router, then you can create your own WLAN.\n",
                "This will let other devices continue to use Bad Router.\n"
            ]
        },
        {
            "id": "65-4-5",
            "pair": [
                "This will let other devices continue to use Bad Router.\n",
                "I wasn't very specific on my title, but I'll elaborate. For the purposes of my question, I'll call one router Bad Router and the other router Good Router (Which is true, btw). Here is the outline:\n"
            ]
        },
        {
            "id": "65-5-6",
            "pair": [
                "I wasn't very specific on my title, but I'll elaborate. For the purposes of my question, I'll call one router Bad Router and the other router Good Router (Which is true, btw). Here is the outline:\n",
                "I have the Good Router connect by Ethernet. I don't have physical access to the Bad Router (For ignorant non-parental fascistic reasons). However, I have wireless access to it (Including configuration). \n"
            ]
        },
        {
            "id": "65-6-7",
            "pair": [
                "I have the Good Router connect by Ethernet. I don't have physical access to the Bad Router (For ignorant non-parental fascistic reasons). However, I have wireless access to it (Including configuration). \n",
                "I want to know if I can connect the Good Router to the Bad Router, and then configure the Bad Router to directly connect my Good Router to the modem so it can handle everything (Almost bypassing it, if you will). The Bad Router should preferably stay with its own DHCP. Even though I want to bridge the Modem through the Bad Router to my Good Router, when reading the Wireless Bridging methods that I found on Google (And here) this is not what seems to be described.\n"
            ]
        },
        {
            "id": "65-7-8",
            "pair": [
                "I want to know if I can connect the Good Router to the Bad Router, and then configure the Bad Router to directly connect my Good Router to the modem so it can handle everything (Almost bypassing it, if you will). The Bad Router should preferably stay with its own DHCP. Even though I want to bridge the Modem through the Bad Router to my Good Router, when reading the Wireless Bridging methods that I found on Google (And here) this is not what seems to be described.\n",
                "Scheme: [Modem] --Bridge--> [Bad Router] --Bridge--> [Good Router] --DHCP/NAT--> Connected devices\n"
            ]
        },
        {
            "id": "65-8-9",
            "pair": [
                "Scheme: [Modem] --Bridge--> [Bad Router] --Bridge--> [Good Router] --DHCP/NAT--> Connected devices\n",
                "I'm asking the question in a generic way. Meaning that I'm not specifically limiting it to my routers  or firmware being used. I saw similar questions here and found other ways of doing what I asked in the title, but nothing specifically like this.\n"
            ]
        },
        {
            "id": "65-9-10",
            "pair": [
                "I'm asking the question in a generic way. Meaning that I'm not specifically limiting it to my routers  or firmware being used. I saw similar questions here and found other ways of doing what I asked in the title, but nothing specifically like this.\n",
                "Thanks for your attention, hopefully you can help :). \n"
            ]
        },
        {
            "id": "65-10-11",
            "pair": [
                "Thanks for your attention, hopefully you can help :). \n",
                "Found the solution. I was looking at the wrong way of doing it. It can be done if your firmware supports a \"client mode\" where it makes your second wireless router look like a regular device to the first. Then your second router will get the WAN from the first router, without even touching it. It can be easily done with dd-wrt, not limited. Next I'll try using my second band as an AP.\n"
            ]
        },
        {
            "id": "65-11-12",
            "pair": [
                "Found the solution. I was looking at the wrong way of doing it. It can be done if your firmware supports a \"client mode\" where it makes your second wireless router look like a regular device to the first. Then your second router will get the WAN from the first router, without even touching it. It can be easily done with dd-wrt, not limited. Next I'll try using my second band as an AP.\n",
                "P.s.: If you got across this post and wants a mode detailed explanation with links to guides and firmwares, just comment on this answer and I'll do so. "
            ]
        }
    ],
    [
        {
            "id": "66-1-2",
            "pair": [
                "Jsut a random thought that you might look into your DNS entry in the /etc/resolv.conf file and other networking places in the filesystem.Try to \"dig\" your nameserver and well as the router get the information out of that.\n",
                "As you said I would love to look into the /etc/sysconfig/network file and related places to hunt down.\n"
            ]
        },
        {
            "id": "66-2-3",
            "pair": [
                "As you said I would love to look into the /etc/sysconfig/network file and related places to hunt down.\n",
                "I have a couple of Centos linux servers, that have a very simple task, they run nginx + fastcgi for php , and some NFS mounts between them, readonly\n"
            ]
        },
        {
            "id": "66-3-4",
            "pair": [
                "I have a couple of Centos linux servers, that have a very simple task, they run nginx + fastcgi for php , and some NFS mounts between them, readonly\n",
                "They have some RPC commands to start some downloading processes with wget, nothing fancy , from a main server, but their behavior is very unstable, they simply go down, we tried to monitor ram , processor usage, even network connections, they don't load up so much, max network connections up to... 250 max, 15% processor usage and memory , well, doesn't even fill up, 2.5GB from 8GB max , \n"
            ]
        },
        {
            "id": "66-4-5",
            "pair": [
                "They have some RPC commands to start some downloading processes with wget, nothing fancy , from a main server, but their behavior is very unstable, they simply go down, we tried to monitor ram , processor usage, even network connections, they don't load up so much, max network connections up to... 250 max, 15% processor usage and memory , well, doesn't even fill up, 2.5GB from 8GB max , \n",
                "I have no ideea why can a linux server go down like that, they aren't even public servers, no domain names installed no public serving, for sites. \n"
            ]
        },
        {
            "id": "66-5-6",
            "pair": [
                "I have no ideea why can a linux server go down like that, they aren't even public servers, no domain names installed no public serving, for sites. \n",
                "The only thing that I've discovered was that if i didn't restart the network service every couple of hours or so... the servers were becoming very slow, starting apps very slow, but not repoting a high usage of resources...Maybe Centos doesn't free the timeout connections, or something like that...It's based on Red Hat right?\n"
            ]
        },
        {
            "id": "66-6-7",
            "pair": [
                "The only thing that I've discovered was that if i didn't restart the network service every couple of hours or so... the servers were becoming very slow, starting apps very slow, but not repoting a high usage of resources...Maybe Centos doesn't free the timeout connections, or something like that...It's based on Red Hat right?\n",
                "I'm not a linux expert , but I'm sure that there are a few guys out there that can easily have an answer to this , or even have some leads to what i can do ...\n"
            ]
        },
        {
            "id": "66-7-8",
            "pair": [
                "I'm not a linux expert , but I'm sure that there are a few guys out there that can easily have an answer to this , or even have some leads to what i can do ...\n",
                "I haven't installed snort, or other things to view if we have some DOS attacks, still the scheduled script that restarts the network each hour should put the system back online, and it doesn't....\n"
            ]
        },
        {
            "id": "66-8-9",
            "pair": [
                "I haven't installed snort, or other things to view if we have some DOS attacks, still the scheduled script that restarts the network each hour should put the system back online, and it doesn't....\n",
                "I've sent the tech to the site, and this is what the server shows on the screen when the server was down (see image link)\n"
            ]
        },
        {
            "id": "66-9-10",
            "pair": [
                "I've sent the tech to the site, and this is what the server shows on the screen when the server was down (see image link)\n",
                "http://ft.beejive.com/icq/648/480731375/img0328152705_kv044e.jpg\n"
            ]
        },
        {
            "id": "66-10-11",
            "pair": [
                "http://ft.beejive.com/icq/648/480731375/img0328152705_kv044e.jpg\n",
                "Which is very wierd, no kernel panic in messages, and i don't know what could it be"
            ]
        }
    ],
    [
        {
            "id": "67-1-2",
            "pair": [
                "There is a twofold problem at work here.  One cause can be a .DS_Store file containing UTF-8 characters inside of it in the local directory that is being read behind the scenes.  If this is the case, simply adding the encoding to the top of the file will fix it:\n",
                "A second cause can be how we get code from one place to another.  If you are on a Mac and you copy code and then paste it into a file with cat as follows:\n"
            ]
        },
        {
            "id": "67-2-3",
            "pair": [
                "A second cause can be how we get code from one place to another.  If you are on a Mac and you copy code and then paste it into a file with cat as follows:\n",
                "This can lead to an interesting issue.  The formatting, more specifically the leading space, looks Pythonic.  However, your tabs have been replaced with something containing wide characters that no amount of character encoding configuration can seem to resolve.  If you face this, simply replace all of the leading whitespace and Python will be very happy.\n"
            ]
        },
        {
            "id": "67-3-4",
            "pair": [
                "This can lead to an interesting issue.  The formatting, more specifically the leading space, looks Pythonic.  However, your tabs have been replaced with something containing wide characters that no amount of character encoding configuration can seem to resolve.  If you face this, simply replace all of the leading whitespace and Python will be very happy.\n",
                "I'm using OSX Yosemite on a Macbook Pro with a UK keyboard. When I type # into a Python file (using Alt+3) I sometimes, but not always, see this syntax error when I try to run the Python file:\n"
            ]
        },
        {
            "id": "67-4-5",
            "pair": [
                "I'm using OSX Yosemite on a Macbook Pro with a UK keyboard. When I type # into a Python file (using Alt+3) I sometimes, but not always, see this syntax error when I try to run the Python file:\n",
                "I can usually fix it by copying and pasting a hash character from elsewhere in the file. \n"
            ]
        },
        {
            "id": "67-5-6",
            "pair": [
                "I can usually fix it by copying and pasting a hash character from elsewhere in the file. \n",
                "I could fix it by manually setting every Python I ever work on to UTF-8 encoding:\n"
            ]
        },
        {
            "id": "67-6-7",
            "pair": [
                "I could fix it by manually setting every Python I ever work on to UTF-8 encoding:\n",
                "but that's not really practical when I'm working on other people's code. \n"
            ]
        },
        {
            "id": "67-7-8",
            "pair": [
                "but that's not really practical when I'm working on other people's code. \n",
                "Is there any way I can reset the value of this key to an ASCII-friendly # to avoid this error?"
            ]
        }
    ],
    [
        {
            "id": "68-1-2",
            "pair": [
                "The theoretical example where your first hop has 50% loss, but your second has none is quite unlikely, though covered above...   Were your WiFi link lossy to the first hop, it would remain lossy to subsequent hops.\n",
                "As for assigning fault based on traceroutes:   I've professionally done senior-level network operations for over a decade, with full access to switches and routers (error counts, traffic levels, cache flows -- all manners of stats and metrics -- giving me visibility well beyond what a simple traceroute could provide, yet determining where packet loss was occurring to a host on a remote network was still an inexact art, mostly because the return path from the other host was obscured:  Unless I had a traceroute back to me from the remote host, I could only guess at how return traffic was getting back to my workstation.  I shall suggest that beyond the first or second hop in your traceroutes, there's not much you can do (or make meaningful deductions) with ping and traceroute.\n"
            ]
        },
        {
            "id": "68-2-3",
            "pair": [
                "As for assigning fault based on traceroutes:   I've professionally done senior-level network operations for over a decade, with full access to switches and routers (error counts, traffic levels, cache flows -- all manners of stats and metrics -- giving me visibility well beyond what a simple traceroute could provide, yet determining where packet loss was occurring to a host on a remote network was still an inexact art, mostly because the return path from the other host was obscured:  Unless I had a traceroute back to me from the remote host, I could only guess at how return traffic was getting back to my workstation.  I shall suggest that beyond the first or second hop in your traceroutes, there's not much you can do (or make meaningful deductions) with ping and traceroute.\n",
                "The packetloss at your router will probably be an overloaded router dropping packets where normally an error packet would be generated. Try increasing the interval...see if there are changes. Beyond that the packet loss figures don't really mean anything to your connection with the destination ip."
            ]
        }
    ],
    [
        {
            "id": "69-1-2",
            "pair": [
                "The near plane parameter shall be strictly larger than zero.\n",
                "The smaller it is the more precision you burn close to the camera, and with zero, the projection matrix degenerates into unusability.\n"
            ]
        },
        {
            "id": "69-2-3",
            "pair": [
                "The smaller it is the more precision you burn close to the camera, and with zero, the projection matrix degenerates into unusability.\n",
                "If your geometry isn't where you want it to be when you use a conformant projection matrix, address that problem instead.\n"
            ]
        },
        {
            "id": "69-3-4",
            "pair": [
                "If your geometry isn't where you want it to be when you use a conformant projection matrix, address that problem instead.\n",
                "I have just completed my custom mesh class and my engine is very basic right now, but now I am facing this strange issue. I have posted the pictures bellow, it seems like z-buffer is not working properly. I am using the following for projection matrix\n"
            ]
        },
        {
            "id": "69-4-5",
            "pair": [
                "I have just completed my custom mesh class and my engine is very basic right now, but now I am facing this strange issue. I have posted the pictures bellow, it seems like z-buffer is not working properly. I am using the following for projection matrix\n",
                "If I give anything other than 0 for near plane the model barely shows up and if camera is zoomed the geometry seems to be appearing partly and disappearing. I know that my view matrix is ok because if I use 0 near plane the geometry is rendered correctly but then the flickering happens. I used simple HLSL posted bellow. If someone faced the same situation please assist me..\n"
            ]
        },
        {
            "id": "69-5-6",
            "pair": [
                "If I give anything other than 0 for near plane the model barely shows up and if camera is zoomed the geometry seems to be appearing partly and disappearing. I know that my view matrix is ok because if I use 0 near plane the geometry is rendered correctly but then the flickering happens. I used simple HLSL posted bellow. If someone faced the same situation please assist me..\n",
                "EDIT : I used blender to export mesh to custom format, although unlikely but is there any winding order issue causing this problem??"
            ]
        }
    ],
    [
        {
            "id": "7-1-2",
            "pair": [
                "Go into system preferences, Network, highlight Wifi on the left and then click on advanced bottom right.\n",
                "Remove all the wireless networks. Now click OK, then highlight the wifi on the right again and then click the '-' symbol on the wireless adapter. Restart then go back into system preferences, network, then click the + symbol and add the wireless adapter again. \n"
            ]
        },
        {
            "id": "7-2-3",
            "pair": [
                "Remove all the wireless networks. Now click OK, then highlight the wifi on the right again and then click the '-' symbol on the wireless adapter. Restart then go back into system preferences, network, then click the + symbol and add the wireless adapter again. \n",
                "Apply then switch on the wireless adapter and try to rejoin a network.\n"
            ]
        },
        {
            "id": "7-3-4",
            "pair": [
                "Apply then switch on the wireless adapter and try to rejoin a network.\n",
                "I'm having a problem with my MacBook Pro (13-inch, Mid 2012) to connect at the WIFI. The connection is available, but when I click on it, after waiting a while, it appear the diagnosis of connection. \n"
            ]
        },
        {
            "id": "7-4-5",
            "pair": [
                "I'm having a problem with my MacBook Pro (13-inch, Mid 2012) to connect at the WIFI. The connection is available, but when I click on it, after waiting a while, it appear the diagnosis of connection. \n",
                "After processing that (also if I interrupt it in the middle) and after switching off and on again the WIFI, the MacBook correctly connects to the network.\n"
            ]
        },
        {
            "id": "7-5-6",
            "pair": [
                "After processing that (also if I interrupt it in the middle) and after switching off and on again the WIFI, the MacBook correctly connects to the network.\n",
                "Unfortunately this solution is not permanent, in fact the next time I try to connect at the WIFI it happens the same.\n"
            ]
        },
        {
            "id": "7-6-7",
            "pair": [
                "Unfortunately this solution is not permanent, in fact the next time I try to connect at the WIFI it happens the same.\n",
                "I'm using an OpenFiber router by Wind (Italy) and I configured a double connection, one for the guests and one for me. Here some screenshots about the 192.168.1.1 configuration:\n"
            ]
        },
        {
            "id": "7-7-8",
            "pair": [
                "I'm using an OpenFiber router by Wind (Italy) and I configured a double connection, one for the guests and one for me. Here some screenshots about the 192.168.1.1 configuration:\n",
                "I don't have any problems with the Huawei P10, iPhone 8, iPad, MacBook Pro (13-inch, Early 2011). So this makes me think that the router configuration is fine, but that I have problems with my MacBook Pro."
            ]
        }
    ],
    [
        {
            "id": "70-1-2",
            "pair": [
                "your boot process sure calls an init script, you have to add the NFS mount on that script.\n",
                "The init script will receive the variables passed to the kernel at boot time, among the ones you find the NFS mounting parameters.\n"
            ]
        },
        {
            "id": "70-2-3",
            "pair": [
                "The init script will receive the variables passed to the kernel at boot time, among the ones you find the NFS mounting parameters.\n",
                "I want a client which loads kernel and file system from the server through network.\n"
            ]
        },
        {
            "id": "70-3-4",
            "pair": [
                "I want a client which loads kernel and file system from the server through network.\n",
                "Till now, On server side, i changed the settings in /etc/dhcp/dhcpd.conf to get an ip-address to the client. After that i executed service isc-dhcp-server restart command.\n"
            ]
        },
        {
            "id": "70-4-5",
            "pair": [
                "Till now, On server side, i changed the settings in /etc/dhcp/dhcpd.conf to get an ip-address to the client. After that i executed service isc-dhcp-server restart command.\n",
                "I built openwrt kernel image openwrt-x86-generic-vmlinuz by downloading necessary packages\n"
            ]
        },
        {
            "id": "70-5-6",
            "pair": [
                "I built openwrt kernel image openwrt-x86-generic-vmlinuz by downloading necessary packages\n",
                "I created one directory with name /tftpboot and i copied pxelinux.0 from /usr/lib/pxelinux.0 to /tftpboot\n"
            ]
        },
        {
            "id": "70-6-7",
            "pair": [
                "I created one directory with name /tftpboot and i copied pxelinux.0 from /usr/lib/pxelinux.0 to /tftpboot\n",
                "And i also copied openwrt-x86-generic-vmlinuz in /tftpboot\n"
            ]
        },
        {
            "id": "70-7-8",
            "pair": [
                "And i also copied openwrt-x86-generic-vmlinuz in /tftpboot\n",
                "Inside /tftpboot i created one directory with name pxelinux.cfg\n"
            ]
        },
        {
            "id": "70-8-9",
            "pair": [
                "Inside /tftpboot i created one directory with name pxelinux.cfg\n",
                "append boot=nfs root=/dev/nfs nfsroot=nfs:server-ip:/tftpboot ip=dhcp rw\n"
            ]
        },
        {
            "id": "70-9-10",
            "pair": [
                "append boot=nfs root=/dev/nfs nfsroot=nfs:server-ip:/tftpboot ip=dhcp rw\n",
                "At client side , i restarted system and i selected the boot options to pxe booting\n"
            ]
        },
        {
            "id": "70-10-11",
            "pair": [
                "At client side , i restarted system and i selected the boot options to pxe booting\n",
                "It is loading the kernel openwrt-x86-generic-vmlinuz smoothly . Not mounting the file system which i had given in /etc/exports file\n"
            ]
        },
        {
            "id": "70-11-12",
            "pair": [
                "It is loading the kernel openwrt-x86-generic-vmlinuz smoothly . Not mounting the file system which i had given in /etc/exports file\n",
                "Now my question is how can i mount the filesystem from server to client while booting\n"
            ]
        },
        {
            "id": "70-12-13",
            "pair": [
                "Now my question is how can i mount the filesystem from server to client while booting\n",
                "or any other method to mount file system while booting"
            ]
        }
    ],
    [
        {
            "id": "71-1-2",
            "pair": [
                "This won't be a very satisfying answer, but here's my take...\n",
                "Same answer for both: you can't do this for unknown properties, and for known properties it will depend on how the values were computed.\n"
            ]
        },
        {
            "id": "71-2-3",
            "pair": [
                "Same answer for both: you can't do this for unknown properties, and for known properties it will depend on how the values were computed.\n",
                "There is no single transformation that will be appropriate in all cases, whether the properties/values are known or unknown. Even with known properties, you'll likely need a unique transformation for each type: mean, median, mode, min, max, boolean, etc.\n"
            ]
        },
        {
            "id": "71-3-4",
            "pair": [
                "There is no single transformation that will be appropriate in all cases, whether the properties/values are known or unknown. Even with known properties, you'll likely need a unique transformation for each type: mean, median, mode, min, max, boolean, etc.\n",
                "Whenever possible, try to preserve the full granularity of the smallest possible step. Assuming you know how to transform the values, you can always roll-up the steps (e.g., day to month, month to year)... but you won't necessarily be able to reconstruct smaller steps from larger ones following a lossy conversion.\n"
            ]
        },
        {
            "id": "71-4-5",
            "pair": [
                "Whenever possible, try to preserve the full granularity of the smallest possible step. Assuming you know how to transform the values, you can always roll-up the steps (e.g., day to month, month to year)... but you won't necessarily be able to reconstruct smaller steps from larger ones following a lossy conversion.\n",
                "For example, you have the number of time people searched for 'widgets' every day.  Add up the daily totals for a month to get monthly totals.  I would need to see more specifics about the actual data collected at each granularity to give you a more complete version.\n"
            ]
        },
        {
            "id": "71-5-6",
            "pair": [
                "For example, you have the number of time people searched for 'widgets' every day.  Add up the daily totals for a month to get monthly totals.  I would need to see more specifics about the actual data collected at each granularity to give you a more complete version.\n",
                "In physics, a comparable idea is the Nyquist frequency.  The general idea is that you can't add more information than what you already have present in your data without bringing in more data.  Given only the day someone ran a query, how can you tell what time of day that query was ran?  You may be able to make some inferences, but the only way to answer the question is to directly or indirectly bring in more information to the system.  There are things you can do to make informed guesses at the daily state of monthly variables (as \n"
            ]
        },
        {
            "id": "71-6-7",
            "pair": [
                "In physics, a comparable idea is the Nyquist frequency.  The general idea is that you can't add more information than what you already have present in your data without bringing in more data.  Given only the day someone ran a query, how can you tell what time of day that query was ran?  You may be able to make some inferences, but the only way to answer the question is to directly or indirectly bring in more information to the system.  There are things you can do to make informed guesses at the daily state of monthly variables (as \n",
                "gchaks mentioned, interpolation), but your data is still fundamentally monthly data stretched to look daily.\n"
            ]
        },
        {
            "id": "71-7-8",
            "pair": [
                "gchaks mentioned, interpolation), but your data is still fundamentally monthly data stretched to look daily.\n",
                "That totally depends on what you're trying to answer.  \n"
            ]
        },
        {
            "id": "71-8-9",
            "pair": [
                "That totally depends on what you're trying to answer.  \n",
                "The smaller granularity will be more sensitive to noise and other anomalies.  The lager granularity will be able to answer questions more confidently, but loose some of it's usefulness.  For example, if you're trying to see when people start looking up venues to weekend plans to know when to launch marketing campaigns for a new night club, you'll want to be looking at daily data, if not smaller.  If you're looking at the general trending of night clubs to figure out who you want to invest in, then monthly would probably be better."
            ]
        }
    ],
    [
        {
            "id": "72-1-2",
            "pair": [
                "Use the route command to show your routing table. Typically, your computer will have a default route (possibly the last network interface added) through which all your traffic will be routed.\n",
                "The default route will have a destination of 0.0.0.0 or maybe just the word 'default'. That's where your traffic is going through. You can change it with the route command also.\n"
            ]
        },
        {
            "id": "72-2-3",
            "pair": [
                "The default route will have a destination of 0.0.0.0 or maybe just the word 'default'. That's where your traffic is going through. You can change it with the route command also.\n",
                "http://www.cyberciti.biz/faq/linux-setup-default-gateway-with-route-command/\n"
            ]
        },
        {
            "id": "72-3-4",
            "pair": [
                "http://www.cyberciti.biz/faq/linux-setup-default-gateway-with-route-command/\n",
                "It may be possible to configure individual programs to bind to different network interfaces.\n"
            ]
        },
        {
            "id": "72-4-5",
            "pair": [
                "It may be possible to configure individual programs to bind to different network interfaces.\n",
                "You may also look into bonding your network interfaces but I have never tried it.\n"
            ]
        },
        {
            "id": "72-5-6",
            "pair": [
                "You may also look into bonding your network interfaces but I have never tried it.\n",
                "So I'm currently connected to two internet connections. My 3G network and the company's wireless connection. But I don't really understand which one I'm using. Is there a way to find out which connection is being used. And is there a way to use both at the same time. Either by load balancing or dedicating each to separate tasks. For instance, one for downloading, one for surfing?"
            ]
        }
    ],
    [
        {
            "id": "73-1-2",
            "pair": [
                "I'm building a script that will download and install a bunch of programs. One of those programs depends on a virtual driver, which needs its certificate to be imported in order to function. Normally, the installer does this for you in the GUI and just pops up a confirmation dialog asking if you trust this driver. However, that interruption is unacceptable for the script I'm writing.\n",
                "I have found a way to export the certificate via the file properties GUI and can import that file with the script, thus allowing me to install without any user interaction. However, in order to deploy and fully automate this script, I need to also be able to export the certificate file from the installer via the script. Can this be done?\n"
            ]
        },
        {
            "id": "73-2-3",
            "pair": [
                "I have found a way to export the certificate via the file properties GUI and can import that file with the script, thus allowing me to install without any user interaction. However, in order to deploy and fully automate this script, I need to also be able to export the certificate file from the installer via the script. Can this be done?\n",
                "The whole process can be done in powershell using the X509Certificate class available in the .Net framework.\n"
            ]
        },
        {
            "id": "73-3-4",
            "pair": [
                "The whole process can be done in powershell using the X509Certificate class available in the .Net framework.\n",
                "The first thing you need to do is get the cert file from the signed file. This is done using the CreateFromCertFile function. Just take note that the function can only take full paths and not relative ones.\n"
            ]
        },
        {
            "id": "73-4-5",
            "pair": [
                "The first thing you need to do is get the cert file from the signed file. This is done using the CreateFromCertFile function. Just take note that the function can only take full paths and not relative ones.\n",
                "You can then open up the certificate store on the local computer and import the certificate. To write into the Local Machine store, this needs to be executed as an administrator.\n"
            ]
        },
        {
            "id": "73-5-6",
            "pair": [
                "You can then open up the certificate store on the local computer and import the certificate. To write into the Local Machine store, this needs to be executed as an administrator.\n",
                "FYI, I used this method to install VirtualBox Guest Additions that also had driver prompts."
            ]
        }
    ],
    [
        {
            "id": "74-1-2",
            "pair": [
                "You can't effectively get such a list.  You can use inotifywatch if you are using linux and your linux distribution supports it to see files being created and deleted in a directory in realtime, but not historically (such logs are not kept).\n",
                "You should consider checking your filesystem for consistency if files are being lost.  In ext2 and some other filesystems they will appear in the lost+found folder in the partition root, but the exact way to identify this depends on your filesystem.\n"
            ]
        },
        {
            "id": "74-2-3",
            "pair": [
                "You should consider checking your filesystem for consistency if files are being lost.  In ext2 and some other filesystems they will appear in the lost+found folder in the partition root, but the exact way to identify this depends on your filesystem.\n",
                "Check also whether you have a periodic job running which might delete them for some reason (such as by restoring a directory from a backup).\n"
            ]
        },
        {
            "id": "74-3-4",
            "pair": [
                "Check also whether you have a periodic job running which might delete them for some reason (such as by restoring a directory from a backup).\n",
                "i have a image sharing website , users log and upload image \n"
            ]
        },
        {
            "id": "74-4-5",
            "pair": [
                "i have a image sharing website , users log and upload image \n",
                "last night i've lost about 30 newly-consecutive uploaded images ... i mean they have been uploaded ... apparently ... they are in the database but the actual image on the server is gone ! \n"
            ]
        },
        {
            "id": "74-5-6",
            "pair": [
                "last night i've lost about 30 newly-consecutive uploaded images ... i mean they have been uploaded ... apparently ... they are in the database but the actual image on the server is gone ! \n",
                "error log doesn't show anything ... so i thought my best option is to check list of created and deleted files ... if there is any !\n"
            ]
        },
        {
            "id": "74-6-7",
            "pair": [
                "error log doesn't show anything ... so i thought my best option is to check list of created and deleted files ... if there is any !\n",
                "is there a log file for created and deleted files on the server ? i'm using directadmin"
            ]
        }
    ],
    [
        {
            "id": "75-1-2",
            "pair": [
                "The good news is that your script doesn't look exploitable.  In particular, stripping out line break characters protects against header-splitting attacks.  Good job there.\n",
                "The bad news is that it can mangle the text unnecessarily.  For example, if a user submits the form with the subject line\n"
            ]
        },
        {
            "id": "75-2-3",
            "pair": [
                "The bad news is that it can mangle the text unnecessarily.  For example, if a user submits the form with the subject line\n",
                "And for what gain?  strip_tags() is meant as a feeble defense against inappropriate HTML tags, but here you are sending plain text mail \u2014 a problem that has absolutely nothing to do with HTML.\n"
            ]
        },
        {
            "id": "75-3-4",
            "pair": [
                "And for what gain?  strip_tags() is meant as a feeble defense against inappropriate HTML tags, but here you are sending plain text mail \u2014 a problem that has absolutely nothing to do with HTML.\n",
                "It also probably does not make sense to strip all line termination characters from the message.\n"
            ]
        },
        {
            "id": "75-4-5",
            "pair": [
                "It also probably does not make sense to strip all line termination characters from the message.\n",
                "A more general concern I have is your use of the term sanitize, as it leads to confusion.  I recommend striking that word from your programming vocabulary (even if the PHP documentation uses it), to be replaced by three specific terms:\n"
            ]
        },
        {
            "id": "75-5-6",
            "pair": [
                "A more general concern I have is your use of the term sanitize, as it leads to confusion.  I recommend striking that word from your programming vocabulary (even if the PHP documentation uses it), to be replaced by three specific terms:\n",
                "Canonicalization provides user-friendliness.  Validation enforces your business logic.  Escaping, not canonicalization or validation, upholds security.  The term \"sanitize\" conflates the three mechanisms, leading you do write improperly engineered code.\n"
            ]
        },
        {
            "id": "75-6-7",
            "pair": [
                "Canonicalization provides user-friendliness.  Validation enforces your business logic.  Escaping, not canonicalization or validation, upholds security.  The term \"sanitize\" conflates the three mechanisms, leading you do write improperly engineered code.\n",
                "If I pass name=test@fake.com\\nTo: spam@someone.com the newline will not be filtered out. You should use \\\\n instead of \\n, or just nl2br."
            ]
        }
    ],
    [
        {
            "id": "76-1-2",
            "pair": [
                "I'm trying to setup password-less SSH login, and I can't seem to make it work. Here is what I have done so far:\n",
                "When I attempt to login with the private key I get the following output from ssh -vvv:\n"
            ]
        },
        {
            "id": "76-2-3",
            "pair": [
                "When I attempt to login with the private key I get the following output from ssh -vvv:\n",
                "I'm pretty sure the server is running FreeBSD, but it's not mine, and I don't have root access. Any idea on what's going wrong here or what I can try to get this working?\n"
            ]
        },
        {
            "id": "76-3-4",
            "pair": [
                "I'm pretty sure the server is running FreeBSD, but it's not mine, and I don't have root access. Any idea on what's going wrong here or what I can try to get this working?\n",
                "Well, the \"solution\" to this wound up being related to the host. The server is run by a company called HybridCluster. I'm still not exactly sure why, but in order to use public key authentication with their servers, the public key has to be added via their control panel interface and not manually to the ~/.ssh/authorized_keys file.\n"
            ]
        },
        {
            "id": "76-4-5",
            "pair": [
                "Well, the \"solution\" to this wound up being related to the host. The server is run by a company called HybridCluster. I'm still not exactly sure why, but in order to use public key authentication with their servers, the public key has to be added via their control panel interface and not manually to the ~/.ssh/authorized_keys file.\n",
                "I think it has something to do with them dynamically switching hardware, so the host fingerprint changes which causes the key to be rejected, but I really can't say for sure.\n"
            ]
        },
        {
            "id": "76-5-6",
            "pair": [
                "I think it has something to do with them dynamically switching hardware, so the host fingerprint changes which causes the key to be rejected, but I really can't say for sure.\n",
                "Anyway, thanks to all who helped, and hopefully this will help some poor HybridCluster user in the future.\n"
            ]
        },
        {
            "id": "76-6-7",
            "pair": [
                "Anyway, thanks to all who helped, and hopefully this will help some poor HybridCluster user in the future.\n",
                "Why did you chmod your HOME folder on the server? There is no need to do that, and I think it might hamper your connection attempts. Please restore it to 755, and try again. "
            ]
        }
    ],
    [
        {
            "id": "77-1-2",
            "pair": [
                "The dimming of the screen is a Windows feature, but it does not include the fade in/out option that you mentioned.  To change the settings on this, you just need to adjust the power plan settings from Control Panel:\n",
                "Depending on your setup, you may need to go into the advanced options to access the Dim the display setting.  If your computer has in fact done the fading when dimming the screen, then that's something specific to your model, as I have never seen it before.  What I have seen, and what happens on all my Dell and Lenovo laptops, is that when pressing the sleep button, it does fade out the screen (and to a lesser extent, fades it in when resuming).  My guess is that you have seen the fading on Sleep with your computer, and on dim with a Mac, and assumed that it happened that way on dimming for your computer.  \n"
            ]
        },
        {
            "id": "77-2-3",
            "pair": [
                "Depending on your setup, you may need to go into the advanced options to access the Dim the display setting.  If your computer has in fact done the fading when dimming the screen, then that's something specific to your model, as I have never seen it before.  What I have seen, and what happens on all my Dell and Lenovo laptops, is that when pressing the sleep button, it does fade out the screen (and to a lesser extent, fades it in when resuming).  My guess is that you have seen the fading on Sleep with your computer, and on dim with a Mac, and assumed that it happened that way on dimming for your computer.  \n",
                "It doesn't sound windows specific.  Reading through the above solutions others gave the only other thing I could think of is that there MIGHT be a setting in your BIOS settings.  Also, if you have any specific software for settings that only exist for that machine.  Sometimes if Dell or ASUS puts out hardware that is very specific to that machine software often comes with it in order to tweak the settings on it."
            ]
        }
    ],
    [
        {
            "id": "78-1-2",
            "pair": [
                "For your laptop and supported memory:  https://store.hp.com/us/en/pdp/hp-elitebook-745-g5-notebook-pc-customizable-2mg22av-mb\n",
                "For your memory-rank question:  more ranks equals more performance: https://serverfault.com/questions/69612/dimms-single-vs-double-vs-quad-rank\n"
            ]
        },
        {
            "id": "78-2-3",
            "pair": [
                "For your memory-rank question:  more ranks equals more performance: https://serverfault.com/questions/69612/dimms-single-vs-double-vs-quad-rank\n",
                "For your question on RAS/CAS timings:  as you noted, the memory will only operate at the fastest speed supported by the motherboard.  There seems to be people who claim they are working without problems using faster RAM and many others who claim it causes system instability/crashes.  This is likely due to the fact that the stability is relative to the chipset and chipset drivers being able to appropriate handle the incorrect hardware for the mainboard. \n"
            ]
        },
        {
            "id": "78-3-4",
            "pair": [
                "For your question on RAS/CAS timings:  as you noted, the memory will only operate at the fastest speed supported by the motherboard.  There seems to be people who claim they are working without problems using faster RAM and many others who claim it causes system instability/crashes.  This is likely due to the fact that the stability is relative to the chipset and chipset drivers being able to appropriate handle the incorrect hardware for the mainboard. \n",
                " Because of this large disparity, I'd recommend using what your mobo supports.  Or, if you want faster RAM, get a better mobo.\n"
            ]
        },
        {
            "id": "78-4-5",
            "pair": [
                " Because of this large disparity, I'd recommend using what your mobo supports.  Or, if you want faster RAM, get a better mobo.\n",
                "My HP Elitebook 745 G5 has two SODIMM RAM slots feeding it's dual channel memory controller.\n"
            ]
        },
        {
            "id": "78-5-6",
            "pair": [
                "My HP Elitebook 745 G5 has two SODIMM RAM slots feeding it's dual channel memory controller.\n",
                "Apparently there is a performance increase for either Single or Dual Rank SODIMMs.\n"
            ]
        },
        {
            "id": "78-6-7",
            "pair": [
                "Apparently there is a performance increase for either Single or Dual Rank SODIMMs.\n",
                "I'm not sure which one is faster though, Single or Dual.\n"
            ]
        },
        {
            "id": "78-7-8",
            "pair": [
                "I'm not sure which one is faster though, Single or Dual.\n",
                "Crucial 16GB Kit (2 x 8GB) DDR4-2400 SODIMM (Single Ranked)\n"
            ]
        },
        {
            "id": "78-8-9",
            "pair": [
                "Crucial 16GB Kit (2 x 8GB) DDR4-2400 SODIMM (Single Ranked)\n",
                "Crucial 16GB Kit (2 x 8GB) DDR4-2400 SODIMM (Dual Ranked)\n"
            ]
        },
        {
            "id": "78-9-10",
            "pair": [
                "Crucial 16GB Kit (2 x 8GB) DDR4-2400 SODIMM (Dual Ranked)\n",
                "Crucial 32GB Kit (2 x 16GB) DDR4-2400 SODIMM (Dual Ranked)\n"
            ]
        },
        {
            "id": "78-10-11",
            "pair": [
                "Crucial 32GB Kit (2 x 16GB) DDR4-2400 SODIMM (Dual Ranked)\n",
                "I will buy the fastest, followed by whatever's the largest.\n"
            ]
        },
        {
            "id": "78-11-12",
            "pair": [
                "I will buy the fastest, followed by whatever's the largest.\n",
                "The RAM controller is limited to 2400MHZ, but they also sell 2666MHz sticks. They won't run at 2666MHz, but I was wondering, if I bought 2666MHz SODIMMs and it runs them at 2400MHz, would I be able to reduce the RAS/CAS timings? That used to work back in the day, but I haven't built a PC since the DDR2 days!"
            ]
        }
    ],
    [
        {
            "id": "79-1-2",
            "pair": [
                "Library functions, particularly Maybe's Applicative/Alternative instances, can make your code more consise:\n",
                "A more potent approach is to return not the length at which to split, but the results of the split. In fact, StateT String Maybe String makes this work out of the box:\n"
            ]
        },
        {
            "id": "79-2-3",
            "pair": [
                "A more potent approach is to return not the length at which to split, but the results of the split. In fact, StateT String Maybe String makes this work out of the box:\n",
                "For some context, I'm trying to write a streaming tokenizer in Haskell and noticed that I was writing a lot of functions with the type signature String -> Maybe Int that attempted to consume \"the longest possible token matching a particular pattern\" from the input stream. The pattern might be something like a quoted string literal in JSON.\n"
            ]
        },
        {
            "id": "79-3-4",
            "pair": [
                "For some context, I'm trying to write a streaming tokenizer in Haskell and noticed that I was writing a lot of functions with the type signature String -> Maybe Int that attempted to consume \"the longest possible token matching a particular pattern\" from the input stream. The pattern might be something like a quoted string literal in JSON.\n",
                "I tried using Parsec for a while to get a \"prefix matcher\", but I kept getting tripped up over what the appropriate userstate is supposed, and what the m argument in ParsecT means. Parsec is probably a strict superset of the functionality that this little library exposes.\n"
            ]
        },
        {
            "id": "79-4-5",
            "pair": [
                "I tried using Parsec for a while to get a \"prefix matcher\", but I kept getting tripped up over what the appropriate userstate is supposed, and what the m argument in ParsecT means. Parsec is probably a strict superset of the functionality that this little library exposes.\n",
                "A \"Matcher\" is anything with the type String -> Maybe Int. Matchers are supposed to be greedy and produce the longest prefix that they possibly can. There's also an unenforced property that, in cases where the match is a proper prefix of the input string, adding more characters to the end of a string doesn't extend the prefix. The library also encourages / permits users to write sloppy grammars (for instance using symOrExn) that fail in ugly ways.\n"
            ]
        },
        {
            "id": "79-5-6",
            "pair": [
                "A \"Matcher\" is anything with the type String -> Maybe Int. Matchers are supposed to be greedy and produce the longest prefix that they possibly can. There's also an unenforced property that, in cases where the match is a proper prefix of the input string, adding more characters to the end of a string doesn't extend the prefix. The library also encourages / permits users to write sloppy grammars (for instance using symOrExn) that fail in ugly ways.\n",
                "I could use some helping figuring out how to generalize String -> Maybe Int. I don't know what class I should insist on for the input Eq a => [a] is the first thing that comes to mind, but it might be possible to make something more general than that. As for the output type maybe a type that's constrained to be a monoid or a group? I really just need a notion of zero and the ability to increment for the index. ... although it might also be possible to generalize the notion of a \"prefix\" to tree-like structures.\n"
            ]
        },
        {
            "id": "79-6-7",
            "pair": [
                "I could use some helping figuring out how to generalize String -> Maybe Int. I don't know what class I should insist on for the input Eq a => [a] is the first thing that comes to mind, but it might be possible to make something more general than that. As for the output type maybe a type that's constrained to be a monoid or a group? I really just need a notion of zero and the ability to increment for the index. ... although it might also be possible to generalize the notion of a \"prefix\" to tree-like structures.\n",
                "I'm hoping to structure this library eventually as a collection of really concrete implementations for a handful of commonly-used text-like types, as well as a generic implementation that can be used in other circumstances. Advice on how to generalize the library is much appreciated."
            ]
        }
    ],
    [
        {
            "id": "8-1-2",
            "pair": [
                "Rotating a bitmap will always result in a blurred image - there is no way around that. (Except for rotations of 90/270/360 degrees.) Even high-end programs like photoshop cannot work around the issue - but they mitigate it.\n",
                "First, a bitmap stores its information pixel per pixel. So you have a large grid of pixels that composes the image. To see it in action, open you favorite bitmap painting tool and zoom in to the max. At that point, you can see the individual pixel that composes an image. \n"
            ]
        },
        {
            "id": "8-2-3",
            "pair": [
                "First, a bitmap stores its information pixel per pixel. So you have a large grid of pixels that composes the image. To see it in action, open you favorite bitmap painting tool and zoom in to the max. At that point, you can see the individual pixel that composes an image. \n",
                "Those pixels are always arranged in a grid of horizontal rows and vertical columns. Always vertical and horizontal. This is where the rotation problem arise...\n"
            ]
        },
        {
            "id": "8-3-4",
            "pair": [
                "Those pixels are always arranged in a grid of horizontal rows and vertical columns. Always vertical and horizontal. This is where the rotation problem arise...\n",
                "The image above shows what is going on. In the top left you have an example bitmap. Notice that its rows and column are horizontal and vertical. In the bottom right, you first have the same bitmap, with a slight rotation. But as I explained earlier, you cannot leave rows non-horizontal, so you have to transcribe the rotated image to an horizontal/vertical grid (shown in red in the image). \n"
            ]
        },
        {
            "id": "8-4-5",
            "pair": [
                "The image above shows what is going on. In the top left you have an example bitmap. Notice that its rows and column are horizontal and vertical. In the bottom right, you first have the same bitmap, with a slight rotation. But as I explained earlier, you cannot leave rows non-horizontal, so you have to transcribe the rotated image to an horizontal/vertical grid (shown in red in the image). \n",
                "By the way, each square represents a pixel. As you can see the pixels of the red grid do not align cleanly with the bitmap behind. (In fact the bitmap is now a little bit larger than it used to be - another effect of the rotation.) To figure how each of the grid pixel should be colored, you take an average of the pixels in the bitmap overlapping the pixel you want to draw. \n"
            ]
        },
        {
            "id": "8-5-6",
            "pair": [
                "By the way, each square represents a pixel. As you can see the pixels of the red grid do not align cleanly with the bitmap behind. (In fact the bitmap is now a little bit larger than it used to be - another effect of the rotation.) To figure how each of the grid pixel should be colored, you take an average of the pixels in the bitmap overlapping the pixel you want to draw. \n",
                "Which, of course, results is colors that are slightly different and therefore a blurred image.\n"
            ]
        },
        {
            "id": "8-6-7",
            "pair": [
                "Which, of course, results is colors that are slightly different and therefore a blurred image.\n",
                "The technique you use to average the color are varied and some result it much much better images, but they also take longer to evaluate.\n"
            ]
        },
        {
            "id": "8-7-8",
            "pair": [
                "The technique you use to average the color are varied and some result it much much better images, but they also take longer to evaluate.\n",
                "I suspect that the Android .createBitmap image does do a good job at it. You have a few solutions:\n"
            ]
        },
        {
            "id": "8-8-9",
            "pair": [
                "I suspect that the Android .createBitmap image does do a good job at it. You have a few solutions:\n",
                "If you are not using OpenGL, then most games load sprite sheets with rotation already done (see other image).. You only need a quadrant worth of rotation pre-calculated. It saves memory and you can rotate on the fly to 90/180/270 without loosing quality to cover the other 4 quadrant (going back to the first figure, notice that the grids will align perfectly with rotations of 90/180/270 degrees).\n"
            ]
        },
        {
            "id": "8-9-10",
            "pair": [
                "If you are not using OpenGL, then most games load sprite sheets with rotation already done (see other image).. You only need a quadrant worth of rotation pre-calculated. It saves memory and you can rotate on the fly to 90/180/270 without loosing quality to cover the other 4 quadrant (going back to the first figure, notice that the grids will align perfectly with rotations of 90/180/270 degrees).\n",
                "I am developing an android game. and in this I want to move an object along a Bezier path.\n"
            ]
        },
        {
            "id": "8-10-11",
            "pair": [
                "I am developing an android game. and in this I want to move an object along a Bezier path.\n",
                "my object is rotating with some angle on this curve.\n"
            ]
        },
        {
            "id": "8-11-12",
            "pair": [
                "my object is rotating with some angle on this curve.\n",
                "my object is moving and rotating as i supposed but image is getting blur. am not getting why is so."
            ]
        }
    ],
    [
        {
            "id": "80-1-2",
            "pair": [
                "I never used iTerm (or Mac OS); but since iTerm is a terminal emulator, it is probably unsuitable to schedule tasks. On *nixiod systems login scripts are usually executed from files like ~/.profile, /etc/profile, and in shell-specific files like ~/.bashrc and /etc/bash.bashrc. You can read about them in the manual page of you shell (e. g. man 1 bash).\n",
                "iTerm2 is an awesome utility and makes my interaction w/ the command line much easier. There are a lot of update commands I run on a daily basis, and thought it would be amazing if those could be launched on my user's login. I know that in order to run a script on a user login event a directory has to be created at /System/Library/StartupItems w/ the script that is wished to be run having the same name as the directory (e.g. update_sys/update_sys.sh). The two main issues I have after this is that 1) when the commands are executed there's no terminal window to show any output, and 2) I don't know how to designate iTerm2 (or any other utility) as the primary terminal utility to run scripts. I do have iTerm2 set as the default terminal utility, however some executed scripts will still be launched in Terminal.\n"
            ]
        },
        {
            "id": "80-2-3",
            "pair": [
                "iTerm2 is an awesome utility and makes my interaction w/ the command line much easier. There are a lot of update commands I run on a daily basis, and thought it would be amazing if those could be launched on my user's login. I know that in order to run a script on a user login event a directory has to be created at /System/Library/StartupItems w/ the script that is wished to be run having the same name as the directory (e.g. update_sys/update_sys.sh). The two main issues I have after this is that 1) when the commands are executed there's no terminal window to show any output, and 2) I don't know how to designate iTerm2 (or any other utility) as the primary terminal utility to run scripts. I do have iTerm2 set as the default terminal utility, however some executed scripts will still be launched in Terminal.\n",
                "(If my conceptions on how to add commands to a user login event are incorrect or if there is an easier way to do so, please let me know!)"
            ]
        }
    ],
    [
        {
            "id": "81-1-2",
            "pair": [
                "I was trying to get python and set up an environment to work on the upcoming project. \n",
                "In Windows 10 Ubuntu bash, I run the following command successfully:\n"
            ]
        },
        {
            "id": "81-2-3",
            "pair": [
                "In Windows 10 Ubuntu bash, I run the following command successfully:\n",
                "sudo wget https://www.python.org/ftp/python/2.7.6/Python-2.7.6.tgz\n"
            ]
        },
        {
            "id": "81-3-4",
            "pair": [
                "sudo wget https://www.python.org/ftp/python/2.7.6/Python-2.7.6.tgz\n",
                "Then I unzipped it and type in some other commands too, until I found that I need gcc in one particular step which I didn't have. So I try the following command:\n"
            ]
        },
        {
            "id": "81-4-5",
            "pair": [
                "Then I unzipped it and type in some other commands too, until I found that I need gcc in one particular step which I didn't have. So I try the following command:\n",
                "But then it failed with the following error messages:\n"
            ]
        },
        {
            "id": "81-5-6",
            "pair": [
                "But then it failed with the following error messages:\n",
                "Err:1 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 binutils amd64 2.26.1-1ubuntu1~16.04.4\n"
            ]
        },
        {
            "id": "81-6-7",
            "pair": [
                "Err:1 http://archive.ubuntu.com/ubuntu xenial-updates/main amd64 binutils amd64 2.26.1-1ubuntu1~16.04.4\n",
                "  Could not connect to archive.ubuntu.com:80 (2001:67c:1560:8001::14). - connect (111: Connection refused) [IP: 2001:67c:1560:8001::14 80]\n"
            ]
        },
        {
            "id": "81-7-8",
            "pair": [
                "  Could not connect to archive.ubuntu.com:80 (2001:67c:1560:8001::14). - connect (111: Connection refused) [IP: 2001:67c:1560:8001::14 80]\n",
                "Err:2 http://security.ubuntu.com/ubuntu xenial-security/main amd64 libc-dev-bin amd64 2.23-0ubuntu9\n"
            ]
        },
        {
            "id": "81-8-9",
            "pair": [
                "Err:2 http://security.ubuntu.com/ubuntu xenial-security/main amd64 libc-dev-bin amd64 2.23-0ubuntu9\n",
                "  Unable to connect to archive.ubuntu.com:http: [IP: 2001:67c:1560:8001::14 80]\n"
            ]
        },
        {
            "id": "81-9-10",
            "pair": [
                "  Unable to connect to archive.ubuntu.com:http: [IP: 2001:67c:1560:8001::14 80]\n",
                "Ign:3 http://security.ubuntu.com/ubuntu xenial-security/main amd64 linux-libc-dev amd64 4.4.0-96.119\n"
            ]
        },
        {
            "id": "81-10-11",
            "pair": [
                "Ign:3 http://security.ubuntu.com/ubuntu xenial-security/main amd64 linux-libc-dev amd64 4.4.0-96.119\n",
                "Ign:4 http://security.ubuntu.com/ubuntu xenial-security/main amd64 libc6-dev amd64 2.23-0ubuntu9\n"
            ]
        },
        {
            "id": "81-11-12",
            "pair": [
                "Ign:4 http://security.ubuntu.com/ubuntu xenial-security/main amd64 libc6-dev amd64 2.23-0ubuntu9\n",
                "81% [Connecting to security.ubuntu.com (91.189.88.161)]\n"
            ]
        },
        {
            "id": "81-12-13",
            "pair": [
                "81% [Connecting to security.ubuntu.com (91.189.88.161)]\n",
                "Err:2 http://security.ubuntu.com/ubuntu xenial-security/main amd64 libc-dev-bin amd64 2.23-0ubuntu9\n"
            ]
        },
        {
            "id": "81-13-14",
            "pair": [
                "Err:2 http://security.ubuntu.com/ubuntu xenial-security/main amd64 libc-dev-bin amd64 2.23-0ubuntu9\n",
                "  Unable to connect to archive.ubuntu.com:http: [IP: 2001:67c:1560:8001::14 80]\n"
            ]
        },
        {
            "id": "81-14-15",
            "pair": [
                "  Unable to connect to archive.ubuntu.com:http: [IP: 2001:67c:1560:8001::14 80]\n",
                "Err:3 http://security.ubuntu.com/ubuntu xenial-security/main amd64 linux-libc-dev amd64 4.4.0-96.119\n"
            ]
        },
        {
            "id": "81-15-16",
            "pair": [
                "Err:3 http://security.ubuntu.com/ubuntu xenial-security/main amd64 linux-libc-dev amd64 4.4.0-96.119\n",
                "  Unable to connect to archive.ubuntu.com:http: [IP: 2001:67c:1560:8001::14 80]\n"
            ]
        },
        {
            "id": "81-16-17",
            "pair": [
                "  Unable to connect to archive.ubuntu.com:http: [IP: 2001:67c:1560:8001::14 80]\n",
                "Err:4 http://security.ubuntu.com/ubuntu xenial-security/main amd64 libc6-dev amd64 2.23-0ubuntu9\n"
            ]
        },
        {
            "id": "81-17-18",
            "pair": [
                "Err:4 http://security.ubuntu.com/ubuntu xenial-security/main amd64 libc6-dev amd64 2.23-0ubuntu9\n",
                "  Unable to connect to archive.ubuntu.com:http: [IP: 2001:67c:1560:8001::14 80]\n"
            ]
        },
        {
            "id": "81-18-19",
            "pair": [
                "  Unable to connect to archive.ubuntu.com:http: [IP: 2001:67c:1560:8001::14 80]\n",
                "E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/b/binutils/binutils_2.26.1-1ubuntu1~16.04.4_amd64.deb  Could not connect to archive.ubuntu.com:80 (2001:67c:1560:8001::14). - connect (111: Connection refused) [IP: 2001:67c:1560:8001::14 80]\n"
            ]
        },
        {
            "id": "81-19-20",
            "pair": [
                "E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/b/binutils/binutils_2.26.1-1ubuntu1~16.04.4_amd64.deb  Could not connect to archive.ubuntu.com:80 (2001:67c:1560:8001::14). - connect (111: Connection refused) [IP: 2001:67c:1560:8001::14 80]\n",
                "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/g/glibc/libc-dev-bin_2.23-0ubuntu9_amd64.deb  Unable to connect to archive.ubuntu.com:http: [IP: 2001:67c:1560:8001::14 80]\n"
            ]
        },
        {
            "id": "81-20-21",
            "pair": [
                "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/g/glibc/libc-dev-bin_2.23-0ubuntu9_amd64.deb  Unable to connect to archive.ubuntu.com:http: [IP: 2001:67c:1560:8001::14 80]\n",
                "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/l/linux/linux-libc-dev_4.4.0-96.119_amd64.deb  Unable to connect to archive.ubuntu.com:http: [IP: 2001:67c:1560:8001::14 80]\n"
            ]
        },
        {
            "id": "81-21-22",
            "pair": [
                "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/l/linux/linux-libc-dev_4.4.0-96.119_amd64.deb  Unable to connect to archive.ubuntu.com:http: [IP: 2001:67c:1560:8001::14 80]\n",
                "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/g/glibc/libc6-dev_2.23-0ubuntu9_amd64.deb  Unable to connect to archive.ubuntu.com:http: [IP: 2001:67c:1560:8001::14 80]\n"
            ]
        },
        {
            "id": "81-22-23",
            "pair": [
                "E: Failed to fetch http://security.ubuntu.com/ubuntu/pool/main/g/glibc/libc6-dev_2.23-0ubuntu9_amd64.deb  Unable to connect to archive.ubuntu.com:http: [IP: 2001:67c:1560:8001::14 80]\n",
                "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n"
            ]
        },
        {
            "id": "81-23-24",
            "pair": [
                "E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?\n",
                "I am not familiar with it and it seems to me that it is some kind of connection problem. Why I am encountering this failure and how may I solve it? Any suggestion is appreciated!\n"
            ]
        },
        {
            "id": "81-24-25",
            "pair": [
                "I am not familiar with it and it seems to me that it is some kind of connection problem. Why I am encountering this failure and how may I solve it? Any suggestion is appreciated!\n",
                "The problem is most likely in the firewall settings that you are using, and possibly in settings IPv4 and IPv6. Reading:\n"
            ]
        },
        {
            "id": "81-25-26",
            "pair": [
                "The problem is most likely in the firewall settings that you are using, and possibly in settings IPv4 and IPv6. Reading:\n",
                "https://www.reddit.com/r/bashonubuntuonwindows/comments/6h71ku/bash_aptget_wont_work/ (last comment)"
            ]
        }
    ],
    [
        {
            "id": "82-1-2",
            "pair": [
                "My objective is computing a table of primes up to n, where n is a natural number passed as an argument to the prime_table function.\n",
                "Something I'd like but I'm not sure is possible would be changing get_multiples to return an iterator of the results vector instead of the vector itself, but I'm not sure how. If there is a way, I suspect it necessitates lifetimes, something I haven't grasped yet\n"
            ]
        },
        {
            "id": "82-2-3",
            "pair": [
                "Something I'd like but I'm not sure is possible would be changing get_multiples to return an iterator of the results vector instead of the vector itself, but I'm not sure how. If there is a way, I suspect it necessitates lifetimes, something I haven't grasped yet\n",
                "My main concern is avoiding unnecessarily allocating memory.\n"
            ]
        },
        {
            "id": "82-3-4",
            "pair": [
                "My main concern is avoiding unnecessarily allocating memory.\n",
                "Returning an iterator is fairly easy with impl. Here's how you can do it. Note that you need to use move with the closure so that it takes ownership of i. Since u32 can be copied, this just copies it instead of taking a reference, which lets you avoid dealing with lifetimes on the return type.\n"
            ]
        },
        {
            "id": "82-4-5",
            "pair": [
                "Returning an iterator is fairly easy with impl. Here's how you can do it. Note that you need to use move with the closure so that it takes ownership of i. Since u32 can be copied, this just copies it instead of taking a reference, which lets you avoid dealing with lifetimes on the return type.\n",
                "As another note, I have a recommendation for improving performance on your sieve. Currently, every time you call remove_item, you have to search all the way through the vector, remove the item, then shift all items after it forward by one. This is awful for performance. Instead, you could have a Vec<bool> and simply flip the value based on index rather than remove it. Then when you're done sieving, you can go through this vector along with indices and create a Vec<u32> to return."
            ]
        }
    ],
    [
        {
            "id": "83-1-2",
            "pair": [
                "If the attenuation/loss of the link is higher than the power budget on the optic an insufficient amount of light will make it to the receiving optic and the link will (probably) not function as desired.  You can see a complete lack of link, receive errors, intermittent link or have a link that functions just fine but is very, very close to failing.  If the link does work you are susceptible to future failures from anything causes a change to the fiber you are using (even moving the fiber cable can cause an outage with that tight of a link budget).\n",
                "Basically the power budget is the engineering guideline in which the optics are designed to work.  If you exceed the guidelines then you are outside the expected environment and all problems are yours to troubleshoot.  You can move up to a higher power optic (EX/ZX/etc.) with a larger link budget, and if necessary use attenuators to make sure you don't exceed the rated receive power and cause saturation/eventually burn out the receive sensor.\n"
            ]
        },
        {
            "id": "83-2-3",
            "pair": [
                "Basically the power budget is the engineering guideline in which the optics are designed to work.  If you exceed the guidelines then you are outside the expected environment and all problems are yours to troubleshoot.  You can move up to a higher power optic (EX/ZX/etc.) with a larger link budget, and if necessary use attenuators to make sure you don't exceed the rated receive power and cause saturation/eventually burn out the receive sensor.\n",
                "If you are using the newer gigabit Cisco optics (with the D suffix) or 10G optics and appropriate code, you should be able to check light levels with the show int gi#/# transceiver detail command.\n"
            ]
        },
        {
            "id": "83-3-4",
            "pair": [
                "If you are using the newer gigabit Cisco optics (with the D suffix) or 10G optics and appropriate code, you should be able to check light levels with the show int gi#/# transceiver detail command.\n",
                "I know that before you use the fiber optic cable you should calculate the power budget and make sure that the total power should be below the Transceiver power budget margin to be sure that the system will work as it designed. If I'm using a 1G, it will run without any problem smoothly.\n"
            ]
        },
        {
            "id": "83-4-5",
            "pair": [
                "I know that before you use the fiber optic cable you should calculate the power budget and make sure that the total power should be below the Transceiver power budget margin to be sure that the system will work as it designed. If I'm using a 1G, it will run without any problem smoothly.\n",
                "1) But my question is, let us say that we are using a 1G GBIC card, and after calculating the power budget for my system it shows that it is above the margin. what will happen ? will it works but with lower speed, or it will not work at all since its beyond the margin and the receiver can't receive it ?\n"
            ]
        },
        {
            "id": "83-5-6",
            "pair": [
                "1) But my question is, let us say that we are using a 1G GBIC card, and after calculating the power budget for my system it shows that it is above the margin. what will happen ? will it works but with lower speed, or it will not work at all since its beyond the margin and the receiver can't receive it ?\n",
                "ex: using a 1000BASE-LX cisco card that have a 10.5 dbm power margin and my system is having a 12dB. will it work but with lower speed than 1G ? or it will not work at all ?\n"
            ]
        },
        {
            "id": "83-6-7",
            "pair": [
                "ex: using a 1000BASE-LX cisco card that have a 10.5 dbm power margin and my system is having a 12dB. will it work but with lower speed than 1G ? or it will not work at all ?\n",
                "2)If it will work with lower speed,then can I use a card and I exceed the budget margin and use it with lower speed, rather than buying a new card ? or it will damage the system ? Because I may don't need that high speed.\n"
            ]
        },
        {
            "id": "83-7-8",
            "pair": [
                "2)If it will work with lower speed,then can I use a card and I exceed the budget margin and use it with lower speed, rather than buying a new card ? or it will damage the system ? Because I may don't need that high speed.\n",
                "Sorry for my lot of question. but I need to understand that power budget better."
            ]
        }
    ],
    [
        {
            "id": "84-1-2",
            "pair": [
                "What you want is reverse ssh tunneling with a machine acting as a middleman.\n",
                "You should follow rogerovo's hint and create the port forwarding from the VPS (some random high port) to the LinuxBox (port 22). However you will also need a SSHD server running on the LinuxBox. \n"
            ]
        },
        {
            "id": "84-2-3",
            "pair": [
                "You should follow rogerovo's hint and create the port forwarding from the VPS (some random high port) to the LinuxBox (port 22). However you will also need a SSHD server running on the LinuxBox. \n",
                "Only having two clients is not going to work. A client coming from the LinuxBox, cannot take a command from the VPS back to the LinuxBox. You need a server on the LinuxBox that actually receives and handles commands.  \n"
            ]
        },
        {
            "id": "84-3-4",
            "pair": [
                "Only having two clients is not going to work. A client coming from the LinuxBox, cannot take a command from the VPS back to the LinuxBox. You need a server on the LinuxBox that actually receives and handles commands.  \n",
                "Breaking tunnels -> You better take a look at the keep alive option\n"
            ]
        },
        {
            "id": "84-4-5",
            "pair": [
                "Breaking tunnels -> You better take a look at the keep alive option\n",
                "Second tunnel -> To simplify logging in you could add another tunnel from the Windows Box to the VPS. \n"
            ]
        },
        {
            "id": "84-5-6",
            "pair": [
                "Second tunnel -> To simplify logging in you could add another tunnel from the Windows Box to the VPS. \n",
                "(Win 22222 -> VPS 22221 -> VPS 22222 -> LinuxBox 22)\n"
            ]
        },
        {
            "id": "84-6-7",
            "pair": [
                "(Win 22222 -> VPS 22221 -> VPS 22222 -> LinuxBox 22)\n",
                "Then you need to login only once to reach the linux box. No idea however how to do port forwarding from Windows :/.\n"
            ]
        },
        {
            "id": "84-7-8",
            "pair": [
                "Then you need to login only once to reach the linux box. No idea however how to do port forwarding from Windows :/.\n",
                "Hi I want to remotely administer (command line interface) a linux box from my home Windows PC.  \n"
            ]
        },
        {
            "id": "84-8-9",
            "pair": [
                "Hi I want to remotely administer (command line interface) a linux box from my home Windows PC.  \n",
                "I have an ssh client on the linux box.  I have an ssh client on my home Windows PC.  I have a login on a VPS running an ssh server.  I am able to ssh from the linux box to the VPS.  I am able to ssh from my home Windows PC to the VPS.\n"
            ]
        },
        {
            "id": "84-9-10",
            "pair": [
                "I have an ssh client on the linux box.  I have an ssh client on my home Windows PC.  I have a login on a VPS running an ssh server.  I am able to ssh from the linux box to the VPS.  I am able to ssh from my home Windows PC to the VPS.\n",
                "How do I connect the two ssh clients?  Is there an easy way to connect the ports locally (on the VPS/ssh server)?"
            ]
        }
    ],
    [
        {
            "id": "85-1-2",
            "pair": [
                "The router will be ok. The main challenge for you is to ensure that client are splitted equally between APs. Also pls note that switching between APs will be not seamless for the client, i.e all active connections will be dropped. In order to provide seamless switching you need to buy wi-fi controller. There are hardware and software ones. Software one is much cheaper and will be enough for 25 clients.\n",
                "I'm looking into setting up a more robust wireless system for the company I work for. Right now we're running off a NETGEAR AC1200 consumer grade router with about 25 people in one medium size office all with wireless laptops connected to it at once. As you can imagine, wireless performance isn't great.\n"
            ]
        },
        {
            "id": "85-2-3",
            "pair": [
                "I'm looking into setting up a more robust wireless system for the company I work for. Right now we're running off a NETGEAR AC1200 consumer grade router with about 25 people in one medium size office all with wireless laptops connected to it at once. As you can imagine, wireless performance isn't great.\n",
                "After doing some research, it sounds like what we need is a couple of wireless access points on different channels that then connect to a router with wireless disabled (for the sake of avoiding interference) and the router connects to the modem. This way half of the people can use access point A, and the other half access point B.\n"
            ]
        },
        {
            "id": "85-3-4",
            "pair": [
                "After doing some research, it sounds like what we need is a couple of wireless access points on different channels that then connect to a router with wireless disabled (for the sake of avoiding interference) and the router connects to the modem. This way half of the people can use access point A, and the other half access point B.\n",
                "My question is, can I re-use the NETGEAR AC1200 we're using right now as a router for the two access points? Would it be able to keep up with the traffic of 25 people from both access points? Or should I look into something with more power? If so, what type of device should I look at?\n"
            ]
        },
        {
            "id": "85-4-5",
            "pair": [
                "My question is, can I re-use the NETGEAR AC1200 we're using right now as a router for the two access points? Would it be able to keep up with the traffic of 25 people from both access points? Or should I look into something with more power? If so, what type of device should I look at?\n",
                "Or am I looking at this situation in the completely wrong way? "
            ]
        }
    ],
    [
        {
            "id": "86-1-2",
            "pair": [
                "For future reference, small and odd-sized optical disks only work in certain kinds of tray-loading drives. The slot-loading optical drives on almost all Macs are only designed to work with full-sized 120mm round discs.\n",
                "There's no way for a slot-loading drive to get a nonstandard disk fully injected and onto the spindle. There's no way to guarantee that the motorized rollers that handle inject and eject will be able to sense it and grip it and do anything with it.\n"
            ]
        },
        {
            "id": "86-2-3",
            "pair": [
                "There's no way for a slot-loading drive to get a nonstandard disk fully injected and onto the spindle. There's no way to guarantee that the motorized rollers that handle inject and eject will be able to sense it and grip it and do anything with it.\n",
                "So once you've inserted a nonstandard disk into the slot-loading drive, it's equivalent to having stuck a credit card or guitar pick or some other thin rigid sheet of plastic in there. It's just loose debris in your drive and you have to treat it that way.\n"
            ]
        },
        {
            "id": "86-3-4",
            "pair": [
                "So once you've inserted a nonstandard disk into the slot-loading drive, it's equivalent to having stuck a credit card or guitar pick or some other thin rigid sheet of plastic in there. It's just loose debris in your drive and you have to treat it that way.\n",
                "Pointing the slot down and shaking it gently hoping it comes out is probably as good as anything. Most technicians would recommend against sticking any tools in there, as you're likely to damage something that way. The only safe bet is time- and hassle-intensive: If you want to protect your warranty, have an authorized technician take the thing apart. Otherwise, take the thing apart yourself.\n"
            ]
        },
        {
            "id": "86-4-5",
            "pair": [
                "Pointing the slot down and shaking it gently hoping it comes out is probably as good as anything. Most technicians would recommend against sticking any tools in there, as you're likely to damage something that way. The only safe bet is time- and hassle-intensive: If you want to protect your warranty, have an authorized technician take the thing apart. Otherwise, take the thing apart yourself.\n",
                "http://guides.macrumors.com/Force_Eject_a_Stuck_CD_or_DVD"
            ]
        }
    ],
    [
        {
            "id": "87-1-2",
            "pair": [
                "Burning a CD-ROM compatible with ISO9660 on command-line will always require a step to build the iso(9660) image, I think.\n",
                "However, graphical tools like k3b (KDE) or brasero (gnome) will allow you to create data CD-ROMs without you need to explicitely run this step, they probably build the image on the fly during burning process.\n"
            ]
        },
        {
            "id": "87-2-3",
            "pair": [
                "However, graphical tools like k3b (KDE) or brasero (gnome) will allow you to create data CD-ROMs without you need to explicitely run this step, they probably build the image on the fly during burning process.\n",
                "Yes and no. Yes, you must create a suitable stream of bytes; and no, because you don't need to store it as a regular file on your HDD or SSD.\n"
            ]
        },
        {
            "id": "87-3-4",
            "pair": [
                "Yes and no. Yes, you must create a suitable stream of bytes; and no, because you don't need to store it as a regular file on your HDD or SSD.\n",
                "Many Linux tools follow \"do one thing and do it well\" rule. We have mkisofs for creating an ISO image, then e.g. cdrecord or cdrskin for burning it. In Windows tools that bloat to do all things by themselves are far more common. Another answer has already suggested they may build images on the fly; they may also store these images as temporary files.\n"
            ]
        },
        {
            "id": "87-4-5",
            "pair": [
                "Many Linux tools follow \"do one thing and do it well\" rule. We have mkisofs for creating an ISO image, then e.g. cdrecord or cdrskin for burning it. In Windows tools that bloat to do all things by themselves are far more common. Another answer has already suggested they may build images on the fly; they may also store these images as temporary files.\n",
                "Note you can achieve something similar in Linux. Check -o option of mkisofs; if this option is lacking, stdout is used. Programs writing to  optical media can use their stdin in \"Track At Once\" mode.\n"
            ]
        },
        {
            "id": "87-5-6",
            "pair": [
                "Note you can achieve something similar in Linux. Check -o option of mkisofs; if this option is lacking, stdout is used. Programs writing to  optical media can use their stdin in \"Track At Once\" mode.\n",
                "Or you can create temporary files, even in memory (like in /dev/shm/)."
            ]
        }
    ],
    [
        {
            "id": "88-1-2",
            "pair": [
                "While looking at my mail queue I noticed the server was attempting to send back a bounce email, presumably to a spammer. Reading the error, I noticed that it was revealing the expanded value of a virtual alias. Is there any way to hide this address and keep it transparent?\n",
                "local@myhost.tld email@destination.tld in the alias file, and here is the mail content:\n"
            ]
        },
        {
            "id": "88-2-3",
            "pair": [
                "local@myhost.tld email@destination.tld in the alias file, and here is the mail content:\n",
                "Do not solve this problem. Solve the original problem.\n"
            ]
        },
        {
            "id": "88-3-4",
            "pair": [
                "Do not solve this problem. Solve the original problem.\n",
                "There is no need to relay Spam. This is what you are doing here. You accept any mail and then forward all mails to a Google account. Google's Spam filter jumps in and rejects the mail.\n"
            ]
        },
        {
            "id": "88-4-5",
            "pair": [
                "There is no need to relay Spam. This is what you are doing here. You accept any mail and then forward all mails to a Google account. Google's Spam filter jumps in and rejects the mail.\n",
                "So please think of replacing this strange mail setup. If you want to store any mail in Googlemail because it is convenient, then please use Google-Apps and move the MX of myhost.tld to Google's server farm. If it is a per user setting to forward mails to Google, then you have to filter for Spam and only forward the clean ones.\n"
            ]
        },
        {
            "id": "88-5-6",
            "pair": [
                "So please think of replacing this strange mail setup. If you want to store any mail in Googlemail because it is convenient, then please use Google-Apps and move the MX of myhost.tld to Google's server farm. If it is a per user setting to forward mails to Google, then you have to filter for Spam and only forward the clean ones.\n",
                "In general you should never bounce Spam mails. The bounces do not reach the correct person as 99.999% of all sender addresses are forged.\n"
            ]
        },
        {
            "id": "88-6-7",
            "pair": [
                "In general you should never bounce Spam mails. The bounces do not reach the correct person as 99.999% of all sender addresses are forged.\n",
                "There is a better solution I will only sketch shortly.\n"
            ]
        },
        {
            "id": "88-7-8",
            "pair": [
                "There is a better solution I will only sketch shortly.\n",
                "For relaying mails to Google create a new transport and a corresponding service in master.cf. In the options of this particular smtp service set the -o bounce_service_name=discard. This should discard any bounce mail from Google. This will stop backscatter but if Google temporarily defers mail it won't be affected and retried later."
            ]
        }
    ],
    [
        {
            "id": "89-1-2",
            "pair": [
                "DNS queries are also based on RTT (Round-trip time), just like ICMP. RTT of a DNS query is the measurement of the delay between a DNS query being issued and the time the answer is received, meaning that the reply (in your case - reply to #1 received from A) must arrive before the configured maximum timeout in order for it to be considered valid. It's quite identical to how ICMP works. \n",
                "Technically, RTT is the length of time it takes for a signal to be sent plus the length of time it takes for an acknowledgment of that signal to be received.\n"
            ]
        },
        {
            "id": "89-2-3",
            "pair": [
                "Technically, RTT is the length of time it takes for a signal to be sent plus the length of time it takes for an acknowledgment of that signal to be received.\n",
                "I've tested the behaviour in my lab. Linux resolver will indeed take a successful reply if it comes while it is still resolving the domain, even if the reply comes with a delay greater than 1 second. In my lab, I've set a DNS server delay of 3 seconds and with {{options timeout:1 attempts:5}} set, the client still resolved the domain. Packet capture prooved the reply was received after multiple queries to both configured DNS servers were already sent out by the client.\n"
            ]
        },
        {
            "id": "89-3-4",
            "pair": [
                "I've tested the behaviour in my lab. Linux resolver will indeed take a successful reply if it comes while it is still resolving the domain, even if the reply comes with a delay greater than 1 second. In my lab, I've set a DNS server delay of 3 seconds and with {{options timeout:1 attempts:5}} set, the client still resolved the domain. Packet capture prooved the reply was received after multiple queries to both configured DNS servers were already sent out by the client.\n",
                "Changing default values or {{options timeout:5 attempts:1}} to {{options timeout:5 attempts:1}} hence decreases DNS server failover delay, while still allowing slow DNS servers to provide the answer in the same time."
            ]
        }
    ],
    [
        {
            "id": "9-1-2",
            "pair": [
                "A multicast group is formed of all computers listening to a certain multicast address. Therefore a multicast group automatically exists whenever at least one computer is listening to the multicast address given.\n",
                "You simply have to start a program which is listening to multicast packets on a certain address. For example a program listening to UDP packets.\n"
            ]
        },
        {
            "id": "9-2-3",
            "pair": [
                "You simply have to start a program which is listening to multicast packets on a certain address. For example a program listening to UDP packets.\n",
                "Using Wireshark I saw that Linux sends an IGMP joining message when the program is started and an IGMP leaving message when the program is finished.\n"
            ]
        },
        {
            "id": "9-3-4",
            "pair": [
                "Using Wireshark I saw that Linux sends an IGMP joining message when the program is started and an IGMP leaving message when the program is finished.\n",
                "If there are multiple programs listening to the same address Linux may behave like this: The \"join\" message will only be sent when the first of these programs is started and the \"leave\" message will only be sent when the last program is finished.\n"
            ]
        },
        {
            "id": "9-4-5",
            "pair": [
                "If there are multiple programs listening to the same address Linux may behave like this: The \"join\" message will only be sent when the first of these programs is started and the \"leave\" message will only be sent when the last program is finished.\n",
                "I don't know if a ready-to-use program exists but I found an example C program here.\n"
            ]
        },
        {
            "id": "9-5-6",
            "pair": [
                "I don't know if a ready-to-use program exists but I found an example C program here.\n",
                "Simply send some packet (e.g. an UDP packet) to a multicast address. The sender need not even be listening to that multicast address (it need not be part of the multicast address group).\n"
            ]
        },
        {
            "id": "9-6-7",
            "pair": [
                "Simply send some packet (e.g. an UDP packet) to a multicast address. The sender need not even be listening to that multicast address (it need not be part of the multicast address group).\n",
                "The example program in the link above seems also to be able to send messages...\n"
            ]
        },
        {
            "id": "9-7-8",
            "pair": [
                "The example program in the link above seems also to be able to send messages...\n",
                "My ultimate goal is to setup IGMP snooping on a linux machine using netfilter hooks. This is for exploratory purposes to understand IGMP protocol.\n"
            ]
        },
        {
            "id": "9-8-9",
            "pair": [
                "My ultimate goal is to setup IGMP snooping on a linux machine using netfilter hooks. This is for exploratory purposes to understand IGMP protocol.\n",
                "What we plan to do is create a Multicast group on Host2 so that we can send the multicast traffic from Host1 to Host2 and using the hooks on Host2 we can then check for membership messages (join, leaving, timing info, port etc) on Host2 and create our own Multicast Database similar to mdb.\n"
            ]
        },
        {
            "id": "9-9-10",
            "pair": [
                "What we plan to do is create a Multicast group on Host2 so that we can send the multicast traffic from Host1 to Host2 and using the hooks on Host2 we can then check for membership messages (join, leaving, timing info, port etc) on Host2 and create our own Multicast Database similar to mdb.\n",
                "How can we create a Multicast group ? (commands to use on linux ?)\n"
            ]
        },
        {
            "id": "9-10-11",
            "pair": [
                "How can we create a Multicast group ? (commands to use on linux ?)\n",
                "How can we attach interfaces or Host's to that multicast group ?\n"
            ]
        },
        {
            "id": "9-11-12",
            "pair": [
                "How can we attach interfaces or Host's to that multicast group ?\n",
                "How can we transfer packets between interfaces belonging to multicast group?"
            ]
        }
    ],
    [
        {
            "id": "90-1-2",
            "pair": [
                "Answering this requires diving into Storage Geek. I apologize in advance.\n",
                "The reason Microsoft seems to suggest 48 separate partitions is for one reason: to maximize in-OS parallelization for I/O's. By having 48 LUNs, the OS has to keep 48 separate I/O queues, and those queues can in theory be served in parallel. If one LUN is particularly slow (it's doing heavy random writes) it won't hold up access to other LUNs. \n"
            ]
        },
        {
            "id": "90-2-3",
            "pair": [
                "The reason Microsoft seems to suggest 48 separate partitions is for one reason: to maximize in-OS parallelization for I/O's. By having 48 LUNs, the OS has to keep 48 separate I/O queues, and those queues can in theory be served in parallel. If one LUN is particularly slow (it's doing heavy random writes) it won't hold up access to other LUNs. \n",
                "On modern hardware this is a fractional-percentage gain for a LOT of storage headache. Unless you know you will be pressing your data warehouse to the absolute upper limit, it isn't worth it. Modern RAID cards are fast enough that they can handle this for you. Having 4 LUNs could yield gains. 48 may actually hurt.\n"
            ]
        },
        {
            "id": "90-3-4",
            "pair": [
                "On modern hardware this is a fractional-percentage gain for a LOT of storage headache. Unless you know you will be pressing your data warehouse to the absolute upper limit, it isn't worth it. Modern RAID cards are fast enough that they can handle this for you. Having 4 LUNs could yield gains. 48 may actually hurt.\n",
                "Storage these days is generally characterized by the performance metric of I/O Operations per second (I/O Ops). Each drive has its own upper limit for random I/O (ranges between 90-180 per drive, depending on RPMs and a few other things). When you gang drives together, such as in a RAID10 set, this I/O Ops count is additive. A 12-disk RAID10 set will have the same I/O Ops capacity as 6 Raid1 pairs, and doesn't force you into creating six separate DB files. By creating a single large RAID10 set you can create a single large DB file that can handle huge amounts of load. \n"
            ]
        },
        {
            "id": "90-4-5",
            "pair": [
                "Storage these days is generally characterized by the performance metric of I/O Operations per second (I/O Ops). Each drive has its own upper limit for random I/O (ranges between 90-180 per drive, depending on RPMs and a few other things). When you gang drives together, such as in a RAID10 set, this I/O Ops count is additive. A 12-disk RAID10 set will have the same I/O Ops capacity as 6 Raid1 pairs, and doesn't force you into creating six separate DB files. By creating a single large RAID10 set you can create a single large DB file that can handle huge amounts of load. \n",
                "Going back to what I said in the second paragraph about a slow LUN not holding up access to other LUNs, this is why maximizing I/O Ops for a LUN makes sense. It is far less likely to block at all if it has enough I/O Op overhead. By creating a large RAID10 array, the parallelization is pushed onto the RAID card, not the operating system, which leaves the OS free to do other things. You'll still get the parallelization advantage, and you leverage dedicated hardware for it.\n"
            ]
        },
        {
            "id": "90-5-6",
            "pair": [
                "Going back to what I said in the second paragraph about a slow LUN not holding up access to other LUNs, this is why maximizing I/O Ops for a LUN makes sense. It is far less likely to block at all if it has enough I/O Op overhead. By creating a large RAID10 array, the parallelization is pushed onto the RAID card, not the operating system, which leaves the OS free to do other things. You'll still get the parallelization advantage, and you leverage dedicated hardware for it.\n",
                "For database servers it is wise to keep data-file and log-file I/O on different spindles. The exact percentage of which I'll leave to the SQL Server experts (I'm not one), and is likely based on your exact configuration and use-patterns. As it is a data-warehouse you'll need lots of log-space to handle the bulk loads. Log I/O is significantly sequential, where data I/O is significantly random, so maximal logging performance is best found by putting the logs on different spindles than the data-files are.\n"
            ]
        },
        {
            "id": "90-6-7",
            "pair": [
                "For database servers it is wise to keep data-file and log-file I/O on different spindles. The exact percentage of which I'll leave to the SQL Server experts (I'm not one), and is likely based on your exact configuration and use-patterns. As it is a data-warehouse you'll need lots of log-space to handle the bulk loads. Log I/O is significantly sequential, where data I/O is significantly random, so maximal logging performance is best found by putting the logs on different spindles than the data-files are.\n",
                "In your case, you may be able to get away with 2 LUNs. A big RAID10 set for your data-files, and smaller RAID10 set for your log-files.\n"
            ]
        },
        {
            "id": "90-7-8",
            "pair": [
                "In your case, you may be able to get away with 2 LUNs. A big RAID10 set for your data-files, and smaller RAID10 set for your log-files.\n",
                "I'm designing a datawarehouse solution and I'm a newbie in disk configuration issues, let me explain you.\n"
            ]
        },
        {
            "id": "90-8-9",
            "pair": [
                "I'm designing a datawarehouse solution and I'm a newbie in disk configuration issues, let me explain you.\n",
                "Our storage is spread over 6 storage enlosures having each of them 5 raid-1 disk arrays, and having 2 LUNS defined per each disk array, which makes a total 48 LUNS (this is following Microsoft fast track recommendations for datawarehouse architectures). \n"
            ]
        },
        {
            "id": "90-9-10",
            "pair": [
                "Our storage is spread over 6 storage enlosures having each of them 5 raid-1 disk arrays, and having 2 LUNS defined per each disk array, which makes a total 48 LUNS (this is following Microsoft fast track recommendations for datawarehouse architectures). \n",
                "I would like to partition my data, on other projects I have worked before, we always followed a 1 partition - 1 filegroup rule. On the microsoft fast track recomendations it is advised to create a filegroup and then for that filegroup a data file per each lun... but I pretend to have a week level partitioning... if I apply that rule I think that I'll get too many files and a complex layout.\n"
            ]
        },
        {
            "id": "90-10-11",
            "pair": [
                "I would like to partition my data, on other projects I have worked before, we always followed a 1 partition - 1 filegroup rule. On the microsoft fast track recomendations it is advised to create a filegroup and then for that filegroup a data file per each lun... but I pretend to have a week level partitioning... if I apply that rule I think that I'll get too many files and a complex layout.\n",
                "I'm thinking of just creating just one filegroup (with the 48 lun data files), but still create the partitions since I want to keep soem of the benefits of partitions like partition switching... Is this scenario not recommended? What would you suggest?"
            ]
        }
    ],
    [
        {
            "id": "91-1-2",
            "pair": [
                "So I'm pretty sure this question has been asked before, but I simply cannot find the answer by Googling - feel free to point me to the right site/post to look up the answer. Apologies for my poor search-fu... :(\n",
                "I am developing a PHP application on Windows with WAMP. This needs to be deployed to a UNIX web host. The application will ultimately live on its own domain, e.g. www.example.com but I am developing it on my WAMP server under localhost/app1, i.e. I have multiple applications I'm working on, each one lives in its own directory.\n"
            ]
        },
        {
            "id": "91-2-3",
            "pair": [
                "I am developing a PHP application on Windows with WAMP. This needs to be deployed to a UNIX web host. The application will ultimately live on its own domain, e.g. www.example.com but I am developing it on my WAMP server under localhost/app1, i.e. I have multiple applications I'm working on, each one lives in its own directory.\n",
                "My problem is that I am referring to shared resources by their absolute paths, e.g. /images/test.jpg. Their of course works fine on my production server, but when I try to use this on my local machine, it fails to load those resources because I'm running from within a \"subdirectory\" in WAMP, i.e. localhost/app1/, so the link to /images/test.jpg fails. It should instead be mapped to localhost/app1/images/test.jpg.\n"
            ]
        },
        {
            "id": "91-3-4",
            "pair": [
                "My problem is that I am referring to shared resources by their absolute paths, e.g. /images/test.jpg. Their of course works fine on my production server, but when I try to use this on my local machine, it fails to load those resources because I'm running from within a \"subdirectory\" in WAMP, i.e. localhost/app1/, so the link to /images/test.jpg fails. It should instead be mapped to localhost/app1/images/test.jpg.\n",
                "directive in my .htaccess in the /xxx/app1 directory, but that merely results in an error:\n"
            ]
        },
        {
            "id": "91-4-5",
            "pair": [
                "directive in my .htaccess in the /xxx/app1 directory, but that merely results in an error:\n",
                "Request exceeded the limit of 10 internal redirects due to probable configuration error. Use 'LimitInternalRecursion' to increase the limit if necessary. Use 'LogLevel debug' to get a backtrace/\n"
            ]
        },
        {
            "id": "91-5-6",
            "pair": [
                "Request exceeded the limit of 10 internal redirects due to probable configuration error. Use 'LimitInternalRecursion' to increase the limit if necessary. Use 'LogLevel debug' to get a backtrace/\n",
                "The certainly makes sense, since now every call to localhost/app1/ redirects to localhost/app1/, etc.\n"
            ]
        },
        {
            "id": "91-6-7",
            "pair": [
                "The certainly makes sense, since now every call to localhost/app1/ redirects to localhost/app1/, etc.\n",
                "So, my question is: how should I set up my application so that I can run the same code on localhost with WAMP as well as on my production host? I realize I may have been barking up the wrong tree here and should be structuring this completely differently, so any pointers in the right direction in this regard would be most appreciated!\n"
            ]
        },
        {
            "id": "91-7-8",
            "pair": [
                "So, my question is: how should I set up my application so that I can run the same code on localhost with WAMP as well as on my production host? I realize I may have been barking up the wrong tree here and should be structuring this completely differently, so any pointers in the right direction in this regard would be most appreciated!\n",
                "The way I develop on Windows (using WampDeveloper Pro) is I either:\n"
            ]
        },
        {
            "id": "91-8-9",
            "pair": [
                "The way I develop on Windows (using WampDeveloper Pro) is I either:\n",
                "A. Use the same exact domain name and enable Local DNS for the website (Windows host file resolve of domain to 127.0.0.1).\n"
            ]
        },
        {
            "id": "91-9-10",
            "pair": [
                "A. Use the same exact domain name and enable Local DNS for the website (Windows host file resolve of domain to 127.0.0.1).\n",
                "B. Use a config.inc.php file for my website which uses a variable or constant to define the domain-name as dev.example.com, and on production as www.example.com. And also define any full paths."
            ]
        }
    ],
    [
        {
            "id": "92-1-2",
            "pair": [
                "In short, usually no. Without some prior arrangement, those servers won't have any mechanism that would let you relay arbitrary IP data through them.\n",
                "No. What you're asking is to make your ISP think the packet is for destination A, but the rest of the internet think it's for destination B... but the packet has only one 'destination' field and you can't put both addresses in there.\n"
            ]
        },
        {
            "id": "92-2-3",
            "pair": [
                "No. What you're asking is to make your ISP think the packet is for destination A, but the rest of the internet think it's for destination B... but the packet has only one 'destination' field and you can't put both addresses in there.\n",
                "Once the packet leaves your network, you are no longer in control of it; you cannot magically reroute it mid-flight \u2013 if it had a \"free server\" listed as its destination in the beginning, it'll always have the same destination through its entire trip, and it will go to that server, not anywhere else.\n"
            ]
        },
        {
            "id": "92-3-4",
            "pair": [
                "Once the packet leaves your network, you are no longer in control of it; you cannot magically reroute it mid-flight \u2013 if it had a \"free server\" listed as its destination in the beginning, it'll always have the same destination through its entire trip, and it will go to that server, not anywhere else.\n",
                "(IP packets can have various \"source routing\" and \"segment routing\" headers, but those do not work on the public Internet \u2013 they'll be either ignored or cause the whole packet to be discarded.)\n"
            ]
        },
        {
            "id": "92-4-5",
            "pair": [
                "(IP packets can have various \"source routing\" and \"segment routing\" headers, but those do not work on the public Internet \u2013 they'll be either ignored or cause the whole packet to be discarded.)\n",
                "Besides, your outgoing traffic is usually a very small part of the total \u2013 the majority of your non-game data is inbound. So it's not just you that would need to spoof their destination; all servers you're communicating with would also need to spoof the source of their response packets to make them look like they came from a \"free\" server.\n"
            ]
        },
        {
            "id": "92-5-6",
            "pair": [
                "Besides, your outgoing traffic is usually a very small part of the total \u2013 the majority of your non-game data is inbound. So it's not just you that would need to spoof their destination; all servers you're communicating with would also need to spoof the source of their response packets to make them look like they came from a \"free\" server.\n",
                "Also no. Either the packet has that server's address as destination, or it does not \u2013 there's no such thing as \"wrongly\" sent packets, at least not in the way you're imagining. If the packet has the server's address, then the server is the correct recipient, otherwise it wouldn't have received the packet in the first place.\n"
            ]
        },
        {
            "id": "92-6-7",
            "pair": [
                "Also no. Either the packet has that server's address as destination, or it does not \u2013 there's no such thing as \"wrongly\" sent packets, at least not in the way you're imagining. If the packet has the server's address, then the server is the correct recipient, otherwise it wouldn't have received the packet in the first place.\n",
                "It is possible to put IP packets inside IP packets, with the outer packet having a \"free server\" as destination, and the inner packet being the one you actually want. That's an IP-IP tunnel and it is the simplest form of a VPN protocol. However, the outer recipient must be configured to expect this \u2013 if you send nested packets to any random server without prior arrangement, they'll be ignored.\n"
            ]
        },
        {
            "id": "92-7-8",
            "pair": [
                "It is possible to put IP packets inside IP packets, with the outer packet having a \"free server\" as destination, and the inner packet being the one you actually want. That's an IP-IP tunnel and it is the simplest form of a VPN protocol. However, the outer recipient must be configured to expect this \u2013 if you send nested packets to any random server without prior arrangement, they'll be ignored.\n",
                "This is slightly unethical, but I would still ask.\n"
            ]
        },
        {
            "id": "92-8-9",
            "pair": [
                "This is slightly unethical, but I would still ask.\n",
                "I have a mobile data plan which allows me to play some online games for free (i.e. the data used by connections to servers of the games specified doesn't count toward my data limit). Is there a way to forge, spoof, or otherwise pretend that all my traffic is going to those servers? It wouldn't be hard to find the IPs that are considered \"free\". Does the protocols allow servers to redirect \"wrongly\" sent packets to the correct recipient? "
            ]
        }
    ],
    [
        {
            "id": "93-1-2",
            "pair": [
                "I imagine you would want penetration tester to sit in the less secure / outsize zone.  So, plug the Internet link, pen testing server/service, and Dell server into an 'outside' VLAN on the switch.  Plug your computers and Dell into the 'inside' VLAN on the switch.  Make the Dell the layer 3 gateway for both inside endpoints.  No routing on the switch, the Dell is the layer 3 gateway and firewall which has 2 interfaces (whether physical or logical).\n",
                "I currently setting up my home network with a Dell PowerEdge R300 running pfSense as my router and a Cisco Catalyst 3560G switch. In the end I want to have two separate networks, one that is connected to the internet with a LAN, and another that consists of only LAN for pen-testing and various other things that I want to have completely cut off from the internet.\n"
            ]
        },
        {
            "id": "93-2-3",
            "pair": [
                "I currently setting up my home network with a Dell PowerEdge R300 running pfSense as my router and a Cisco Catalyst 3560G switch. In the end I want to have two separate networks, one that is connected to the internet with a LAN, and another that consists of only LAN for pen-testing and various other things that I want to have completely cut off from the internet.\n",
                "Is this achievable through VLANS or other pfSense configurations? Would the VLAN be configured in pfSense, on my switch, or both?\n"
            ]
        },
        {
            "id": "93-3-4",
            "pair": [
                "Is this achievable through VLANS or other pfSense configurations? Would the VLAN be configured in pfSense, on my switch, or both?\n",
                "If this is not achievable through VLANS, what would be the best method? I would prefer not to run a second routing machine but I will if I have to."
            ]
        }
    ],
    [
        {
            "id": "94-1-2",
            "pair": [
                "This isn't working like that. /etc/hosts is not about redirecting at all, you would have to edit this file on all client systems until DNS has propagated completely so this would work only if you control all clients. \n",
                "What might work is to set up some forwarding via iptables or ssh on the old machine to the new one. \n"
            ]
        },
        {
            "id": "94-2-3",
            "pair": [
                "What might work is to set up some forwarding via iptables or ssh on the old machine to the new one. \n",
                "Something to try (untested, just a quick idea, 192.168.0.2 is the new, 192.168.0.1 is the old server): \n"
            ]
        },
        {
            "id": "94-3-4",
            "pair": [
                "Something to try (untested, just a quick idea, 192.168.0.2 is the new, 192.168.0.1 is the old server): \n",
                "On the old server, deactivate mysql. On the new server, start an ssh session like that: \n"
            ]
        },
        {
            "id": "94-4-5",
            "pair": [
                "On the old server, deactivate mysql. On the new server, start an ssh session like that: \n",
                "and keep it open. This should redirect the traffic arriving at port 3306 to the new server, port 3306. If you use another port for mysql, adapt accordingly. \n"
            ]
        },
        {
            "id": "94-5-6",
            "pair": [
                "and keep it open. This should redirect the traffic arriving at port 3306 to the new server, port 3306. If you use another port for mysql, adapt accordingly. \n",
                "I don't follow what your proposal achieves.  How will tweaking /etc/hosts on the old master SQL help you with your goal of minimizing DNS propagation delay?\n"
            ]
        },
        {
            "id": "94-6-7",
            "pair": [
                "I don't follow what your proposal achieves.  How will tweaking /etc/hosts on the old master SQL help you with your goal of minimizing DNS propagation delay?\n",
                "If you control the DNS servers and resolvers on the hosts, you might be able to publish the old master with a shorter TTL in advance of the change, and then when you make the change restart any nscd processes.\n"
            ]
        },
        {
            "id": "94-7-8",
            "pair": [
                "If you control the DNS servers and resolvers on the hosts, you might be able to publish the old master with a shorter TTL in advance of the change, and then when you make the change restart any nscd processes.\n",
                "Best, based on what I'm assuming to be the scale of your business, would be to declare an outage window during which time you deem the databases to be read-only."
            ]
        }
    ],
    [
        {
            "id": "95-1-2",
            "pair": [
                "It has nothing to do with static IPs. Your clients need to use the domain controller(s) for the domain as their DNS server(s). Your modem/router is likely setting itself as the DNS server. Change this setting there so that it gives your clients the proper DNS server as part of the DHCP configuration. \n",
                "As, just as a side note: please read our faq. Home networking is off topic here. \n"
            ]
        },
        {
            "id": "95-2-3",
            "pair": [
                "As, just as a side note: please read our faq. Home networking is off topic here. \n",
                "I have set up a Windows Server 2012 Standard edition at home for learning purposes with the IP 10.1.1.2/16. I also have an ADSL wireless router which connects everything together and works as the DHCP server, with the IP 10.1.1.1/16. The Server name is in server_name.domain_name.org format and netBIOS name is as server_name.\n"
            ]
        },
        {
            "id": "95-3-4",
            "pair": [
                "I have set up a Windows Server 2012 Standard edition at home for learning purposes with the IP 10.1.1.2/16. I also have an ADSL wireless router which connects everything together and works as the DHCP server, with the IP 10.1.1.1/16. The Server name is in server_name.domain_name.org format and netBIOS name is as server_name.\n",
                "When I try to join a client win7 PC to the domain_name.org domain by searching the domain as domain_name.org it says unable to find domain and if I try using just domain_name it finds the domain but fails to connect after entering username/password by giving the error Unable to resolve DNS name of the domain\n"
            ]
        },
        {
            "id": "95-4-5",
            "pair": [
                "When I try to join a client win7 PC to the domain_name.org domain by searching the domain as domain_name.org it says unable to find domain and if I try using just domain_name it finds the domain but fails to connect after entering username/password by giving the error Unable to resolve DNS name of the domain\n",
                "I put a static IP to the win7 client with the DNS settings as primary:10.1.1.2 (server) & secondary: 10.1.1.2 (router) with default gateway: 10.1.1.1. Then searched and tried connecting to domain_name.org it worked really fine.\n"
            ]
        },
        {
            "id": "95-5-6",
            "pair": [
                "I put a static IP to the win7 client with the DNS settings as primary:10.1.1.2 (server) & secondary: 10.1.1.2 (router) with default gateway: 10.1.1.1. Then searched and tried connecting to domain_name.org it worked really fine.\n",
                "My problem is, I don't want to use static IPs for my clients. I suspect the issue should be with my ADSL router which is the DHCP server for the LAN. Please help me to solve this. Thanks."
            ]
        }
    ],
    [
        {
            "id": "96-1-2",
            "pair": [
                "I had a Voip server die on me in the last 3 months.  Die, perhaps isn't the best word, since the machine would be bootable after a kernel panic.  Typically, the machine would function flawlessly between 7am and 7pm.  Then, at random intervals separated by 1-30 days, it would be locked-up and unresponsive at the system console when I returned to the office at 7am.\n",
                "After about 12 iterations of this situation... which invariably happened between 11pm and 7am, it was determined that the motherboard had failed, and specifically, the capacitors were to blame.  I think I read somewhere that temperature extremes can hasten this death.  I suppose my small office is not unusual, but I typically allowed the temperatures to swing as much as 15 degrees F above and 20 degrees below 75 degrees during the off hours.  So, I believe, that small-time operations, that aren't using a chilled data center, are likely to suffer temperature induced failures during the wee hours of the morning.\n"
            ]
        },
        {
            "id": "96-2-3",
            "pair": [
                "After about 12 iterations of this situation... which invariably happened between 11pm and 7am, it was determined that the motherboard had failed, and specifically, the capacitors were to blame.  I think I read somewhere that temperature extremes can hasten this death.  I suppose my small office is not unusual, but I typically allowed the temperatures to swing as much as 15 degrees F above and 20 degrees below 75 degrees during the off hours.  So, I believe, that small-time operations, that aren't using a chilled data center, are likely to suffer temperature induced failures during the wee hours of the morning.\n",
                "My recollection, again, is that the logs showed failure during the 8 hours before we opened our shop in the morning -- always.\n"
            ]
        },
        {
            "id": "96-3-4",
            "pair": [
                "My recollection, again, is that the logs showed failure during the 8 hours before we opened our shop in the morning -- always.\n",
                "Typical \"working hours\" are no more than 40 hours of a week. Less in some parts of the world. A week contains a total of 168 hours. 40/168 = less than 24% of the time of a week is 'working hours'.\n"
            ]
        },
        {
            "id": "96-4-5",
            "pair": [
                "Typical \"working hours\" are no more than 40 hours of a week. Less in some parts of the world. A week contains a total of 168 hours. 40/168 = less than 24% of the time of a week is 'working hours'.\n",
                "That suggests that failures of systems that are running 24/7 will occur 3-times more often during non-working hours than working hours.\n"
            ]
        },
        {
            "id": "96-5-6",
            "pair": [
                "That suggests that failures of systems that are running 24/7 will occur 3-times more often during non-working hours than working hours.\n",
                "Obviously, there are many other considerations that could go into this; multiple shifts, peak times (which for many, might further bias failures toward non-working hours), etc."
            ]
        }
    ],
    [
        {
            "id": "97-1-2",
            "pair": [
                "I'm trying to configure a shared mailbox for my company with Office 365. I've followed the docs as outlined here: https://support.office.com/en-us/article/Create-shared-mailboxes-in-Office-365-871a246d-3acd-4bba-948e-5de8be0544c9. However, I tried to send an email to the shared mailbox, but none of my members received the email. \n",
                "Also, another thing I noticed that when I create a shared mailbox, a user has been added automatically to the list of Active Users, which got me thinking, is this new user need to have a license? Are there any additional configurations I need to do to get the Shared Mailbox feature up and running properly?\n"
            ]
        },
        {
            "id": "97-2-3",
            "pair": [
                "Also, another thing I noticed that when I create a shared mailbox, a user has been added automatically to the list of Active Users, which got me thinking, is this new user need to have a license? Are there any additional configurations I need to do to get the Shared Mailbox feature up and running properly?\n",
                "In case you're wondering if the DNS stuff such as MX, SRV, TEXT, and CNAME entries are correct, I think they already have, since I can properly login from my mobile devices, Mac, Windows PC as well as receiving and sending emails through each of the personal emails.\n"
            ]
        },
        {
            "id": "97-3-4",
            "pair": [
                "In case you're wondering if the DNS stuff such as MX, SRV, TEXT, and CNAME entries are correct, I think they already have, since I can properly login from my mobile devices, Mac, Windows PC as well as receiving and sending emails through each of the personal emails.\n",
                "Or perhaps, do the shared mailboxes need 24 hours to take effect? Any help would be appreciated, thanks.\n"
            ]
        },
        {
            "id": "97-4-5",
            "pair": [
                "Or perhaps, do the shared mailboxes need 24 hours to take effect? Any help would be appreciated, thanks.\n",
                "Note that, if you send a mail to the shared mailbox, the users of that mailbox won\u2019t get that e-mail in their own personal mailbox. Adding members to a shared mailbox means that, they can view the contents of the shared mailbox. For viewing contents in your shared mailbox from Outlook Web App,\n"
            ]
        },
        {
            "id": "97-5-6",
            "pair": [
                "Note that, if you send a mail to the shared mailbox, the users of that mailbox won\u2019t get that e-mail in their own personal mailbox. Adding members to a shared mailbox means that, they can view the contents of the shared mailbox. For viewing contents in your shared mailbox from Outlook Web App,\n",
                "https://support.office.com/en-us/article/Open-and-use-a-shared-mailbox-in-Outlook-on-the-web-for-business-bc127866-42be-4de7-92ae-1ef2f787fd5c?ui=en-US&rs=en-US&ad=US\n"
            ]
        },
        {
            "id": "97-6-7",
            "pair": [
                "https://support.office.com/en-us/article/Open-and-use-a-shared-mailbox-in-Outlook-on-the-web-for-business-bc127866-42be-4de7-92ae-1ef2f787fd5c?ui=en-US&rs=en-US&ad=US\n",
                "If you want users to receive the mails in their own personal mailbox, try configuring a distribution list instead of a shared mailbox."
            ]
        }
    ],
    [
        {
            "id": "98-1-2",
            "pair": [
                "You will have to move the configuration over to the nginx configuration folder.\n",
                "nginx syntax is fairly straightforward once you get the hang of it, and their documentation is pretty good. And there are a lot of resources online on how to implement a given feature on Apache in nginx instead.\n"
            ]
        },
        {
            "id": "98-2-3",
            "pair": [
                "nginx syntax is fairly straightforward once you get the hang of it, and their documentation is pretty good. And there are a lot of resources online on how to implement a given feature on Apache in nginx instead.\n",
                "You may be able to use a .htaccess -> nginx converter like this one to make all necessary changes.\n"
            ]
        },
        {
            "id": "98-3-4",
            "pair": [
                "You may be able to use a .htaccess -> nginx converter like this one to make all necessary changes.\n",
                "Then, instead of storing it in that directory, you add it under a location in your config.\n"
            ]
        },
        {
            "id": "98-4-5",
            "pair": [
                "Then, instead of storing it in that directory, you add it under a location in your config.\n",
                "Let's say you have an .htaccess file in /var/sites/example.com/foo\n"
            ]
        },
        {
            "id": "98-5-6",
            "pair": [
                "Let's say you have an .htaccess file in /var/sites/example.com/foo\n",
                "Convert it to nginx and then store it in /etc/nginx/includes (or wherever your nginx config folder is) as foo.access.\n"
            ]
        },
        {
            "id": "98-6-7",
            "pair": [
                "Convert it to nginx and then store it in /etc/nginx/includes (or wherever your nginx config folder is) as foo.access.\n",
                "Then you would include it in the main config file:\n"
            ]
        },
        {
            "id": "98-7-8",
            "pair": [
                "Then you would include it in the main config file:\n",
                "If you have limited access to the config folder (if, for example, you only have FTP access to your site under a shared server), you can put the nginx-compatible files in your webroot and ask the server admin to soft-link from within the includes directory:\n"
            ]
        },
        {
            "id": "98-8-9",
            "pair": [
                "If you have limited access to the config folder (if, for example, you only have FTP access to your site under a shared server), you can put the nginx-compatible files in your webroot and ask the server admin to soft-link from within the includes directory:\n",
                "Since they're in the webroot and thus publicly accessible, you'd want to add this to the main config:\n"
            ]
        },
        {
            "id": "98-9-10",
            "pair": [
                "Since they're in the webroot and thus publicly accessible, you'd want to add this to the main config:\n",
                "That way anyone trying to access the files will get blocked from doing so.\n"
            ]
        },
        {
            "id": "98-10-11",
            "pair": [
                "That way anyone trying to access the files will get blocked from doing so.\n",
                "I have a script that has 2 .htaccess files. The fist .htaccess file is in the root directory, and the second is in a folder under the root directory called upload.\n"
            ]
        },
        {
            "id": "98-11-12",
            "pair": [
                "I have a script that has 2 .htaccess files. The fist .htaccess file is in the root directory, and the second is in a folder under the root directory called upload.\n",
                "However, what I can not figure out is what would be the proper way to configure nginx to apply the proper rules to the .htaccess file in the upload folder?"
            ]
        }
    ],
    [
        {
            "id": "99-1-2",
            "pair": [
                "Let me clarify my question. In todays world there are lots of devices that has built-in support for Contacts or a Calendar, if not both. In this case, I'm going to imagine a personal set of devices: A windows PC, an iPad and a Mobile 7 smartphone. (I currently own the windows PC and the iPad, but the remaining two are upcoming)\n",
                "I have a dream of a central location for all my contacts and calendars. Currently I'm using Google Contacts and Google Calendar (which covers Windows) synchronized using a windows Exchange profile in iPad for my calendar and contacts. I'm fairly happy with it, but it lacks some data fields in the contact sync, for instance birth dates.\n"
            ]
        },
        {
            "id": "99-2-3",
            "pair": [
                "I have a dream of a central location for all my contacts and calendars. Currently I'm using Google Contacts and Google Calendar (which covers Windows) synchronized using a windows Exchange profile in iPad for my calendar and contacts. I'm fairly happy with it, but it lacks some data fields in the contact sync, for instance birth dates.\n",
                "A scenario from previously mentioned dream would be (Just an example, you don't have to read it):\n"
            ]
        },
        {
            "id": "99-3-4",
            "pair": [
                "A scenario from previously mentioned dream would be (Just an example, you don't have to read it):\n",
                "The reason I'm posting this question is to figure out the best method of storing my Contacts and Calendars. (\"Best\" meaning covering as many devices as possible retaining as many features as possible, and obviously being live two-way synchronization)\n"
            ]
        },
        {
            "id": "99-4-5",
            "pair": [
                "The reason I'm posting this question is to figure out the best method of storing my Contacts and Calendars. (\"Best\" meaning covering as many devices as possible retaining as many features as possible, and obviously being live two-way synchronization)\n",
                "Examples of such methods would be Google Contacts & Google Calendar, Microsoft Exchange, Apple MobileMe -\n"
            ]
        },
        {
            "id": "99-5-6",
            "pair": [
                "Examples of such methods would be Google Contacts & Google Calendar, Microsoft Exchange, Apple MobileMe -\n",
                "The most reasonable and convincing answer will be marked as correct.\n"
            ]
        },
        {
            "id": "99-6-7",
            "pair": [
                "The most reasonable and convincing answer will be marked as correct.\n",
                "What is your prob with Gmail/contacts/calendar? They do exactly what you are looking for.\n"
            ]
        },
        {
            "id": "99-7-8",
            "pair": [
                "What is your prob with Gmail/contacts/calendar? They do exactly what you are looking for.\n",
                "I have all three google services synced via Exchange to iPhone/iPad. On PC, Gmail web interface has the same functionality as well."
            ]
        }
    ]
]